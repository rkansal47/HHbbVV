{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import plotting\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "from HHbbVV.hh_vars import data_key, years\n",
    "import postprocessing\n",
    "\n",
    "# ignore these because they don't seem to apply\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "\n",
    "from PyPDF2 import PdfMerger\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "plt.style.use(hep.style.CMS)\n",
    "hep.style.use(\"CMS\")\n",
    "formatter = mticker.ScalarFormatter(useMathText=True)\n",
    "formatter.set_powerlimits((-3, 3))\n",
    "plt.rcParams.update({\"font.size\": 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dir = Path(\"../../../plots/ttsfs/24Jul28Distortion\")\n",
    "plot_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_samples = {\n",
    "    \"QCD\": \"QCD\",\n",
    "    \"SingleTop\": [\"ST\"],\n",
    "    \"TTbar\": [\"TTTo2L2Nu\", \"TTToHadronic\"],\n",
    "    \"W+Jets\": \"WJets\",\n",
    "    \"Diboson\": [\"WW\", \"WZ\", \"ZZ\"],\n",
    "    \"Data\": \"SingleMuon\",\n",
    "}\n",
    "\n",
    "sig_samples = {\"Top\": [\"TTToSemiLeptonic\"]}\n",
    "\n",
    "samples = {**bg_samples, **sig_samples}\n",
    "\n",
    "top_matched_key = \"TT Top Matched\"\n",
    "\n",
    "# data_dir = \"../../../../data/ttsfs/24Feb28_update_lp/\"\n",
    "data_dir = \"/ceph/cms/store/user/rkansal/bbVV/ttsfs/24Feb28_update_lp/\"\n",
    "signal_data_dir = \"/ceph/cms/store/user/rkansal/bbVV/ttsfs/24Jul23BLDistortion/\"\n",
    "year = \"2018\"\n",
    "\n",
    "# filters = [(\"('ak8FatJetPt', '0')\", \">=\", 500)]\n",
    "filters = None\n",
    "\n",
    "events_dict = postprocessing.load_samples(data_dir, bg_samples, year, hem_cleaning=False)\n",
    "events_dict |= postprocessing.load_samples(signal_data_dir, sig_samples, year, hem_cleaning=False)\n",
    "\n",
    "cutflow = pd.DataFrame(index=list(samples.keys()))\n",
    "utils.add_to_cutflow(events_dict, \"Selection\", \"weight\", cutflow)\n",
    "cutflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HEM Veto Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_pre = 0\n",
    "tot_hem = 0\n",
    "for run in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "    cf = utils.get_pickles(\n",
    "        f\"{data_dir}/{year}/SingleMuon_Run2018{run}/pickles\", year, f\"SingleMuon_Run2018{run}\"\n",
    "    )[\"cutflow\"]\n",
    "    tot_pre += cf[\"ak4_jet\"]\n",
    "    tot_hem += cf[\"hem_cleaning\"]\n",
    "\n",
    "print(tot_hem / tot_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_samples = [\"Top\", \"TTbar\", \"SingleTop\", \"W+Jets\"]  # normalizations are off\n",
    "\n",
    "total_scale = 0\n",
    "total_noscale = 0\n",
    "for sample, events in events_dict.items():\n",
    "    if sample in scale_samples:\n",
    "        total_scale += events[\"weight\"].sum().values[0]\n",
    "    elif sample != \"Data\":\n",
    "        total_noscale += events[\"weight\"].sum().values[0]\n",
    "\n",
    "print(f\"Total MC: {total_scale + total_noscale}\")\n",
    "\n",
    "sf = (events_dict[\"Data\"][\"weight\"].sum().values[0] - total_noscale) / total_scale\n",
    "for sample, events in events_dict.items():\n",
    "    if sample in scale_samples:\n",
    "        events[\"weight\"] *= sf\n",
    "\n",
    "total = 0\n",
    "for sample, events in events_dict.items():\n",
    "    if sample != \"Data\":\n",
    "        total += events[\"weight\"].sum().values[0]\n",
    "\n",
    "print(f\"New Total MC: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.add_to_cutflow(events_dict, \"Scale\", \"weight\", cutflow)\n",
    "cutflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in events_dict:\n",
    "    events_dict[key] = events_dict[key][events_dict[key][\"ak8FatJetPt\"][0] >= 500]\n",
    "    events_dict[key] = events_dict[key][events_dict[key][\"ak8FatJetMsd\"][0] >= 125]\n",
    "    events_dict[key] = events_dict[key][events_dict[key][\"ak8FatJetMsd\"][0] <= 225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_dict[top_matched_key] = events_dict[\"Top\"].loc[events_dict[\"Top\"][\"top_matched\"][0] == 1]\n",
    "events_dict[\"TT W Matched\"] = events_dict[\"Top\"].loc[events_dict[\"Top\"][\"w_matched\"][0] == 1]\n",
    "events_dict[\"TT Unmatched\"] = pd.concat(\n",
    "    [\n",
    "        events_dict[\"TTbar\"],\n",
    "        # events_dict[\"SingleTop\"],\n",
    "        events_dict[\"Top\"].loc[events_dict[\"Top\"][\"unmatched\"][0] == 1],\n",
    "    ]\n",
    ")\n",
    "# del events_dict[\"Top\"]\n",
    "# del events_dict[\"TTbar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(events_dict[top_matched_key][\"weight\"]))\n",
    "print(np.sum(events_dict[\"TT W Matched\"][\"weight\"]))\n",
    "print(np.sum(events_dict[\"TT Unmatched\"][\"weight\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LP SF Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP = 5.0\n",
    "\n",
    "events = events_dict[top_matched_key]\n",
    "\n",
    "up_prong_rc = (\n",
    "    (events[\"lp_sf_outside_boundary_quarks\"][0] > 0)\n",
    "    | (events[\"lp_sf_double_matched_event\"][0] > 0)\n",
    "    | (events[\"lp_sf_unmatched_quarks\"][0] > 0)\n",
    ").to_numpy()\n",
    "\n",
    "down_prong_rc = (\n",
    "    (events[\"lp_sf_inside_boundary_quarks\"][0] > 0)\n",
    "    | (events[\"lp_sf_double_matched_event\"][0] > 0)\n",
    "    | (events[\"lp_sf_unmatched_quarks\"][0] > 0)\n",
    ").to_numpy()\n",
    "\n",
    "rc_unmatched = events[\"lp_sf_rc_unmatched_quarks\"][0] > 0\n",
    "\n",
    "for shift, prong_rc in [(\"up\", up_prong_rc), (\"down\", down_prong_rc)]:\n",
    "    np_sfs = events[\"lp_sf_lnN\"][0].to_numpy()\n",
    "    np_sfs[prong_rc] = events[f\"lp_sf_np_{shift}\"][0][prong_rc]\n",
    "    np_sfs = np.nan_to_num(np.clip(np_sfs, 1.0 / CLIP, CLIP))\n",
    "    events.loc[:, (f\"lp_sf_np_{shift}\", 0)] = np_sfs / np.mean(np_sfs, axis=0)\n",
    "\n",
    "for shift in [\"up\", \"down\"]:\n",
    "    np_sfs = events[\"lp_sf_lnN\"][0].to_numpy()\n",
    "    np_sfs[rc_unmatched] = CLIP if shift == \"up\" else 1.0 / CLIP\n",
    "    np_sfs = np.nan_to_num(np.clip(np_sfs, 1.0 / CLIP, CLIP))\n",
    "    events[f\"lp_sf_unmatched_{shift}\"] = np_sfs / np.mean(np_sfs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(rc_unmatched))\n",
    "print(np.mean(events[\"lp_sf_unmatched_quarks\"][0] > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize scale factors to average to 1\n",
    "for key in [\n",
    "    # \"lp_sf\",\n",
    "    \"lp_sf_lnN\",\n",
    "    \"lp_sf_sys_down\",\n",
    "    \"lp_sf_sys_up\",\n",
    "    \"lp_sf_dist_down\",\n",
    "    \"lp_sf_dist_up\",\n",
    "    \"lp_sf_pt_extrap_vars\",\n",
    "    \"lp_sfs_bl_ratio\",\n",
    "]:\n",
    "    # cut off at 5\n",
    "    events_dict[top_matched_key].loc[:, key] = np.clip(\n",
    "        events_dict[top_matched_key].loc[:, key].values, 1.0 / 5.0, 5.0\n",
    "    )\n",
    "\n",
    "    if key == \"lp_sfs_bl_ratio\":\n",
    "        mean_lp_sfs = np.mean(\n",
    "            np.nan_to_num(\n",
    "                events_dict[top_matched_key][key][0] * events_dict[top_matched_key][\"lp_sf_lnN\"][0]\n",
    "            ),\n",
    "            axis=0,\n",
    "        )\n",
    "    else:\n",
    "        mean_lp_sfs = np.mean(np.nan_to_num(events_dict[top_matched_key][key]), axis=0)\n",
    "\n",
    "    events_dict[top_matched_key].loc[:, key] = (\n",
    "        np.nan_to_num(events_dict[top_matched_key].loc[:, key]) / mean_lp_sfs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\"font.size\": 24})\n",
    "plt.figure(figsize=(12, 12))\n",
    "_ = plt.hist(\n",
    "    events_dict[top_matched_key][\"lp_sf_lnN\"][10].values,\n",
    "    np.logspace(-4, 2, 101, base=10),\n",
    "    histtype=\"step\",\n",
    ")\n",
    "plt.xscale(\"log\")\n",
    "# plt.yscale(\"log\")\n",
    "plt.xlabel(\"LP SF\")\n",
    "plt.title(\"Scale factor distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# events = events_dict[top_matched_key]\n",
    "# sj_matching_unc = (\n",
    "#     (np.sum(events[\"lp_sf_unmatched_quarks\"]) / (len(events) * 3))\n",
    "#     # OR of double matched and boundary quarks\n",
    "#     # >0.1 to avoid floating point errors\n",
    "#     + (\n",
    "#         np.sum((events[\"lp_sf_double_matched_event\"] + events[\"lp_sf_boundary_quarks\"]) > 0.1)\n",
    "#         / (len(events))\n",
    "#     )\n",
    "# ).values[0]\n",
    "# sj_matching_unc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing distortion uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "from tqdm import tqdm\n",
    "\n",
    "for dist_year in tqdm(years[1:]):\n",
    "    f = uproot.open(package_path / f\"corrections/lp_ratios/ratio_{dist_year}.root\")\n",
    "\n",
    "    # 3D histogram: [subjet_pt, ln(0.8/Delta), ln(kT/GeV)]\n",
    "    mc_nom = f[\"mc_nom\"].to_numpy()\n",
    "    ratio_edges = mc_nom[1:]\n",
    "    mc_nom = mc_nom[0]\n",
    "\n",
    "    mc_tot_pt = np.sum(mc_nom, axis=(1, 2), keepdims=True)\n",
    "    mc_density = mc_nom / mc_tot_pt\n",
    "    plotting.plot_lund_plane_six(\n",
    "        mc_density, ratio_edges, name=f\"{plot_dir}/{dist_year}_MC.pdf\", show=False\n",
    "    )\n",
    "\n",
    "    # ratio_nom = f[\"ratio_nom\"].to_numpy()[0]\n",
    "\n",
    "    for sig in [\"GluGluToHHTobbVV_node_cHHH1\", \"VBF_HHTobbVV_CV_1_C2V_2_C3_1\"]:\n",
    "        with (package_path / f\"corrections/lp_ratios/signals/{dist_year}_{sig}.hist\").open(\n",
    "            \"rb\"\n",
    "        ) as f:\n",
    "            sig_lp_hist = pickle.load(f)\n",
    "            sig_tot_pt = np.sum(sig_lp_hist.values(), axis=(1, 2), keepdims=True)\n",
    "            sig_density = sig_lp_hist.values() / sig_tot_pt\n",
    "\n",
    "            mc_sig_ratio = np.nan_to_num(mc_density / sig_density, nan=1.0)\n",
    "            mc_sig_ratio[mc_sig_ratio == 0] = 1.0\n",
    "            mc_sig_ratio = np.clip(mc_sig_ratio, 0.2, 5.0)\n",
    "\n",
    "            plotting.plot_lund_plane_six(\n",
    "                sig_density, ratio_edges, name=f\"{plot_dir}/{dist_year}_{sig}.pdf\", show=False\n",
    "            )\n",
    "            plotting.plot_lund_plane_six(\n",
    "                mc_sig_ratio,\n",
    "                ratio_edges,\n",
    "                name=f\"{plot_dir}/{dist_year}_{sig}_ratio.pdf\",\n",
    "                show=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_tot = np.sum(mc_nom)\n",
    "sig_tot = sig_lp_hist.sum()\n",
    "sig_mc_ratio = np.clip(\n",
    "    np.nan_to_num((sig_lp_hist.values() / sig_tot) / (mc_nom / mc_tot), nan=1), 0.5, 2.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(mc_sig_ratio_pt, axis=(1, 2)))\n",
    "print(np.mean(mc_sig_old_ratio_pt, axis=(1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_tot_pt = np.sum(sig_lp_hist.values(), axis=(1, 2), keepdims=True)\n",
    "mc_tot_pt = np.sum(mc_nom, axis=(1, 2), keepdims=True)\n",
    "mc_sig_ratio_pt = np.nan_to_num((mc_nom / mc_tot_pt) / (sig_lp_hist.values() / sig_tot_pt), nan=1.0)\n",
    "mc_sig_ratio_pt[mc_sig_ratio_pt == 0] = 1.0\n",
    "mc_sig_ratio_pt = np.clip(mc_sig_ratio_pt, 0.5, 2.0)\n",
    "plt.imshow(mc_sig_ratio_pt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_tot_pt = np.sum(sig_old_lp_hist.values(), axis=(1, 2), keepdims=True)\n",
    "mc_tot_pt = np.sum(mc_nom, axis=(1, 2), keepdims=True)\n",
    "mc_sig_old_ratio_pt = np.nan_to_num(\n",
    "    (mc_nom / mc_tot_pt) / (sig_old_lp_hist.values() / sig_tot_pt), nan=1.0\n",
    ")\n",
    "mc_sig_old_ratio_pt[mc_sig_old_ratio_pt == 0] = 1.0\n",
    "mc_sig_old_ratio_pt = np.clip(mc_sig_old_ratio_pt, 0.5, 2.0)\n",
    "plt.imshow(mc_sig_old_ratio_pt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples = [\n",
    "    \"QCD\",\n",
    "    \"Diboson\",\n",
    "    # \"Single Top\",\n",
    "    \"W+Jets\",\n",
    "    \"TT Unmatched\",\n",
    "    \"TT W Matched\",\n",
    "    top_matched_key,\n",
    "]\n",
    "\n",
    "bg_colours = {\n",
    "    \"QCD\": \"lightblue\",\n",
    "    \"Single Top\": \"darkblue\",\n",
    "    \"TT Unmatched\": \"darkgreen\",\n",
    "    \"TT W Matched\": \"green\",\n",
    "    \"TT Top Matched\": \"orange\",\n",
    "    \"W+Jets\": \"darkred\",\n",
    "    \"Diboson\": \"red\",\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {var: (bins, label)}\n",
    "plot_vars = {\n",
    "    # \"ak8FatJetMsd\": ([20, 125, 225], r\"$m_{SD}$ (GeV)\"),\n",
    "    # \"ak8FatJetParticleNetMass\": ([30, 50, 200], r\"$m_{reg}$ (GeV)\"),\n",
    "    # \"ak8FatJetPt\": ([30, 0, 1200], r\"$p_T$ (GeV)\"),\n",
    "    # \"MET_pt\": ([30, 0, 200], r\"MET (GeV)\"),\n",
    "    # \"ak8FatJetnPFCands\": ([20, 0, 120], r\"# of PF Candidates\"),\n",
    "    # \"ak8FatJetParticleNet_Th4q\": ([20, 0.6, 1], r\"ParticleNet $T_{H4q}$ Non-MD\"),\n",
    "    \"ak8FatJetParTMD_THWW4q\": ([20, 0.2, 1], r\"$T_{HVV}$\"),\n",
    "    # \"tau21\": ([20, 0.04, 0.8], r\"$\\tau_{21}$\"),\n",
    "    # \"tau32\": ([20, 0.2, 1], r\"$\\tau_{32}$\"),\n",
    "    # \"tau43\": ([20, 0.42, 1], r\"$\\tau_{43}$\"),\n",
    "    # \"tau42\": ([20, 0, 1], r\"$\\tau_{42}$\"),\n",
    "    # \"tau41\": ([20, 0, 1], r\"$\\tau_{41}$\"),\n",
    "}\n",
    "\n",
    "pre_hists = {}\n",
    "\n",
    "for var, (bins, label) in plot_vars.items():\n",
    "    if var not in pre_hists:\n",
    "        pre_hists[var] = utils.singleVarHistNoMask(\n",
    "            events_dict, var, bins, label, weight_key=\"weight\"\n",
    "        )\n",
    "\n",
    "merger_pre_plots = PdfMerger()\n",
    "\n",
    "for var, var_hist in pre_hists.items():\n",
    "    name = f\"{plot_dir}/pre_{var}.pdf\"\n",
    "    plotting.ratioLinePlot(\n",
    "        var_hist,\n",
    "        plot_samples,\n",
    "        year,\n",
    "        # bg_err=None,\n",
    "        name=name,\n",
    "        bg_colours=bg_colours,\n",
    "        # bg_order=plot_samples,\n",
    "        # ratio_ylims=[0.6, 1.3],\n",
    "    )\n",
    "    merger_pre_plots.append(name)\n",
    "\n",
    "merger_pre_plots.write(f\"{plot_dir}/PrePlots.pdf\")\n",
    "merger_pre_plots.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_hists = {}\n",
    "post_hists_err = {}\n",
    "uncs_dict = {}\n",
    "\n",
    "events = events_dict[top_matched_key]\n",
    "\n",
    "for var, (bins, label) in plot_vars.items():\n",
    "    # if var not in post_hists:\n",
    "    toy_hists = []\n",
    "    for i in range(events[\"lp_sf\"].shape[1]):\n",
    "        toy_hists.append(\n",
    "            np.histogram(\n",
    "                events[var][0].values.squeeze(),\n",
    "                np.linspace(*bins[1:], bins[0] + 1),\n",
    "                weights=events[\"weight\"][0].values * events[\"lp_sf\"][i].values,\n",
    "            )[0]\n",
    "        )\n",
    "\n",
    "    sys_up_down = []\n",
    "    for key in [\"lp_sf_sys_up\", \"lp_sf_sys_down\"]:\n",
    "        sys_up_down.append(\n",
    "            np.histogram(\n",
    "                events[var][0].values.squeeze(),\n",
    "                np.linspace(*bins[1:], bins[0] + 1),\n",
    "                weights=events[\"weight\"][0].values * events[key][0].values,\n",
    "            )[0]\n",
    "        )\n",
    "\n",
    "    np_up_down = []\n",
    "    for key in [\"lp_sf_np_up\", \"lp_sf_np_down\"]:\n",
    "        np_up_down.append(\n",
    "            np.histogram(\n",
    "                events[var][0].values.squeeze(),\n",
    "                np.linspace(*bins[1:], bins[0] + 1),\n",
    "                weights=events[\"weight\"][0].values * events[key][0].values,\n",
    "            )[0]\n",
    "        )\n",
    "\n",
    "    um_up_down = []\n",
    "    for key in [\"lp_sf_unmatched_up\", \"lp_sf_unmatched_down\"]:\n",
    "        um_up_down.append(\n",
    "            np.histogram(\n",
    "                events[var][0].values.squeeze(),\n",
    "                np.linspace(*bins[1:], bins[0] + 1),\n",
    "                weights=events[\"weight\"][0].values * events[key].values,\n",
    "            )[0]\n",
    "        )\n",
    "\n",
    "    nom_vals = toy_hists[0]  # first column are nominal values\n",
    "\n",
    "    pt_toy_hists = []\n",
    "    for i in range(events[\"lp_sf_pt_extrap_vars\"].shape[1]):\n",
    "        pt_toy_hists.append(\n",
    "            np.histogram(\n",
    "                events[var][0].values.squeeze(),\n",
    "                np.linspace(*bins[1:], bins[0] + 1),\n",
    "                weights=events[\"weight\"][0].values * events[\"lp_sf_pt_extrap_vars\"][i].values,\n",
    "            )[0]\n",
    "        )\n",
    "\n",
    "    b_ratio_hist = np.histogram(\n",
    "        events[var][0].values.squeeze(),\n",
    "        np.linspace(*bins[1:], bins[0] + 1),\n",
    "        weights=events[\"weight\"][0].values\n",
    "        * events[\"lp_sfs_bl_ratio\"][0].values\n",
    "        * events[\"lp_sf_lnN\"][0].values,\n",
    "    )[0]\n",
    "\n",
    "    uncs = {\n",
    "        \"stat_unc\": np.minimum(nom_vals, np.std(toy_hists[1:], axis=0)),  # cap at 100% unc\n",
    "        \"syst_rat_unc\": np.minimum(nom_vals, (np.abs(sys_up_down[0] - sys_up_down[1])) / 2),\n",
    "        \"np_unc\": np.minimum(nom_vals, (np.abs(np_up_down[0] - np_up_down[1])) / 2),\n",
    "        \"um_unc\": np.minimum(nom_vals, (np.abs(um_up_down[0] - um_up_down[1])) / 2),\n",
    "        # \"syst_sjm_unc\": nom_vals * sj_matching_unc,\n",
    "        \"syst_sjpt_unc\": np.minimum(nom_vals, np.std(pt_toy_hists, axis=0)),\n",
    "        \"syst_b_unc\": np.abs(1 - (b_ratio_hist / nom_vals)) * nom_vals,\n",
    "    }\n",
    "\n",
    "    # uncs = {}\n",
    "\n",
    "    # for i, shift in enumerate([\"up\", \"down\"]):\n",
    "    #     uncs[shift] = {\n",
    "    #         \"syst_rat_unc\": np.clip(sys_up_down[i], 0, 2 * nom_vals),\n",
    "    #         \"np_unc\": np.clip(np_up_down[i], 0, 2 * nom_vals),\n",
    "    #         \"um_unc\": np.clip(um_up_down[i], 0, 2 * nom_vals),\n",
    "    #     }\n",
    "\n",
    "    #     uncs[shift]\n",
    "\n",
    "    #     for key, val in uncs_symm.items():\n",
    "    #         if shift == \"up\":\n",
    "    #             uncs[shift][key] = nom_vals + val\n",
    "    #         else:\n",
    "    #             uncs[shift][key] = nom_vals - val\n",
    "\n",
    "    uncs_dict[var] = uncs\n",
    "\n",
    "    unc = np.linalg.norm(list(uncs.values()), axis=0)\n",
    "\n",
    "    thist = deepcopy(pre_hists[var])\n",
    "    top_matched_key_index = np.where(np.array(list(thist.axes[0])) == top_matched_key)[0][0]\n",
    "    thist.view(flow=False)[top_matched_key_index, :].value = nom_vals\n",
    "    post_hists[var] = thist\n",
    "    post_hists_err[var] = unc\n",
    "\n",
    "\n",
    "merger_post_plots = PdfMerger()\n",
    "\n",
    "for var, var_hist in post_hists.items():\n",
    "    name = f\"{plot_dir}/post_{var}.pdf\"\n",
    "    plotting.ratioLinePlot(\n",
    "        var_hist,\n",
    "        plot_samples,\n",
    "        year,\n",
    "        bg_colours=bg_colours,\n",
    "        bg_err=post_hists_err[var],\n",
    "        name=name,\n",
    "    )\n",
    "    merger_post_plots.append(name)\n",
    "\n",
    "merger_post_plots.write(f\"{plot_dir}/PostPlots.pdf\")\n",
    "merger_post_plots.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post LnN Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_lnN_hists = {}\n",
    "post_lnN_hists_err = {}\n",
    "uncs_lnN_dict = {}\n",
    "\n",
    "events = events_dict[top_matched_key]\n",
    "\n",
    "for var, (bins, label) in plot_vars.items():\n",
    "    if var not in post_lnN_hists:\n",
    "        toy_hists = []\n",
    "        for i in range(events[\"lp_sf_lnN\"].shape[1]):\n",
    "            toy_hists.append(\n",
    "                np.histogram(\n",
    "                    events[var][0].values.squeeze(),\n",
    "                    np.linspace(*bins[1:], bins[0] + 1),\n",
    "                    weights=events[\"weight\"][0].values * events[\"lp_sf_lnN\"][i].values,\n",
    "                )[0]\n",
    "            )\n",
    "\n",
    "        sys_up_down = []\n",
    "        for key in [\"lp_sf_sys_up\", \"lp_sf_sys_down\"]:\n",
    "            sys_up_down.append(\n",
    "                np.histogram(\n",
    "                    events[var][0].values.squeeze(),\n",
    "                    np.linspace(*bins[1:], bins[0] + 1),\n",
    "                    weights=events[\"weight\"][0].values * events[key][0].values,\n",
    "                )[0]\n",
    "            )\n",
    "\n",
    "        np_up_down = []\n",
    "        for key in [\"lp_sf_np_up\", \"lp_sf_np_down\"]:\n",
    "            np_up_down.append(\n",
    "                np.histogram(\n",
    "                    events[var][0].values.squeeze(),\n",
    "                    np.linspace(*bins[1:], bins[0] + 1),\n",
    "                    weights=events[\"weight\"][0].values * np.nan_to_num(events[key][0].values),\n",
    "                )[0]\n",
    "            )\n",
    "\n",
    "        dist_up_down = []\n",
    "        for key in [\"lp_sf_dist_up\", \"lp_sf_dist_down\"]:\n",
    "            dist_up_down.append(\n",
    "                np.histogram(\n",
    "                    events[var][0].values.squeeze(),\n",
    "                    np.linspace(*bins[1:], bins[0] + 1),\n",
    "                    weights=events[\"weight\"][0].values * np.nan_to_num(events[key][0].values),\n",
    "                )[0]\n",
    "            )\n",
    "\n",
    "        um_up_down = []\n",
    "        for key in [\"lp_sf_unmatched_up\", \"lp_sf_unmatched_down\"]:\n",
    "            um_up_down.append(\n",
    "                np.histogram(\n",
    "                    events[var][0].values.squeeze(),\n",
    "                    np.linspace(*bins[1:], bins[0] + 1),\n",
    "                    weights=events[\"weight\"][0].values * np.nan_to_num(events[key].values),\n",
    "                )[0]\n",
    "            )\n",
    "\n",
    "        nom_vals = toy_hists[0]  # first column are nominal values\n",
    "\n",
    "        pt_toy_hists = []\n",
    "        for i in range(events[\"lp_sf_pt_extrap_vars\"].shape[1]):\n",
    "            pt_toy_hists.append(\n",
    "                np.histogram(\n",
    "                    events[var][0].values.squeeze(),\n",
    "                    np.linspace(*bins[1:], bins[0] + 1),\n",
    "                    weights=events[\"weight\"][0].values * events[\"lp_sf_pt_extrap_vars\"][i].values,\n",
    "                )[0]\n",
    "            )\n",
    "\n",
    "        b_ratio_hist = np.histogram(\n",
    "            events[var][0].values.squeeze(),\n",
    "            np.linspace(*bins[1:], bins[0] + 1),\n",
    "            weights=events[\"weight\"][0].values\n",
    "            * events[\"lp_sfs_bl_ratio\"][0].values\n",
    "            * events[\"lp_sf_lnN\"][0].values,\n",
    "        )[0]\n",
    "\n",
    "        uncs = {\n",
    "            \"stat_unc\": np.minimum(nom_vals, np.std(toy_hists[1:], axis=0)),  # cap at 100% unc\n",
    "            \"syst_rat_unc\": np.minimum(nom_vals, (np.abs(sys_up_down[0] - sys_up_down[1])) / 2),\n",
    "            \"np_unc\": np.minimum(nom_vals, (np.abs(np_up_down[0] - np_up_down[1])) / 2),\n",
    "            \"dist_unc\": np.minimum(nom_vals, (np.abs(dist_up_down[0] - dist_up_down[1])) / 2),\n",
    "            \"um_unc\": np.minimum(nom_vals, (np.abs(um_up_down[0] - um_up_down[1])) / 2),\n",
    "            # \"syst_sjm_unc\": nom_vals * sj_matching_unc,\n",
    "            \"syst_sjpt_unc\": np.minimum(nom_vals, np.std(pt_toy_hists, axis=0)),\n",
    "            \"syst_b_unc\": np.abs(1 - (b_ratio_hist / nom_vals)) * nom_vals,\n",
    "        }\n",
    "\n",
    "        uncs_lnN_dict[var] = uncs\n",
    "\n",
    "        unc = np.linalg.norm(list(uncs.values()), axis=0)\n",
    "\n",
    "        thist = deepcopy(pre_hists[var])\n",
    "        top_matched_key_index = np.where(np.array(list(thist.axes[0])) == top_matched_key)[0][0]\n",
    "        thist.view(flow=False)[top_matched_key_index, :].value = nom_vals\n",
    "        post_lnN_hists[var] = thist\n",
    "\n",
    "        post_lnN_hists_err[var] = unc\n",
    "\n",
    "\n",
    "merger_post_plots = PdfMerger()\n",
    "\n",
    "for var, var_hist in post_lnN_hists.items():\n",
    "    name = f\"{plot_dir}/postlnN_{var}.pdf\"\n",
    "    plotting.ratioLinePlot(\n",
    "        var_hist,\n",
    "        plot_samples,\n",
    "        year,\n",
    "        bg_colours=bg_colours,\n",
    "        bg_err=post_lnN_hists_err[var],\n",
    "        name=name,\n",
    "    )\n",
    "    merger_post_plots.append(name)\n",
    "\n",
    "merger_post_plots.write(f\"{plot_dir}/PostLnNPlots.pdf\")\n",
    "merger_post_plots.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binn = -1\n",
    "tvar = \"ak8FatJetParTMD_THWW4q\"\n",
    "pre_vals = pre_hists[tvar].view(flow=False)[top_matched_key_index, :].value\n",
    "nom_vals = post_hists[tvar].view(flow=False)[top_matched_key_index, :].value\n",
    "unc = post_hists_err[tvar]\n",
    "print(\"SF: \", nom_vals[binn] / pre_vals[binn])\n",
    "print(\"Uncs: \", {key: val[binn] / nom_vals[binn] * 100 for key, val in uncs_dict[tvar].items()})\n",
    "print(\"Combined: \", unc[binn] / nom_vals[binn] * 100)\n",
    "print(\"Abs: \", unc[binn] / pre_vals[binn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binn = -1\n",
    "tvar = \"ak8FatJetParTMD_THWW4q\"\n",
    "pre_vals = pre_hists[tvar].view(flow=False)[top_matched_key_index, :].value\n",
    "nom_vals = post_lnN_hists[tvar].view(flow=False)[top_matched_key_index, :].value\n",
    "unc = post_lnN_hists_err[tvar]\n",
    "print(\"SF: \", nom_vals[binn] / pre_vals[binn])\n",
    "print(\"Uncs: \", {key: val[binn] / nom_vals[binn] * 100 for key, val in uncs_lnN_dict[tvar].items()})\n",
    "print(\"Combined: \", unc[binn] / nom_vals[binn] * 100)\n",
    "print(\"Abs: \", unc[binn] / pre_vals[binn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi^2 improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chisquare(mc, data):\n",
    "    return np.sum(np.square(data - mc) / data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vals = pre_hists[tvar][\"Data\", ...].values()\n",
    "pre_MC_vals = (\n",
    "    pre_hists[tvar][sum, :].values()\n",
    "    - data_vals\n",
    "    - pre_hists[tvar][\"TTbar\", :].values()  # remove repeated data\n",
    "    - pre_hists[tvar][\"Top\", :].values()\n",
    "    - pre_hists[tvar][\"SingleTop\", :].values()\n",
    ")\n",
    "post_lnN_MC_vals = (\n",
    "    post_lnN_hists[tvar][sum, :].values()\n",
    "    - data_vals\n",
    "    - post_lnN_hists[tvar][\"TTbar\", :].values()  # remove repeated data\n",
    "    - post_lnN_hists[tvar][\"Top\", :].values()\n",
    "    - post_lnN_hists[tvar][\"SingleTop\", :].values()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = 10\n",
    "print(\"Pre chi2:\", chisquare(pre_MC_vals[-lb:], data_vals[-lb:]))\n",
    "print(\"Post chi2:\", chisquare(post_lnN_MC_vals[-lb:], data_vals[-lb:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = 5\n",
    "print(\"Pre chi2:\", chisquare(pre_MC_vals[-lb:], data_vals[-lb:]))\n",
    "print(\"Post chi2:\", chisquare(post_lnN_MC_vals[-lb:], data_vals[-lb:]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratio plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvar = \"ak8FatJetParTMD_THWW4q\"\n",
    "\n",
    "# plt.figure(figsize=(12, 12))\n",
    "# hists = pre_hists[tvar]\n",
    "# bg_tot = np.sum(hists[plot_samples, :].values(), axis=0)\n",
    "# mcdata_ratio = (bg_tot + 1e-5) / hists[data_key, :].values()\n",
    "# _ = plt.hist(mcdata_ratio - 1, np.linspace(-0.5, 0.5, 10), histtype='step')\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "hists = post_hists[tvar]\n",
    "bg_tot = np.sum(hists[plot_samples, :].values(), axis=0)\n",
    "data_tot = hists[data_key, :].values()\n",
    "unc = post_hists_err[tvar]\n",
    "mcdata_ratio = (bg_tot) / data_tot\n",
    "_ = plt.hist(((bg_tot - data_tot) / (unc))[10:], np.linspace(-6.5, 4.5, 23), histtype=\"step\")\n",
    "plt.xlabel(\"(MC - Data) / Unc.\")\n",
    "plt.savefig(f\"{plot_dir}/pull_hist.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.ratioLinePlot(\n",
    "    post_hists[tvar],\n",
    "    plot_samples,\n",
    "    year,\n",
    "    bg_err=post_hists_err[tvar],\n",
    "    name=f\"{plot_dir}/post_ak8FatJetParTMD_THWW4q_pulls.pdf\",\n",
    "    pulls=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_dict = {}\n",
    "\n",
    "for key in events_dict:\n",
    "    cut_dict[key] = events_dict[key][events_dict[key][\"tau42\"][0] <= 0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {var: (bins, label)}\n",
    "plot_vars = {\n",
    "    \"ak8FatJetParTMD_THWW4q\": ([20, 0.6, 1], r\"ParT $T_{HWW4q}$ MD\"),\n",
    "}\n",
    "\n",
    "pre_hists_cut = {}\n",
    "\n",
    "for var, (bins, label) in plot_vars.items():\n",
    "    if var not in pre_hists_cut:\n",
    "        pre_hists_cut[var] = utils.singleVarHistNoMask(\n",
    "            cut_dict, var, bins, label, weight_key=\"weight\"\n",
    "        )\n",
    "\n",
    "merger_pre_plots = PdfFileMerger()\n",
    "\n",
    "for var, var_hist in pre_hists_cut.items():\n",
    "    name = f\"{plot_dir}/pre_{var}_tau42_cut.pdf\"\n",
    "    plotting.ratioLinePlot(\n",
    "        var_hist,\n",
    "        plot_samples,\n",
    "        year,\n",
    "        bg_err=None,\n",
    "        name=name,\n",
    "    )\n",
    "    merger_pre_plots.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_lnN_cut_hists = {}\n",
    "post_lnN_cut_hists_err = {}\n",
    "uncs_lnN_cut_dict = {}\n",
    "\n",
    "events = cut_dict[top_matched_key]\n",
    "\n",
    "for var, (bins, label) in plot_vars.items():\n",
    "    if var not in post_lnN_cut_hists:\n",
    "        toy_hists = []\n",
    "        for i in range(events[\"lp_sf_lnN\"].shape[1]):\n",
    "            toy_hists.append(\n",
    "                np.histogram(\n",
    "                    events[var][0].values.squeeze(),\n",
    "                    np.linspace(*bins[1:], bins[0] + 1),\n",
    "                    weights=events[\"weight\"][0].values * events[\"lp_sf_lnN\"][i].values,\n",
    "                )[0]\n",
    "            )\n",
    "\n",
    "        sys_up_down = []\n",
    "        for key in [\"lp_sf_sys_up\", \"lp_sf_sys_down\"]:\n",
    "            sys_up_down.append(\n",
    "                np.histogram(\n",
    "                    events[var][0].values.squeeze(),\n",
    "                    np.linspace(*bins[1:], bins[0] + 1),\n",
    "                    weights=events[\"weight\"][0].values * events[key][0].values,\n",
    "                )[0]\n",
    "            )\n",
    "\n",
    "        nom_vals = toy_hists[0]  # first column are nominal values\n",
    "\n",
    "        uncs = {\n",
    "            \"stat_unc\": np.minimum(nom_vals, np.std(toy_hists[1:], axis=0)),  # cap at 100% unc\n",
    "            \"syst_rat_unc\": np.minimum(nom_vals, (np.abs(sys_up_down[0] - sys_up_down[1])) / 2),\n",
    "            \"syst_sjm_unc\": nom_vals * sj_matching_unc,\n",
    "            \"syst_sjpt_unc\": nom_vals * sj_pt_unc,\n",
    "        }\n",
    "\n",
    "        uncs_lnN_cut_dict[var] = uncs\n",
    "\n",
    "        unc = np.linalg.norm(list(uncs.values()), axis=0)\n",
    "\n",
    "        thist = deepcopy(pre_hists[var])\n",
    "        top_matched_key_index = np.where(np.array(list(thist.axes[0])) == top_matched_key)[0][0]\n",
    "        thist.view(flow=False)[top_matched_key_index, :].value = nom_vals\n",
    "        post_lnN_cut_hists[var] = thist\n",
    "\n",
    "        post_lnN_cut_hists_err[var] = unc\n",
    "\n",
    "\n",
    "merger_post_plots = PdfFileMerger()\n",
    "\n",
    "for var, var_hist in post_lnN_cut_hists.items():\n",
    "    name = f\"{plot_dir}/postlnN_{var}_cut.pdf\"\n",
    "    plotting.ratioLinePlot(\n",
    "        var_hist,\n",
    "        plot_samples,\n",
    "        year,\n",
    "        bg_err=post_lnN_cut_hists_err[var],\n",
    "        name=name,\n",
    "    )\n",
    "    merger_post_plots.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_hist = utils.singleVarHistNoMask(\n",
    "    events_dict, \"ak8FatJetMass\", [20, 125, 225], r\"$m_{SD}$\", weight_key=\"weight\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.ratioHistPlot(\n",
    "    mass_hist,\n",
    "    [\"QCD\", \"Diboson\", \"Single Top\", \"W+Jets\", \"TT Unmatched\", \"TT W Matched\", top_matched_key],\n",
    "    f\"{plot_dir}/\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('python310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31a7b1cb5f073f7a7d37b3db504c6954ce2b88e0f82e412b65ad0b5f2dd17394"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
