{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import pickle\n",
    "import warnings\n",
    "from os import listdir\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils\n",
    "from hh_vars import norm_preserving_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Syst:\n",
    "    samples: List[str] = None\n",
    "    years: List[str] = field(default_factory=lambda: [\"2017\"])\n",
    "    label: str = None\n",
    "\n",
    "\n",
    "weight_shifts = {\n",
    "    \"pileup\": [],\n",
    "    \"pileupID\": [],\n",
    "    # \"PDFalphaS\": Syst(samples=nonres_sig_keys, label=\"PDF\"),\n",
    "    # \"QCDscale\": Syst(samples=nonres_sig_keys, label=\"QCDscale\"),\n",
    "    \"ISRPartonShower\": [],\n",
    "    \"FSRPartonShower\": [],\n",
    "    \"L1EcalPrefiring\": [],\n",
    "    # \"top_pt\": [\"TT\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# events = pd.read_parquet(\"0-1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2017\n",
    "signals = [\"hhbbvv\", \"xhy\"]\n",
    "samples = {\n",
    "    \"GluGluToHHTobbVV\": \"HHbbVV\",\n",
    "    \"xhy\": \"X[3000]->HY[250]\",\n",
    "    \"TTToSemiLeptonic\": \"TT SL\",\n",
    "    \"qcd\": \"QCD\",\n",
    "    \"data\": \"Data\",\n",
    "}\n",
    "\n",
    "data_key = \"Data\"\n",
    "nonres_sig_keys = [\"HHbbVV\"]\n",
    "res_sig_keys = [\"X[3000]->HY[250]\"]\n",
    "\n",
    "samples = {val: key for key, val in list(samples.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pickles(pickles_path, year, sample_name):\n",
    "    \"\"\"Accumulates all pickles in ``pickles_path`` directory\"\"\"\n",
    "    from coffea.processor.accumulator import accumulate\n",
    "\n",
    "    out_pickles = [f for f in listdir(pickles_path) if f != \".DS_Store\"]\n",
    "\n",
    "    file_name = out_pickles[0]\n",
    "    with open(f\"{pickles_path}/{file_name}\", \"rb\") as file:\n",
    "        # out = pickle.load(file)[year][sample_name]  # TODO: uncomment and delete below\n",
    "        out = pickle.load(file)[year]\n",
    "        sample_name = list(out.keys())[0]\n",
    "        out = out[sample_name]\n",
    "\n",
    "    for file_name in out_pickles[1:]:\n",
    "        with open(f\"{pickles_path}/{file_name}\", \"rb\") as file:\n",
    "            out_dict = pickle.load(file)[year][sample_name]\n",
    "            out = accumulate([out, out_dict])\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def normalize_weights(events: pd.DataFrame, totals: Dict, sample: str, isData: bool):\n",
    "    # don't need any reweighting for data\n",
    "    if isData:\n",
    "        events[\"finalWeight\"] = events[\"weight\"]\n",
    "        return\n",
    "\n",
    "    # check weights are scaled\n",
    "    if \"weight_noxsec\" in events:\n",
    "        if np.all(events[\"weight\"] == events[\"weight_noxsec\"]):\n",
    "            warnings.warn(f\"{sample} has not been scaled by its xsec and lumi!\")\n",
    "\n",
    "    # checking that trigger efficiencies have been applied\n",
    "    if \"weight_noTrigEffs\" in events and not np.all(\n",
    "        np.isclose(events[\"weight\"], events[\"weight_noTrigEffs\"], rtol=1e-5)\n",
    "    ):\n",
    "        # normalize weights with and without trigger efficiencies\n",
    "        events[\"finalWeight\"] = events[\"weight\"] / totals[\"np_nominal\"]\n",
    "        events[\"weight_noTrigEffs\"] /= totals[\"np_nominal\"]\n",
    "    else:\n",
    "        events[\"weight\"] /= totals[\"np_nominal\"]\n",
    "\n",
    "    # normalize all the variations\n",
    "    for wvar in weight_shifts:\n",
    "        if f\"weight_{wvar}Up\" not in events:\n",
    "            continue\n",
    "\n",
    "        for shift in [\"Up\", \"Down\"]:\n",
    "            wlabel = wvar + shift\n",
    "            if wvar in norm_preserving_weights:\n",
    "                # normalize by their totals\n",
    "                events[f\"weight_{wlabel}\"] /= totals[f\"np_{wlabel}\"]\n",
    "            else:\n",
    "                # normalize by the nominal\n",
    "                events[f\"weight_{wlabel}\"] /= totals[\"np_nominal\"]\n",
    "\n",
    "    # normalize scale and PDF weights\n",
    "    for wkey in [\"scale_weights\", \"pdf_weights\"]:\n",
    "        if wkey in events:\n",
    "            events[wkey] /= totals[f\"np_{wkey}\"]\n",
    "\n",
    "\n",
    "def load_samples(\n",
    "    data_dir: str,\n",
    "    samples: Dict[str, str],\n",
    "    year: str,\n",
    "    filters: List = None,\n",
    "    columns: List = None,\n",
    "    hem_cleaning: bool = True,\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Loads events with an optional filter.\n",
    "    Divides MC samples by the totla before skimming, to take the acceptance into account.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): path to data directory.\n",
    "        samples (Dict[str, str]): dictionary of samples and selectors to load.\n",
    "        year (str): year.\n",
    "        filters (List): Optional filters when loading data.\n",
    "        columns (List): Optional columns to load.\n",
    "        hem_cleaning (bool): Whether to apply HEM cleaning to 2018 data.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, pd.DataFrame]: ``events_dict`` dictionary of events dataframe for each sample.\n",
    "\n",
    "    \"\"\"\n",
    "    data_dir = Path(data_dir) / year\n",
    "    full_samples_list = listdir(data_dir)  # get all directories in data_dir\n",
    "    events_dict = {}\n",
    "\n",
    "    # label - key of sample in events_dict\n",
    "    # selector - string used to select directories to load in for this sample\n",
    "    for label, selector in samples.items():\n",
    "        events_dict[label] = []  # list of directories we load in for this sample\n",
    "        for sample in full_samples_list:\n",
    "            # check if this directory passes our selector string\n",
    "            if not utils.check_selector(sample, selector):\n",
    "                continue\n",
    "\n",
    "            sample_path = data_dir / sample\n",
    "            parquet_path, pickles_path = sample_path / \"parquet\", sample_path / \"pickles\"\n",
    "\n",
    "            # no parquet directory?\n",
    "            if not parquet_path.exists():\n",
    "                warnings.warn(f\"No parquet directory for {sample}!\")\n",
    "                continue\n",
    "\n",
    "            # print(f\"Loading {sample}\")\n",
    "            events = pd.read_parquet(parquet_path, filters=filters, columns=columns)\n",
    "\n",
    "            # no events?\n",
    "            if not len(events):\n",
    "                warnings.warn(f\"No events for {sample}!\")\n",
    "                continue\n",
    "\n",
    "            # normalize by total events\n",
    "            totals = get_pickles(pickles_path, year, sample)[\"totals\"]\n",
    "            normalize_weights(events, totals, sample, isData=label == data_key)\n",
    "\n",
    "            if year == \"2018\" and hem_cleaning:\n",
    "                events = utils._hem_cleaning(sample, events)\n",
    "\n",
    "            events_dict[label].append(events)\n",
    "            print(f\"Loaded {sample: <50}: {len(events)} entries\")\n",
    "\n",
    "        if len(events_dict[label]):\n",
    "            events_dict[label] = pd.concat(events_dict[label])\n",
    "        else:\n",
    "            del events_dict[label]\n",
    "\n",
    "    return events_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_dict = load_samples(\"../../../tmp/test_outputs\", samples, \"2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(events.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = events_dict[\"HHbbVV\"]\n",
    "\n",
    "for column in events:\n",
    "    if \"weight\" in column[0] or \"Weight\" in column[0]:\n",
    "        print(f\"{column!s:<50} {np.sum(events[column]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_idx = [0, 1, 3, 5, 7, 8, 4]\n",
    "events = events_dict[\"TT SL\"]\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "for i, idx in enumerate(sw_idx):\n",
    "    plt.rcParams.update({\"font.size\": 20})\n",
    "    plt.hist(\n",
    "        events[\"ak8FatJetParticleNetMass\"][0].values,\n",
    "        np.arange(60, 260, 20),\n",
    "        histtype=\"step\",\n",
    "        weights=events[\"scale_weights\"][i],\n",
    "        label=f\"Scale Weight {idx}\",\n",
    "    )\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel(\"Events\")\n",
    "plt.xlabel(\"FatJet 1 Regressed Mass (GeV)\")\n",
    "plt.title(\"2017 ggF TT SL\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "for i in range(10):\n",
    "    plt.rcParams.update({\"font.size\": 20})\n",
    "    plt.hist(\n",
    "        events[\"ak8FatJetParticleNetMass\"][0].values,\n",
    "        np.arange(60, 260, 20),\n",
    "        histtype=\"step\",\n",
    "        weights=events[\"pdf_weights\"][i],\n",
    "    )\n",
    "\n",
    "# plt.legend()\n",
    "plt.ylabel(\"Events\")\n",
    "plt.xlabel(\"FatJet 1 Regressed Mass (GeV)\")\n",
    "plt.title(\"2017 ggF HHbbVV PDF Variations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expect 31fb * 40fb-1 * 0.58 * 0.24 * (0.67 ** 2) * 2 * 1.8% acceptance\n",
    "31 * 41 * 0.018 * 0.58 * 0.24 * (0.67**2) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pickles(\"../../../tmp/test_outputs/2017/hhbbvv/pickles\", \"2017\", \"GluGluToHHTobbVV_node_cHHH1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "642 / 35652"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
