{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import plotting\n",
    "import postprocessing\n",
    "import corrections\n",
    "\n",
    "from utils import CUT_MAX_VAL\n",
    "from hh_vars import (\n",
    "    years,\n",
    "    data_key,\n",
    "    qcd_key,\n",
    "    bg_keys,\n",
    "    samples,\n",
    "    nonres_sig_keys,\n",
    "    # res_samples,\n",
    "    # res_sig_keys,\n",
    "    nonres_samples,\n",
    "    txbb_wps,\n",
    "    jec_shifts,\n",
    "    jmsr_shifts,\n",
    ")\n",
    "from postprocessing import res_shape_vars\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle, json\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "from hist import Hist\n",
    "\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from inspect import cleandoc\n",
    "import warnings\n",
    "\n",
    "# ignore these because they don't seem to apply\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# low mass list\n",
    "low_m_higgs = np.array(\n",
    "    [\n",
    "        15,\n",
    "        20,\n",
    "        25,\n",
    "        30,\n",
    "        40,\n",
    "        50,\n",
    "        60,\n",
    "        70,\n",
    "        80,\n",
    "        90,\n",
    "        100,\n",
    "        110,\n",
    "        120,\n",
    "        130,\n",
    "        140,\n",
    "        150,\n",
    "        160,\n",
    "        170,\n",
    "        180,\n",
    "        190,\n",
    "        200,\n",
    "        210,\n",
    "        220,\n",
    "        230,\n",
    "        240,\n",
    "        250,\n",
    "    ]\n",
    ")\n",
    "low_m_res = np.arange(600, 6000, 100)\n",
    "\n",
    "# high mass list\n",
    "m_higgs = np.arange(260, 660, 10)\n",
    "# # minimum m_res changes s.t. it is always > 2x m_higgs to avoid off-shell Higgses\n",
    "# m_res_min = np.linspace(600, 1600, len(m_higgs))\n",
    "\n",
    "# reweight points such that there are the same number of events at 260 as 250 GeV\n",
    "# and then continuously decrease the weight from there till 650 GeV\n",
    "num_low_points = len(low_m_higgs)\n",
    "num_high_points = len(m_higgs)\n",
    "\n",
    "# solve system of equations s.t. 1) total weight sums to 1, and 2) the first weight is 1 / (# of low points) i.e. same # of events as 260 GeV\n",
    "m = np.array([[num_high_points, num_high_points * (num_high_points - 1) / 2], [1, num_high_points]])\n",
    "b = np.array([1, 1 / num_low_points])\n",
    "# a is smallest weight, d is spacing between weights\n",
    "a, d = np.linalg.inv(m).dot(b)\n",
    "\n",
    "\n",
    "def mh_weight(mh):\n",
    "    idx = np.where(m_higgs == mh)[0][0]\n",
    "    return a + d * (len(m_higgs) - idx - 1)\n",
    "\n",
    "\n",
    "def mres_min(mh):\n",
    "    \"\"\"Choose mX for mH s.t. mX^2 - 4mH^2 remains constant for each mH\"\"\"\n",
    "    mdel = 600**2 - 4 * 250**2\n",
    "    return np.sqrt(mdel + 4 * mh**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mh in m_higgs:\n",
    "    m_res = np.linspace(mres_min(mh), mres_min(mh) * 10, len(low_m_res), endpoint=False)\n",
    "    for mx in m_res:\n",
    "        print(\"BulkGravitonToHH_MX%.0f_MH%.0f weight %.5f\" % (mx, mh, mh_weight(mh)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_samples = OrderedDict()\n",
    "\n",
    "res_mps = [\n",
    "    (1000, 125),\n",
    "    (1400, 125),\n",
    "    (1400, 150),\n",
    "    (1800, 125),\n",
    "    (1800, 150),\n",
    "    (1800, 190),\n",
    "    (2200, 125),\n",
    "    (2200, 150),\n",
    "    (2200, 190),\n",
    "    (2200, 250),\n",
    "    (3000, 125),\n",
    "    (3000, 150),\n",
    "    (3000, 190),\n",
    "    (3000, 250),\n",
    "    (3000, 350),\n",
    "]\n",
    "\n",
    "for mX, mY in res_mps:\n",
    "    res_samples[\n",
    "        f\"X[{mX}]->H(bb)Y[{mY}](VV)\"\n",
    "    ] = f\"NMSSM_XToYH_MX{mX}_MY{mY}_HTo2bYTo2W_hadronicDecay\"\n",
    "\n",
    "res_sig_keys = list(res_samples.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del nonres_samples[\"VBFHHbbVV\"]\n",
    "samples = nonres_samples | samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_split_points = [\n",
    "    [\n",
    "        (1000, 125),\n",
    "        (1400, 125),\n",
    "        (1800, 125),\n",
    "        (2200, 125),\n",
    "        (3000, 125),\n",
    "    ],\n",
    "    [\n",
    "        (1400, 150),\n",
    "        (1800, 150),\n",
    "        (1800, 190),\n",
    "        (2200, 150),\n",
    "        (2200, 190),\n",
    "        (2200, 250),\n",
    "    ],\n",
    "    [\n",
    "        (3000, 125),\n",
    "        (3000, 150),\n",
    "        (3000, 190),\n",
    "        (3000, 250),\n",
    "        (3000, 350),\n",
    "    ],\n",
    "]\n",
    "\n",
    "sig_splits = [\n",
    "    [\"HHbbVV\"] + [f\"X[{mX}]->H(bb)Y[{mY}](VV)\" for (mX, mY) in mps] for mps in sig_split_points\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_splits = [res_sig_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = \"../../../\"\n",
    "samples_dir = f\"{MAIN_DIR}/../data/skimmer/Feb24\"\n",
    "signal_samples_dir = f\"{MAIN_DIR}/../data/skimmer/Mar10_2\"\n",
    "year = \"2017\"\n",
    "\n",
    "date = \"23May21\"\n",
    "plot_dir = f\"../../../plots/PostProcessing/{date}/\"\n",
    "templates_dir = f\"templates/{date}/\"\n",
    "for i in range(len(sig_splits)):\n",
    "    _ = os.system(f\"mkdir -p {plot_dir}/ControlPlots/{year}/sigs{i}/\")\n",
    "_ = os.system(f\"mkdir -p {plot_dir}/cutflows/\")\n",
    "_ = os.system(f\"mkdir -p {plot_dir}/templates/wshifts\")\n",
    "_ = os.system(f\"mkdir -p {plot_dir}/templates/jshifts\")\n",
    "_ = os.system(f\"mkdir -p {plot_dir}/templates/hists2d\")\n",
    "_ = os.system(f\"mkdir -p {templates_dir}\")\n",
    "\n",
    "selection_regions = postprocessing.get_res_selection_regions(year)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both Jet's Regressed Mass above 50, electron veto\n",
    "filters = [\n",
    "    [\n",
    "        (\"('ak8FatJetParticleNetMass', '0')\", \">=\", 50),\n",
    "        (\"('ak8FatJetParticleNetMass', '1')\", \">=\", 50),\n",
    "        # (\"('nGoodElectrons', '0')\", \"==\", 0),\n",
    "    ],\n",
    "]\n",
    "systematics = {year: {}}\n",
    "\n",
    "# save cutflow as pandas table\n",
    "cutflow = pd.DataFrame(index=list(samples.keys()) + list(res_samples.keys()))\n",
    "\n",
    "# utils.remove_empty_parquets(samples_dir, year)\n",
    "events_dict = utils.load_samples(signal_samples_dir, res_samples, year, filters)\n",
    "events_dict |= utils.load_samples(samples_dir, samples, year, filters)\n",
    "\n",
    "utils.add_to_cutflow(events_dict, \"Preselection\", \"weight\", cutflow)\n",
    "\n",
    "print(\"\")\n",
    "# print weighted sample yields\n",
    "for sample in events_dict:\n",
    "    tot_weight = np.sum(events_dict[sample][\"weight\"].values)\n",
    "    print(f\"Pre-selection {sample} yield: {tot_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "higgs_samples = OrderedDict(\n",
    "    [\n",
    "        (\"Hbb\", \"*HToBB\"),\n",
    "        # (\"HWW\", (\"*HToWW\", \"*HToNonbb\")),\n",
    "        # (\"HH\", (\"VBF_HHTobbVV_CV_1_C2V_1_C3_1\", \"GluGluToHHTo4B_node_cHHH1_preUL\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "events_dict |= utils.load_samples(samples_dir, higgs_samples, year, filters)\n",
    "\n",
    "cutflow = pd.DataFrame(\n",
    "    index=list(samples.keys()) + list(res_samples.keys()) + list(higgs_samples.keys())\n",
    ")\n",
    "utils.add_to_cutflow(events_dict, \"Preselection\", \"weight\", cutflow)\n",
    "\n",
    "print(\"\")\n",
    "# print weighted sample yields\n",
    "for sample in events_dict:\n",
    "    tot_weight = np.sum(events_dict[sample][\"weight\"].values)\n",
    "    print(f\"Pre-selection {sample} yield: {tot_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bins = np.arange(-20, 61, 5)\n",
    "plt.hist(events_dict[\"ST\"][\"weight\"], bins, histtype=\"step\", label=\"ST\")\n",
    "plt.hist(events_dict[\"TT\"][\"weight\"], bins, histtype=\"step\", label=\"TT\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"# Events\")\n",
    "plt.xlabel(\"Weights\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{plot_dir}/sttt_weights.pdf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale factors and bb VV assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessing.apply_weights(events_dict, year, cutflow)\n",
    "bb_masks = postprocessing.bb_VV_assignment(events_dict)\n",
    "cutflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample, events in events_dict.items():\n",
    "    if \"ak8FatJetParTMD_THWWvsT\" not in events:\n",
    "        h4qvst = (events[\"ak8FatJetParTMD_probHWW3q\"] + events[\"ak8FatJetParTMD_probHWW4q\"]) / (\n",
    "            events[\"ak8FatJetParTMD_probHWW3q\"]\n",
    "            + events[\"ak8FatJetParTMD_probHWW4q\"]\n",
    "            + events[\"ak8FatJetParTMD_probQCD\"]\n",
    "            + events[\"ak8FatJetParTMD_probT\"]\n",
    "        )\n",
    "\n",
    "        events_dict[sample] = pd.concat(\n",
    "            [events, pd.concat([h4qvst], axis=1, keys=[\"ak8FatJetParTMD_THWWvsT\"])], axis=1\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Control Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {var: (bins, label)}\n",
    "control_plot_vars = {\n",
    "    # \"MET_pt\": ([50, 0, 300], r\"$p^{miss}_T$ (GeV)\"),\n",
    "    # \"DijetEta\": ([50, -8, 8], r\"$\\eta^{jj}$\"),\n",
    "    # \"DijetPt\": ([50, 0, 750], r\"$p_T^{jj}$ (GeV)\"),\n",
    "    # \"DijetMass\": (\n",
    "    #     # list(range(800, 1400, 100)) + [1400, 1600, 2000, 3000, 4400],\n",
    "    #     [40, 600, 4500],\n",
    "    #     r\"$m^{jj}$ (GeV)\",\n",
    "    # ),\n",
    "    # \"bbFatJetEta\": ([50, -2.4, 2.4], r\"$\\eta^{bb}$\"),\n",
    "    # \"bbFatJetPt\": ([50, 300, 1500], r\"$p^{bb}_T$ (GeV)\"),\n",
    "    # \"bbFatJetParticleNetMass\": ([40, 52.5, 252.5], r\"$m^{bb}_{reg}$ (GeV)\"),\n",
    "    # \"bbFatJetMsd\": ([50, 0, 300], r\"$m^{bb}_{msd}$ (GeV)\"),\n",
    "    # \"bbFatJetParticleNetMD_Txbb\": ([50, 0.8, 1], r\"$T^{bb}_{Xbb}$\"),\n",
    "    # \"VVFatJetEta\": ([50, -2.4, 2.4], r\"$\\eta^{VV}$\"),\n",
    "    # \"VVFatJetPt\": ([50, 300, 1500], r\"$p^{VV}_T$ (GeV)\"),\n",
    "    # \"VVFatJetParticleNetMass\": (\n",
    "    #     # list(range(50, 110, 10)) + list(range(110, 200, 15)) + [200, 220, 250],\n",
    "    #     [20, 50, 250],\n",
    "    #     r\"$m^{VV}_{reg}$ (GeV)\",\n",
    "    # ),\n",
    "    # \"VVFatJetMsd\": ([40, 50, 250], r\"$m^{VV}_{msd}$ (GeV)\"),\n",
    "    # \"VVFatJetParticleNet_Th4q\": ([50, 0, 1], r\"Prob($H \\to 4q$) vs Prob(QCD) (Non-MD)\"),\n",
    "    # \"VVFatJetParTMD_THWW4q\": (\n",
    "    #     [50, 0, 1],\n",
    "    #     r\"Prob($H \\to VV \\to 4q$) vs Prob(QCD) (Mass-Decorrelated)\",\n",
    "    # ),\n",
    "    # \"VVFatJetParTMD_probT\": ([50, 0, 1], r\"Prob(Top) (Mass-Decorrelated)\"),\n",
    "    # \"VVFatJetParTMD_THWWvsT\": (\n",
    "    #     [50, 0, 1],\n",
    "    #     r\"$T^{VV}_{HWW}$\",\n",
    "    # ),\n",
    "    # \"bbFatJetPtOverDijetPt\": ([50, 0, 40], r\"$p^{bb}_T / p_T^{jj}$\"),\n",
    "    # \"VVFatJetPtOverDijetPt\": ([50, 0, 40], r\"$p^{VV}_T / p_T^{jj}$\"),\n",
    "    # \"VVFatJetPtOverbbFatJetPt\": ([50, 0.4, 2.0], r\"$p^{VV}_T / p^{bb}_T$\"),\n",
    "    \"nGoodMuons\": ([3, 0, 3], r\"# of Muons\"),\n",
    "    \"nGoodElectrons\": ([3, 0, 3], r\"# of Electrons\"),\n",
    "    \"nGoodJets\": ([5, 0, 5], r\"# of AK4 B-Jets\"),\n",
    "}\n",
    "\n",
    "# hists = postprocessing.control_plots(\n",
    "#     events_dict,\n",
    "#     bb_masks,\n",
    "#     nonres_sig_keys + res_sig_keys,\n",
    "#     control_plot_vars,\n",
    "#     f\"{plot_dir}/ControlPlots/{year}/\",\n",
    "#     year,\n",
    "#     sig_splits=sig_splits,\n",
    "#     # bg_keys=bg_keys + list(higgs_samples.keys()),\n",
    "#     bg_keys=[\"QCD\", \"TT\", \"ST\", \"V+Jets\", \"Hbb\"],\n",
    "#     show=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection, _ = utils.make_selection(\n",
    "    {\n",
    "        \"VVFatJetParTMD_THWWvsT\": [0.8, CUT_MAX_VAL],\n",
    "        \"bbFatJetParticleNetMD_Txbb\": [0.98, CUT_MAX_VAL],\n",
    "        \"bbFatJetParticleNetMass\": [110, 145],\n",
    "    },\n",
    "    events_dict,\n",
    "    bb_masks,\n",
    ")\n",
    "cutstr = f\"pass_noveto\"\n",
    "\n",
    "postprocessing.control_plots(\n",
    "    events_dict,\n",
    "    bb_masks,\n",
    "    nonres_sig_keys + res_sig_keys,\n",
    "    control_plot_vars,\n",
    "    f\"{plot_dir}/ControlPlots/{year}/\",\n",
    "    year,\n",
    "    sig_splits=sig_splits[:1],\n",
    "    hists={},\n",
    "    # bg_keys=bg_keys + list(higgs_samples.keys()),\n",
    "    # bg_keys=[\"QCD\", \"TT\", \"ST\", \"V+Jets\", \"Hbb\"],\n",
    "    bg_keys=[\"QCD\", \"TT\", \"ST\", \"V+Jets\"],\n",
    "    sig_scale_dict={key: 10 for key in nonres_sig_keys + res_sig_keys},\n",
    "    selection=selection,\n",
    "    cutstr=cutstr,\n",
    "    show=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sb1, sb2 in [[0, 300], [75, 180], [92.5, 162.5]]:\n",
    "    selection, _ = utils.make_selection(\n",
    "        {\n",
    "            \"VVFatJetParTMD_THWWvsT\": [0.8, CUT_MAX_VAL],\n",
    "            \"bbFatJetParticleNetMD_Txbb\": [0.98, CUT_MAX_VAL],\n",
    "            \"bbFatJetParticleNetMass\": [[sb1, 110], [145, sb2]],\n",
    "        },\n",
    "        events_dict,\n",
    "        bb_masks,\n",
    "    )\n",
    "    cutstr = f\"sidebands_{sb1}_{sb2}\"\n",
    "\n",
    "    postprocessing.control_plots(\n",
    "        events_dict,\n",
    "        bb_masks,\n",
    "        nonres_sig_keys + res_sig_keys,\n",
    "        control_plot_vars,\n",
    "        f\"{plot_dir}/ControlPlots/{year}/\",\n",
    "        year,\n",
    "        sig_splits=sig_splits,\n",
    "        hists={},\n",
    "        # bg_keys=bg_keys + list(higgs_samples.keys()),\n",
    "        bg_keys=[\"QCD\", \"TT\", \"ST\", \"V+Jets\", \"Hbb\"],\n",
    "        selection=selection,\n",
    "        cutstr=cutstr,\n",
    "        show=True,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall LP SF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "sel, cf = utils.make_selection(\n",
    "    selection_regions[\"lpsf\"].cuts, events_dict, bb_masks, prev_cutflow=cutflow\n",
    ")\n",
    "\n",
    "sf_table = OrderedDict()\n",
    "\n",
    "for sig_key in tqdm(res_sig_keys):\n",
    "    systematics[sig_key] = {}\n",
    "    # calculate only for current year\n",
    "    events_dict[sig_key] = postprocessing.postprocess_lpsfs(events_dict[sig_key])\n",
    "    lp_sf, unc, uncs = postprocessing.get_lpsf(events_dict[sig_key], sel[sig_key])\n",
    "    # print(f\"BDT LP Scale Factor for {sig_key}: {lp_sf:.2f} ± {unc:.2f}\")\n",
    "    # print(uncs)\n",
    "\n",
    "    systematics[sig_key][\"lp_sf\"] = lp_sf\n",
    "    systematics[sig_key][\"lp_sf_unc\"] = unc / lp_sf\n",
    "\n",
    "    sf_table[sig_key] = {\"SF\": f\"{lp_sf:.2f} ± {unc:.2f}\", **uncs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_df = pd.DataFrame(index=nonres_sig_keys + res_sig_keys)\n",
    "\n",
    "for key in sf_table[sig_key]:\n",
    "    sf_df[key] = [sf_table[skey][key] for skey in nonres_sig_keys + res_sig_keys]\n",
    "\n",
    "sf_df.to_clipboard()\n",
    "sf_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_regions = postprocessing.get_res_selection_regions(\"2017\", txbb_wp=\"HP\", thww_wp=0.8)\n",
    "del selection_regions[\"fail\"], selection_regions[\"failBlinded\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = postprocessing.get_templates(\n",
    "    events_dict,\n",
    "    bb_masks,\n",
    "    year,\n",
    "    # nonres_sig_keys + res_sig_keys[:10],\n",
    "    res_sig_keys,\n",
    "    selection_regions,\n",
    "    res_shape_vars[:1],\n",
    "    systematics,\n",
    "    templates_dir,\n",
    "    bg_keys=[\"QCD\", \"TT\", \"V+Jets\", \"Diboson\", \"Hbb\"],\n",
    "    plot_dir=f\"{plot_dir}/templates/\",\n",
    "    prev_cutflow=cutflow,\n",
    "    # sig_splits=sig_splits[:2],\n",
    "    weight_shifts={},\n",
    "    jshift=\"\",\n",
    "    plot_shifts=False,\n",
    "    pass_ylim=70,\n",
    "    fail_ylim=40000,\n",
    "    blind_pass=True,\n",
    "    show=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_templates(\n",
    "    events_dict,\n",
    "    bb_masks,\n",
    "    args.year,\n",
    "    sig_keys,\n",
    "    selection_regions,\n",
    "    shape_vars,\n",
    "    systematics,\n",
    "    template_dir,\n",
    "    bg_keys=bg_keys,\n",
    "    plot_dir=plot_dir,\n",
    "    prev_cutflow=cutflow,\n",
    "    # sig_splits=sig_splits,\n",
    "    weight_shifts=weight_shifts,\n",
    "    jshift=jshift,\n",
    "    blind_pass=True if args.resonant else False,\n",
    "    show=False,\n",
    "    plot_shifts=args.plot_shifts,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {}\n",
    "\n",
    "for jshift in [\"\"] + jec_shifts + jmsr_shifts:\n",
    "    print(jshift)\n",
    "    ttemps, tsyst = postprocessing.get_templates(\n",
    "        events_dict,\n",
    "        bb_masks,\n",
    "        year,\n",
    "        nonres_sig_keys + res_sig_keys,\n",
    "        res_selection_regions[year],\n",
    "        res_shape_vars,\n",
    "        bg_keys=[\"QCD\", \"TT\", \"V+Jets\"],\n",
    "        plot_dir=plot_dir if jshift == \"\" else \"\",\n",
    "        prev_cutflow=cutflow,\n",
    "        sig_splits=sig_splits,\n",
    "        weight_shifts=postprocessing.weight_shifts,\n",
    "        jshift=jshift,\n",
    "        pass_ylim=7,\n",
    "        fail_ylim=40000,\n",
    "        blind_pass=True,\n",
    "        show=False,\n",
    "        plot_shifts=False,\n",
    "    )\n",
    "\n",
    "    templates = {**templates, **ttemps}\n",
    "    if jshift == \"\":\n",
    "        systematics[year] = tsyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{templates_dir}/{year}_templates.pkl\", \"wb\") as f:\n",
    "    pickle.dump(templates, f)\n",
    "\n",
    "with open(f\"{templates_dir}/systematics.json\", \"w\") as f:\n",
    "    json.dump(systematics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"templates/Apr10//2017_templates.pkl\", \"rb\") as f:\n",
    "    templates = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(templates[\"pass\"].axes[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.hist2ds(\n",
    "    templates,\n",
    "    f\"{plot_dir}/templates/hists2d/\",\n",
    "    regions=[\"pass\", \"fail\", \"passBlinded\", \"failBlinded\"],\n",
    "    region_labels=selection_regions_label,\n",
    "    samples=[\"Data\", \"TT\", \"V+Jets\", \"X[3000]->H(bb)Y[190](VV)\"],\n",
    "    # fail_zlim=5e3,\n",
    "    # pass_zlim=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "systematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates_dict = {}\n",
    "\n",
    "for year in years:\n",
    "    with open(f\"templates/{date}/{year}_templates.pkl\", \"rb\") as f:\n",
    "        templates_dict[year] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = []\n",
    "for year in years:\n",
    "    with open(f\"templates/Apr7//{year}_templates.pkl\", \"rb\") as f:\n",
    "        templates.append(pickle.load(f))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15adc7883e707560d0d9727709639b8fe3f3cff1f197d2d643742923ff23a29c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
