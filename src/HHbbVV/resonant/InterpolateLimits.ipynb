{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse, combine and interpolate limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import mplhep as hep\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import interpolate\n",
    "from tqdm import tqdm\n",
    "\n",
    "from HHbbVV.hh_vars import res_sigs, years, LUMI\n",
    "from HHbbVV.postprocessing import plotting, utils\n",
    "from HHbbVV.postprocessing.utils import mxmy\n",
    "from HHbbVV.resonant import ProcessLimits\n",
    "from HHbbVV.resonant.ProcessLimits import get_lim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = \"../../../\"\n",
    "plot_dir = Path(f\"{MAIN_DIR}/plots/XHY/Limits/25Apr22SigEffs\")\n",
    "plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cards_dir = Path(\"/eos/uscms/store/user/rkansal/bbVV/cards/25Mar29QCDTF11nTF21\")\n",
    "templates_dir = Path(\"/ceph/cms/store/user/rkansal/bbVV/templates/25Feb8XHYFix\")\n",
    "# cards_dir = Path(\"/eos/uscms/store/user/rkansal/bbVV/cards/25Feb19ResUnblinded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_map(key):\n",
    "    if key == \"50.0\":\n",
    "        label = \"Median expected exclusion limits (fb)\"\n",
    "    elif key == \"Observed\":\n",
    "        label = \"Exclusion limits (fb)\"\n",
    "    elif key == \"Significance\":\n",
    "        label = \"Local Significance\"\n",
    "    else:\n",
    "        label = f\"{key}% expected exclusion limits (fb)\"\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load / process limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "limits = ProcessLimits.get_limits(cards_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Amitav's limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alimits_path = Path(\n",
    "    \"/uscms/home/ammitra/nobackup/2DAlphabet/fitting/CMSSW_14_1_0_pre4/src/XHYbbWW/limits/\"\n",
    ")\n",
    "alimits = {\n",
    "    \" 2.5\": [],\n",
    "    \"16.0\": [],\n",
    "    \"50.0\": [],\n",
    "    \"84.0\": [],\n",
    "    \"97.5\": [],\n",
    "    \"Observed\": [],\n",
    "    \"Significance\": [],\n",
    "}\n",
    "key_map = {\n",
    "    # mine: amitav's\n",
    "    \" 2.5\": \"limits_Minus2\",\n",
    "    \"16.0\": \"limits_Minus1\",\n",
    "    \"50.0\": \"limits_Expected\",\n",
    "    \"84.0\": \"limits_Plus1\",\n",
    "    \"97.5\": \"limits_Plus2\",\n",
    "    \"Observed\": \"limits_OBSERVED\",\n",
    "    \"Significance\": \"significance\",\n",
    "}\n",
    "\n",
    "for mkey, akey in key_map.items():\n",
    "    try:\n",
    "        alimits[mkey] = pd.read_csv(alimits_path / f\"{akey}.csv\").values[:, 1:]\n",
    "    except:\n",
    "        print(f\"{alimits_path}/{akey}.csv not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min expected limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(limits[\"50.0\"][:, 2]))\n",
    "print(np.min(alimits[\"50.0\"][:, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking excesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twosigma = limits[\"97.5\"][:, 2] < limits[\"Observed\"][:, 2]\n",
    "\n",
    "for i in range(np.sum(twosigma)):\n",
    "    mx, my = limits[\"50.0\"][twosigma][i][:2]\n",
    "    print(\n",
    "        f\"({mx}, {my}):\\t Expected {limits['50.0'][twosigma][i, 2]}+{limits['97.5'][twosigma][i, 2]}\\t Observed {limits['Observed'][twosigma][i, 2]:.2f}\\t Sign {limits['Significance'][twosigma][i, 2]:.2f}\"\n",
    "    )\n",
    "\n",
    "# print(limits[\"50.0\"][twosigma], limits[\"97.5\"][twosigma], limits[\"Observed\"][twosigma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alimits[\"Significance\"][np.argmax(alimits[\"Significance\"][:, 2])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process and plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosted alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymax = 600\n",
    "mxs = np.logspace(np.log10(900), np.log10(3999), 100, base=10)\n",
    "mys = np.logspace(np.log10(60), np.log10(mymax), 100, base=10)\n",
    "\n",
    "xx, yy = np.meshgrid(mxs, mys)\n",
    "\n",
    "interpolated = {}\n",
    "grids = {}\n",
    "\n",
    "for key, val in limits.items():\n",
    "    interpolated[key] = interpolate.LinearNDInterpolator(val[:, :2], np.log(val[:, 2]))\n",
    "    grids[key] = np.exp(interpolated[key](xx, yy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, grid in grids.items():\n",
    "    if key != \"Significance\":\n",
    "        continue\n",
    "\n",
    "    if key == \"Significance\":\n",
    "        vmin, vmax, log = 0, 5, False\n",
    "    else:\n",
    "        vmin, vmax, log = 0.05, 1e4, True\n",
    "\n",
    "    plotting.colormesh(\n",
    "        xx,\n",
    "        yy,\n",
    "        grid,\n",
    "        label_map(key),\n",
    "        f\"{plot_dir}/upper{mymax}_mesh_{key}_turbo.pdf\",\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "        log=log,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in limits.items():\n",
    "    if key != \"Significance\":\n",
    "        continue\n",
    "\n",
    "    if key == \"Significance\":\n",
    "        vmin, vmax, log = 0, 5, False\n",
    "    else:\n",
    "        vmin, vmax, log = 0.05, 1e4, True\n",
    "\n",
    "    plotting.XHYscatter2d(val, label_map(key), name=f\"{plot_dir}/scatter_{key}.pdf\", show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check whose expected limit is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_better = []\n",
    "alim_med = alimits[\"50.0\"]\n",
    "\n",
    "for mx, my, lim in limits[\"50.0\"]:\n",
    "    match = (alim_med[:, 0] == mx) * (alim_med[:, 1] == my)\n",
    "    if np.any(match):\n",
    "        alim = float(alim_med[:, 2][match])\n",
    "\n",
    "    if alim < lim:\n",
    "        pbetter = (lim - alim) / lim\n",
    "        print(f\"Semiboosted better for ({mx}, {my}) by {pbetter * 100:.2f}%\")\n",
    "        sb_better.append([mx, my, pbetter])\n",
    "\n",
    "sb_better = np.array(sb_better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.scatter2d_overlay(\n",
    "    limits[\"50.0\"],\n",
    "    sb_better,\n",
    "    f\"Median expected exclusion limits (fb)\",\n",
    "    f\"{plot_dir}/scatter_overlay.pdf\",\n",
    "    show=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_limits = {\n",
    "    \" 2.5\": [],\n",
    "    \"16.0\": [],\n",
    "    \"50.0\": [],\n",
    "    \"84.0\": [],\n",
    "    \"97.5\": [],\n",
    "    \"Observed\": [],\n",
    "    \"Significance\": [],\n",
    "}\n",
    "alim_med = alimits[\"50.0\"]\n",
    "blim_med = limits[\"50.0\"]\n",
    "\n",
    "checked_mxmy = []\n",
    "\n",
    "for mxy in np.vstack((alim_med, blim_med))[:, :2]:\n",
    "    mx, my = mxy\n",
    "    mxy = (int(mxy[0]), int(mxy[1]))\n",
    "    if mx < 900:\n",
    "        continue\n",
    "\n",
    "    if mxy in checked_mxmy:\n",
    "        continue\n",
    "\n",
    "    amatch, alim = get_lim(alim_med, mxy)\n",
    "    bmatch, blim = get_lim(blim_med, mxy)\n",
    "\n",
    "    alim = alim[0, 2] if np.any(amatch) else np.inf\n",
    "    blim = blim[0, 2] if np.any(bmatch) else np.inf\n",
    "\n",
    "    if alim < blim and (my < 200):\n",
    "        # skipping samples for which 2018 PFNano failed !! :(\n",
    "        print(f\"Skipping {mxy} because of missing PFNano!\")\n",
    "        continue\n",
    "\n",
    "    if blim < alim and (my > (134.5 + mx * 0.1285)):\n",
    "        print(f\"Skipping {mxy} because of missing from Amitav's limits!\")\n",
    "        continue\n",
    "\n",
    "    use_lims = alimits if alim < blim else limits\n",
    "\n",
    "    for key, lims in combined_limits.items():\n",
    "        umatch, lim = get_lim(use_lims[key], mxy)\n",
    "        if np.any(umatch):\n",
    "            lims.append([*mxy, use_lims[key][umatch][0, 2]])\n",
    "        else:\n",
    "            print(f\"Missing {mxy} for {key}!\")\n",
    "\n",
    "    checked_mxmy.append(mxy)\n",
    "\n",
    "for key, val in combined_limits.items():\n",
    "    combined_limits[key] = np.array(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = {}\n",
    "\n",
    "combined_df[\"MX\"] = combined_limits[\"Observed\"][:, 0]\n",
    "combined_df[\"MY\"] = combined_limits[\"Observed\"][:, 1]\n",
    "\n",
    "for key, val in combined_limits.items():\n",
    "    if key != \"Observed\":\n",
    "        combined_df[f\"Expected {key}\"] = val[:, 2]\n",
    "    else:\n",
    "        combined_df[key] = val[:, 2]\n",
    "\n",
    "\n",
    "pd.DataFrame(combined_df).to_csv(cards_dir / \"combined_limits.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argmax(alimits[\"Significance\"][:, 2])\n",
    "print(alimits[\"Significance\"][idx])\n",
    "idx = np.argmax(limits[\"Significance\"][:, 2])\n",
    "print(limits[\"Significance\"][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking excesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twosigma = combined_limits[\"97.5\"][:, 2] < combined_limits[\"Observed\"][:, 2]\n",
    "\n",
    "for i in range(np.sum(twosigma)):\n",
    "    mx, my = combined_limits[\"50.0\"][twosigma][i][:2]\n",
    "    print(\n",
    "        f\"({mx}, {my}): Expected {combined_limits['50.0'][twosigma][i, 2]}+{combined_limits['97.5'][twosigma][i, 2]}\\t Observed {combined_limits['Observed'][twosigma][i, 2]:.2f}\\t Sign {combined_limits['Significance'][twosigma][i, 2]:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxs = np.logspace(np.log10(800), np.log10(3999), 300, base=10)\n",
    "mys = np.logspace(np.log10(60), np.log10(2800), 300, base=10)\n",
    "cxx, cyy = np.meshgrid(mxs, mys)\n",
    "\n",
    "for key, val in combined_limits.items():\n",
    "    if key != \"Significance\":\n",
    "        continue\n",
    "\n",
    "    if key == \"Significance\":\n",
    "        vmin, vmax, log = 0, 5, False\n",
    "    else:\n",
    "        vmin, vmax, log = 0.05, 1e4, True\n",
    "\n",
    "    interpolated = interpolate.LinearNDInterpolator(val[:, :2], np.log(val[:, 2]))\n",
    "    grid = np.exp(interpolated(cxx, cyy))\n",
    "\n",
    "    plotting.colormesh(\n",
    "        cxx,\n",
    "        cyy,\n",
    "        grid,\n",
    "        label_map(key),\n",
    "        f\"{plot_dir}/combined_mesh_{key}.pdf\",\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "        log=log,\n",
    "        figsize=(12, 8),\n",
    "        show=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in combined_limits.items():\n",
    "    if key != \"Significance\":\n",
    "        continue\n",
    "\n",
    "    if key == \"Significance\":\n",
    "        vmin, vmax, log = 0, 5, False\n",
    "    else:\n",
    "        vmin, vmax, log = 0.05, 1e4, True\n",
    "\n",
    "    plotting.XHYscatter2d(\n",
    "        val, label_map(key), name=f\"{plot_dir}/combined_scatter_{key}.pdf\", show=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Efficiencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = res_sigs[0]\n",
    "# pd.read_csv(templates_dir / sig / \"cutflows\" / \"2018\" / \"pass_cutflow.csv\").to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    pd.read_csv(templates_dir / sig / \"cutflows\" / year / \"pass_cutflow.csv\").to_numpy()[0, -1]\n",
    "    for year in years\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cutflows for each signal in templates_dir\n",
    "tot_lumi = sum([LUMI[year] for year in years]) / 1000.0\n",
    "\n",
    "sig_effs = []\n",
    "for sig in tqdm(res_sigs):\n",
    "    mx, my = utils.mxmy(sig)\n",
    "    try:\n",
    "        sig_yield = sum(\n",
    "            [\n",
    "                pd.read_csv(\n",
    "                    templates_dir / sig / \"cutflows\" / year / \"pass_cutflow.csv\"\n",
    "                ).to_numpy()[0, -1]\n",
    "                for year in years\n",
    "            ]\n",
    "        )\n",
    "        sig_eff = sig_yield / tot_lumi\n",
    "        sig_effs.append([mx, my, sig_eff])\n",
    "    except Exception as e:\n",
    "        print(f\"No cutflows found for {sig}!\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.XHYscatter2d(\n",
    "    np.array(sig_effs),\n",
    "    \"Signal efficiency in SR Pass\",\n",
    "    name=f\"{plot_dir}/scatter_sig_eff.pdf\",\n",
    "    show=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
