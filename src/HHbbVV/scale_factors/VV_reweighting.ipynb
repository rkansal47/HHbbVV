{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed loading compiled JECs\n"
     ]
    }
   ],
   "source": [
    "import uproot\n",
    "import awkward as ak\n",
    "from coffea import nanoevents\n",
    "from coffea.nanoevents.methods.base import NanoEventsArray\n",
    "from coffea.analysis_tools import Weights, PackedSelection\n",
    "from coffea.nanoevents.methods import nanoaod\n",
    "from coffea.nanoevents.methods import vector\n",
    "from coffea.lookup_tools.dense_lookup import dense_lookup\n",
    "from coffea.nanoevents.methods.nanoaod import MuonArray, JetArray, FatJetArray, GenParticleArray\n",
    "\n",
    "ak.behavior.update(vector.behavior)\n",
    "\n",
    "import pickle, json, gzip\n",
    "import numpy as np\n",
    "\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "from copy import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "from matplotlib import colors\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import fastjet\n",
    "\n",
    "import pathlib\n",
    "\n",
    "# import jetnet\n",
    "\n",
    "import os\n",
    "\n",
    "import corrections\n",
    "import correctionlib\n",
    "\n",
    "from utils import P4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dir = \"../../../plots/ScaleFactors/Feb1\"\n",
    "_ = os.system(f\"mkdir -p {plot_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raghav/mambaforge/envs/python310/lib/python3.10/site-packages/coffea/nanoevents/mapping/uproot.py:86: UserWarning: Found duplicate branch FatJetAK15SubJet_nBHadrons in <TTree 'Events' (1975 branches) at 0x7fd7a83c02e0>, taking first instance\n",
      "  warnings.warn(\n",
      "/Users/raghav/mambaforge/envs/python310/lib/python3.10/site-packages/coffea/nanoevents/mapping/uproot.py:86: UserWarning: Found duplicate branch FatJetAK15SubJet_nCHadrons in <TTree 'Events' (1975 branches) at 0x7fd7a83c02e0>, taking first instance\n",
      "  warnings.warn(\n",
      "/Users/raghav/mambaforge/envs/python310/lib/python3.10/site-packages/coffea/nanoevents/mapping/uproot.py:86: UserWarning: Found duplicate branch FatJetAK15_nBHadrons in <TTree 'Events' (1975 branches) at 0x7fd7a83c02e0>, taking first instance\n",
      "  warnings.warn(\n",
      "/Users/raghav/mambaforge/envs/python310/lib/python3.10/site-packages/coffea/nanoevents/mapping/uproot.py:86: UserWarning: Found duplicate branch FatJetAK15_nCHadrons in <TTree 'Events' (1975 branches) at 0x7fd7a83c02e0>, taking first instance\n",
      "  warnings.warn(\n",
      "/Users/raghav/mambaforge/envs/python310/lib/python3.10/site-packages/coffea/nanoevents/mapping/uproot.py:86: UserWarning: Found duplicate branch FatJet_btagDDBvLV2 in <TTree 'Events' (1975 branches) at 0x7fd7a83c02e0>, taking first instance\n",
      "  warnings.warn(\n",
      "/Users/raghav/mambaforge/envs/python310/lib/python3.10/site-packages/coffea/nanoevents/mapping/uproot.py:86: UserWarning: Found duplicate branch FatJet_btagDDCvBV2 in <TTree 'Events' (1975 branches) at 0x7fd7a83c02e0>, taking first instance\n",
      "  warnings.warn(\n",
      "/Users/raghav/mambaforge/envs/python310/lib/python3.10/site-packages/coffea/nanoevents/mapping/uproot.py:86: UserWarning: Found duplicate branch FatJet_btagDDCvLV2 in <TTree 'Events' (1975 branches) at 0x7fd7a83c02e0>, taking first instance\n",
      "  warnings.warn(\n",
      "/Users/raghav/mambaforge/envs/python310/lib/python3.10/site-packages/coffea/nanoevents/mapping/uproot.py:86: UserWarning: Found duplicate branch FatJet_nBHadrons in <TTree 'Events' (1975 branches) at 0x7fd7a83c02e0>, taking first instance\n",
      "  warnings.warn(\n",
      "/Users/raghav/mambaforge/envs/python310/lib/python3.10/site-packages/coffea/nanoevents/mapping/uproot.py:86: UserWarning: Found duplicate branch FatJet_nCHadrons in <TTree 'Events' (1975 branches) at 0x7fd7a83c02e0>, taking first instance\n",
      "  warnings.warn(\n",
      "/Users/raghav/mambaforge/envs/python310/lib/python3.10/site-packages/coffea/nanoevents/mapping/uproot.py:86: UserWarning: Found duplicate branch SubJet_nBHadrons in <TTree 'Events' (1975 branches) at 0x7fd7a83c02e0>, taking first instance\n",
      "  warnings.warn(\n",
      "/Users/raghav/mambaforge/envs/python310/lib/python3.10/site-packages/coffea/nanoevents/mapping/uproot.py:86: UserWarning: Found duplicate branch SubJet_nCHadrons in <TTree 'Events' (1975 branches) at 0x7fd7a83c02e0>, taking first instance\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "events = nanoevents.NanoEventsFactory.from_root(\n",
    "    # \"/eos/uscms/store/user/lpcpfnano/cmantill/v2_3/2017/HH_gen/GluGluToHHTobbVV_node_cHHH1_TuneCP5_13TeV-powheg-pythia8/GluGluToHHTobbVV_node_cHHH1/221017_221918/0000/nano_mc2017_100.root\",\n",
    "    \"../../../../data/2017_UL_nano/NMSSM_XToYH_MX1000_MY400_HTo2bYTo2W_hadronicDecay/nano_mc2017_101.root\",\n",
    "    schemaclass=nanoevents.NanoAODSchema,\n",
    ").events()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HHbbVV Pre-selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_val(\n",
    "    arr: ak.Array,\n",
    "    target: int,\n",
    "    value: float,\n",
    "    axis: int = 0,\n",
    "    to_numpy: bool = True,\n",
    "    clip: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    pads awkward array up to ``target`` index along axis ``axis`` with value ``value``,\n",
    "    optionally converts to numpy array\n",
    "    \"\"\"\n",
    "    ret = ak.fill_none(ak.pad_none(arr, target, axis=axis, clip=clip), value, axis=axis)\n",
    "    return ret.to_numpy() if to_numpy else ret\n",
    "\n",
    "\n",
    "def add_selection(\n",
    "    name: str,\n",
    "    sel: np.ndarray,\n",
    "    selection: PackedSelection,\n",
    "    cutflow: dict = None,\n",
    "    isData: bool = False,\n",
    "    signGenWeights: ak.Array = None,\n",
    "):\n",
    "    \"\"\"adds selection to PackedSelection object and the cutflow dictionary\"\"\"\n",
    "    selection.add(name, sel)\n",
    "    if cutflow is not None:\n",
    "        cutflow[name] = (\n",
    "            np.sum(selection.all(*selection.names))\n",
    "            if isData\n",
    "            # add up sign of genWeights for MC\n",
    "            else np.sum(signGenWeights[selection.all(*selection.names)])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "isData = False\n",
    "signGenWeights = None if isData else np.sign(events[\"genWeight\"])\n",
    "n_events = len(events) if isData else int(np.sum(signGenWeights))\n",
    "selection = PackedSelection()\n",
    "\n",
    "cutflow = {}\n",
    "cutflow[\"all\"] = len(events)\n",
    "\n",
    "preselection_cut_vals = {\"pt\": 250, \"msd\": 20}\n",
    "num_jets = 2\n",
    "\n",
    "# fatjets = corrections.get_jec_jets(events, \"2018\")\n",
    "fatjets = events.FatJet\n",
    "\n",
    "preselection_cut = np.prod(\n",
    "    pad_val(\n",
    "        (events.FatJet.pt > preselection_cut_vals[\"pt\"])\n",
    "        * (events.FatJet.msoftdrop > preselection_cut_vals[\"msd\"]),\n",
    "        num_jets,\n",
    "        False,\n",
    "        axis=1,\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "add_selection(\n",
    "    \"preselection\",\n",
    "    preselection_cut.astype(bool),\n",
    "    selection,\n",
    "    cutflow,\n",
    "    isData,\n",
    "    signGenWeights,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gen Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_PDGID = 1\n",
    "u_PDGID = 2\n",
    "s_PDGID = 3\n",
    "c_PDGID = 4\n",
    "b_PDGID = 5\n",
    "g_PDGID = 21\n",
    "TOP_PDGID = 6\n",
    "\n",
    "ELE_PDGID = 11\n",
    "vELE_PDGID = 12\n",
    "MU_PDGID = 13\n",
    "vMU_PDGID = 14\n",
    "TAU_PDGID = 15\n",
    "vTAU_PDGID = 16\n",
    "\n",
    "Z_PDGID = 23\n",
    "W_PDGID = 24\n",
    "HIGGS_PDGID = 25\n",
    "Y_PDGID = 35\n",
    "\n",
    "b_PDGIDS = [511, 521, 523]\n",
    "\n",
    "GRAV_PDGID = 39\n",
    "\n",
    "GEN_FLAGS = [\"fromHardProcess\", \"isLastCopy\"]\n",
    "\n",
    "FILL_NONE_VALUE = -99999\n",
    "PAD_VAL = -99999\n",
    "\n",
    "skim_vars = {\n",
    "    \"eta\": \"Eta\",\n",
    "    \"phi\": \"Phi\",\n",
    "    \"mass\": \"Mass\",\n",
    "    \"pt\": \"Pt\",\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonresonant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the two gen higgs\n",
    "higgs = events.GenPart[\n",
    "    (abs(events.GenPart.pdgId) == HIGGS_PDGID) * events.GenPart.hasFlags(GEN_FLAGS)\n",
    "]\n",
    "\n",
    "# saving 4-vector info\n",
    "GenHiggsVars = {f\"GenHiggs{key}\": higgs[var].to_numpy() for (var, key) in skim_vars.items()}\n",
    "\n",
    "higgs_children = higgs.children\n",
    "\n",
    "# saving whether H->bb or H->VV\n",
    "GenHiggsVars[\"GenHiggsChildren\"] = abs(higgs_children.pdgId[:, :, 0]).to_numpy()\n",
    "\n",
    "# finding bb and VV children\n",
    "is_bb = abs(higgs_children.pdgId) == b_PDGID\n",
    "is_VV = (abs(higgs_children.pdgId) == W_PDGID) + (abs(higgs_children.pdgId) == Z_PDGID)\n",
    "\n",
    "Hbb = higgs[ak.sum(is_bb, axis=2) == 2]\n",
    "HVV = higgs[ak.sum(is_VV, axis=2) == 2]\n",
    "\n",
    "# checking that there are 2 b's and 2 V's\n",
    "has_bb = ak.sum(ak.flatten(is_bb, axis=2), axis=1) == 2\n",
    "has_VV = ak.sum(ak.flatten(is_VV, axis=2), axis=1) == 2\n",
    "\n",
    "# only select events with 2 b's and 2 V's\n",
    "add_selection(\"has_bbVV\", has_bb * has_VV, selection, cutflow, False, signGenWeights)\n",
    "\n",
    "# saving bb and VV 4-vector info\n",
    "bb = ak.flatten(higgs_children[is_bb], axis=2)\n",
    "VV = ak.flatten(higgs_children[is_VV], axis=2)\n",
    "\n",
    "# have to pad to 2 because of some 4V events\n",
    "GenbbVars = {\n",
    "    f\"Genbb{key}\": pad_val(bb[var], 2, FILL_NONE_VALUE, axis=1) for (var, key) in skim_vars.items()\n",
    "}\n",
    "\n",
    "# selecting only up to the 2nd index because of some 4V events\n",
    "# (doesn't matter which two are selected since these events will be excluded anyway)\n",
    "GenVVVars = {f\"GenVV{key}\": VV[var][:, :2].to_numpy() for (var, key) in skim_vars.items()}\n",
    "\n",
    "# checking that each V has 2 q children\n",
    "VV_children = VV.children\n",
    "\n",
    "quarks = abs(VV_children.pdgId) <= b_PDGID\n",
    "all_q = ak.all(ak.all(quarks, axis=2), axis=1)\n",
    "add_selection(\"all_q\", all_q, selection, cutflow, False, signGenWeights)\n",
    "\n",
    "V_has_2q = ak.count(VV_children.pdgId, axis=2) == 2\n",
    "has_4q = ak.values_astype(ak.prod(V_has_2q, axis=1), np.bool)\n",
    "add_selection(\"has_4q\", has_4q, selection, cutflow, False, signGenWeights)\n",
    "\n",
    "# saving 4q 4-vector info\n",
    "Gen4qVars = {\n",
    "    f\"Gen4q{key}\": ak.to_numpy(\n",
    "        ak.fill_none(\n",
    "            ak.pad_none(ak.pad_none(VV_children[var], 2, axis=1, clip=True), 2, axis=2, clip=True),\n",
    "            FILL_NONE_VALUE,\n",
    "        )\n",
    "    )\n",
    "    for (var, key) in skim_vars.items()\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resonant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fh/cwyrvktn5bz76x4cpy_lbgnh0000gn/T/ipykernel_4103/4007938691.py:29: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  has_4q = ak.values_astype(ak.prod(V_has_2q, axis=1), np.bool)\n"
     ]
    }
   ],
   "source": [
    "# gen higgs and kids\n",
    "higgs = events.GenPart[\n",
    "    (abs(events.GenPart.pdgId) == HIGGS_PDGID) * events.GenPart.hasFlags(GEN_FLAGS)\n",
    "]\n",
    "GenHiggsVars = {f\"GenHiggs{key}\": higgs[var].to_numpy() for (var, key) in skim_vars.items()}\n",
    "is_bb = abs(higgs.children.pdgId) == b_PDGID\n",
    "has_bb = ak.sum(ak.flatten(is_bb, axis=2), axis=1) == 2\n",
    "\n",
    "bb = ak.flatten(higgs.children[is_bb], axis=2)\n",
    "# GenbbVars = {f\"Genbb{key}\": pad_val(bb[var], 2, axis=1) for (var, key) in skim_vars.items()}\n",
    "\n",
    "# gen Y and kids\n",
    "Ys = events.GenPart[(abs(events.GenPart.pdgId) == Y_PDGID) * events.GenPart.hasFlags(GEN_FLAGS)]\n",
    "GenYVars = {f\"GenY{key}\": Ys[var].to_numpy() for (var, key) in skim_vars.items()}\n",
    "is_VV = (abs(Ys.children.pdgId) == W_PDGID) + (abs(Ys.children.pdgId) == Z_PDGID)\n",
    "has_VV = ak.sum(ak.flatten(is_VV, axis=2), axis=1) == 2\n",
    "\n",
    "add_selection(\"has_bbVV\", has_bb * has_VV, selection, cutflow, False, signGenWeights)\n",
    "\n",
    "VV = ak.flatten(Ys.children[is_VV], axis=2)\n",
    "# GenVVVars = {f\"GenVV{key}\": VV[var][:, :2].to_numpy() for (var, key) in skim_vars.items()}\n",
    "\n",
    "VV_children = VV.children\n",
    "quarks = abs(VV_children.pdgId) <= b_PDGID\n",
    "all_q = ak.all(ak.all(quarks, axis=2), axis=1)\n",
    "add_selection(\"all_q\", all_q, selection, cutflow, False, signGenWeights)\n",
    "\n",
    "V_has_2q = ak.count(VV_children.pdgId, axis=2) == 2\n",
    "has_4q = ak.values_astype(ak.prod(V_has_2q, axis=1), bool)\n",
    "add_selection(\"has_4q\", has_4q, selection, cutflow, False, signGenWeights)\n",
    "\n",
    "# Gen4qVars = {\n",
    "#     f\"Gen4q{key}\": ak.to_numpy(\n",
    "#         ak.fill_none(\n",
    "#             ak.pad_none(\n",
    "#                 ak.pad_none(VV_children[var], 2, axis=1, clip=True), 2, axis=2, clip=True\n",
    "#             ),\n",
    "#             PAD_VAL,\n",
    "#         )\n",
    "#     )\n",
    "#     for (var, key) in skim_vars.items()\n",
    "# }\n",
    "\n",
    "# fatjet gen matching\n",
    "Hbb = ak.pad_none(higgs, 1, axis=1, clip=True)[:, 0]\n",
    "HVV = ak.pad_none(Ys, 1, axis=1, clip=True)[:, 0]\n",
    "\n",
    "bbdr = fatjets[:, :2].delta_r(Hbb)\n",
    "vvdr = fatjets[:, :2].delta_r(HVV)\n",
    "\n",
    "match_dR = 0.8\n",
    "Hbb_match = bbdr <= match_dR\n",
    "HVV_match = vvdr <= match_dR\n",
    "\n",
    "# overlap removal - in the case where fatjet is matched to both, match it only to the closest Higgs\n",
    "Hbb_match = (Hbb_match * ~HVV_match) + (bbdr <= vvdr) * (Hbb_match * HVV_match)\n",
    "HVV_match = (HVV_match * ~Hbb_match) + (bbdr > vvdr) * (Hbb_match * HVV_match)\n",
    "\n",
    "VVJets = ak.pad_none(fatjets[HVV_match], 1, axis=1)[:, 0]\n",
    "quarkdrs = ak.flatten(VVJets.delta_r(VV_children), axis=2)\n",
    "num_prongs = ak.sum(quarkdrs < match_dR, axis=1)\n",
    "\n",
    "# GenMatchingVars = {\n",
    "#     \"ak8FatJetHbb\": pad_val(Hbb_match, 2, axis=1),\n",
    "#     \"ak8FatJetHVV\": pad_val(HVV_match, 2, axis=1),\n",
    "#     \"ak8FatJetHVVNumProngs\": ak.fill_none(num_prongs, PAD_VAL).to_numpy(),\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = selection.all(*selection.names)\n",
    "presel_events = events[selection.all(*selection.names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hbb = higgs[ak.sum(is_bb, axis=2) == 2]\n",
    "HVV = higgs[ak.sum(is_VV, axis=2) == 2]\n",
    "\n",
    "Hbb = ak.pad_none(Hbb, 1, axis=1, clip=True)[:, 0]\n",
    "HVV = ak.pad_none(HVV, 1, axis=1, clip=True)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbdr = fatjets[:, :2].delta_r(Hbb)\n",
    "vvdr = fatjets[:, :2].delta_r(HVV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(np.array(bbdr[select]).reshape(-1), np.linspace(0, 5, 101), histtype=\"step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(np.array(vvdr[select]).reshape(-1), np.linspace(0, 5, 101), histtype=\"step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_dR = 0.8\n",
    "Hbb_match = bbdr <= match_dR\n",
    "HVV_match = vvdr <= match_dR\n",
    "\n",
    "# overlap removal - in the case where fatjet is matched to both, match it only to the closest Higgs\n",
    "Hbb_match = (Hbb_match * ~HVV_match) + (bbdr <= vvdr) * (Hbb_match * HVV_match)\n",
    "HVV_match = (HVV_match * ~Hbb_match) + (bbdr > vvdr) * (Hbb_match * HVV_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hbb_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VVJets = ak.pad_none(fatjets[HVV_match], 1, axis=1)[:, 0]\n",
    "quarkdrs = ak.flatten(VVJets.delta_r(VV_children), axis=2)\n",
    "num_prongs = ak.sum(quarkdrs < match_dR, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genbb, genq = (bb, ak.flatten(VV_children, axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GenMatchingVars = {\n",
    "    \"ak8FatJetHbb\": pad_val(ak.fill_none(Hbb_match, [], axis=0), 2, FILL_NONE_VALUE, axis=1),\n",
    "    \"ak8FatJetHVV\": pad_val(ak.fill_none(HVV_match, [], axis=0), 2, FILL_NONE_VALUE, axis=1),\n",
    "    \"ak8FatJetHVVNumProngs\": ak.fill_none(num_prongs, -99999).to_numpy(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skimmed_events = {key: val[select] for key, val in GenMatchingVars.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jet definitions for LP SFs\n",
    "dR = 0.8\n",
    "cadef = fastjet.JetDefinition(fastjet.cambridge_algorithm, dR)\n",
    "ktdef = fastjet.JetDefinition(fastjet.kt_algorithm, dR)\n",
    "n_LP_sf_toys = 100\n",
    "\n",
    "package_path = \"../\"\n",
    "\n",
    "\n",
    "def _get_lund_arrays(events: NanoEventsArray, fatjet_idx: Union[int, ak.Array], num_prongs: int):\n",
    "    \"\"\"\n",
    "    Gets the ``num_prongs`` subjet pTs and Delta and kT per primary LP splitting of fatjets at\n",
    "    ``fatjet_idx`` in each event.\n",
    "\n",
    "    Features are flattened (for now), and offsets are saved in ``ld_offsets`` to recover the event\n",
    "    structure.\n",
    "\n",
    "    Args:\n",
    "        events (NanoEventsArray): nano events\n",
    "        fatjet_idx (int | ak.Array): fatjet index\n",
    "        num_prongs (int): number of prongs / subjets per jet to reweight\n",
    "\n",
    "    Returns:\n",
    "        flat_logD, flat_logkt, flat_subjet_pt, ld_offsets, kt_subjets_vec\n",
    "    \"\"\"\n",
    "\n",
    "    # get pfcands of the top-matched jets\n",
    "    ak8_pfcands = events.FatJetPFCands\n",
    "    ak8_pfcands = ak8_pfcands[ak8_pfcands.jetIdx == fatjet_idx]\n",
    "    pfcands = events.PFCands[ak8_pfcands.pFCandsIdx]\n",
    "\n",
    "    # need to convert to such a structure for FastJet\n",
    "    pfcands_vector_ptetaphi = ak.Array(\n",
    "        [\n",
    "            [{kin_key: cand[kin_key] for kin_key in P4} for cand in event_cands]\n",
    "            for event_cands in pfcands\n",
    "        ],\n",
    "        with_name=\"PtEtaPhiMLorentzVector\",\n",
    "    )\n",
    "\n",
    "    # cluster first with kT\n",
    "    kt_clustering = fastjet.ClusterSequence(pfcands_vector_ptetaphi, ktdef)\n",
    "    kt_subjets = kt_clustering.exclusive_jets(num_prongs)\n",
    "\n",
    "    kt_subjets_vec = ak.zip(\n",
    "        {\"x\": kt_subjets.px, \"y\": kt_subjets.py, \"z\": kt_subjets.pz, \"t\": kt_subjets.E},\n",
    "        with_name=\"LorentzVector\",\n",
    "    )\n",
    "\n",
    "    # save subjet pT\n",
    "    kt_subjets_pt = kt_subjets_vec.pt\n",
    "    # get constituents\n",
    "    kt_subjet_consts = kt_clustering.exclusive_jets_constituents(num_prongs)\n",
    "\n",
    "    # then re-cluster with CA\n",
    "    # won't need to flatten once https://github.com/scikit-hep/fastjet/pull/145 is released\n",
    "    ca_clustering = fastjet.ClusterSequence(ak.flatten(kt_subjet_consts, axis=1), cadef)\n",
    "    lds = ak.flatten(ca_clustering.exclusive_jets_lund_declusterings(1), axis=1)\n",
    "\n",
    "    # flatten and save offsets to unflatten afterwards\n",
    "    ld_offsets = lds.kt.layout.offsets\n",
    "    flat_logD = np.log(0.8 / ak.flatten(lds).Delta).to_numpy()\n",
    "    flat_logkt = np.log(ak.flatten(lds).kt).to_numpy()\n",
    "    # repeat subjet pt for each lund declustering\n",
    "    flat_subjet_pt = np.repeat(ak.flatten(kt_subjets_pt), ak.count(lds.kt, axis=1)).to_numpy()\n",
    "\n",
    "    return flat_logD, flat_logkt, flat_subjet_pt, ld_offsets, kt_subjets_vec\n",
    "\n",
    "\n",
    "def _calc_lund_SFs(\n",
    "    flat_logD: np.ndarray,\n",
    "    flat_logkt: np.ndarray,\n",
    "    flat_subjet_pt: np.ndarray,\n",
    "    ld_offsets: ak.Array,\n",
    "    num_prongs: int,\n",
    "    ratio_lookups: List[dense_lookup],\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates scale factors for jets based on splittings in the primary Lund Plane.\n",
    "\n",
    "    Lookup tables should be binned in [subjet_pt, ln(0.8/Delta), ln(kT/GeV)].\n",
    "\n",
    "    Returns nominal scale factors for each lookup table in the ``ratio_smeared_lookups`` list.\n",
    "\n",
    "    Args:\n",
    "        flat_logD, flat_logkt, flat_subjet_pt, ld_offsets: numpy arrays from the ``lund_arrays`` fn\n",
    "        num_prongs (int): number of prongs / subjets per jet to reweight\n",
    "        ratio_smeared_lookups (List[dense_lookup]): list of lookup tables with smeared values\n",
    "\n",
    "    Returns:\n",
    "        nd.ndarray: SF values per jet for each smearing, shape ``[n_jets, len(ratio_lookups)]``.\n",
    "    \"\"\"\n",
    "\n",
    "    sf_vals = []\n",
    "    # could be parallelised but not sure if memory / time trade-off is worth it\n",
    "    for i, ratio_lookup in enumerate(ratio_lookups):\n",
    "        ratio_vals = ratio_lookup(flat_subjet_pt, flat_logD, flat_logkt)\n",
    "        # recover jagged event structure\n",
    "        reshaped_ratio_vals = ak.Array(\n",
    "            ak.layout.ListOffsetArray64(ld_offsets, ak.layout.NumpyArray(ratio_vals))\n",
    "        )\n",
    "        # nominal values are product of all lund plane SFs\n",
    "        sf_vals.append(\n",
    "            # multiply subjet SFs per jet\n",
    "            np.prod(\n",
    "                # per-subjet SF\n",
    "                ak.prod(reshaped_ratio_vals, axis=1).to_numpy().reshape(-1, num_prongs),\n",
    "                axis=1,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return np.array(sf_vals).T  # output shape: ``[n_jets, len(ratio_lookups)]``\n",
    "\n",
    "\n",
    "def _get_lund_lookups(seed: int = 42, lnN: bool = True, trunc_gauss: bool = False):\n",
    "    import uproot\n",
    "\n",
    "    # initialize lund plane scale factors lookups\n",
    "    f = uproot.open(package_path + \"/corrections/lp_ratio_jan20.root\")\n",
    "\n",
    "    # 3D histogram: [subjet_pt, ln(0.8/Delta), ln(kT/GeV)]\n",
    "    ratio_nom = f[\"ratio_nom\"].to_numpy()\n",
    "    ratio_nom_errs = f[\"ratio_nom\"].errors()\n",
    "    ratio_edges = ratio_nom[1:]\n",
    "    ratio_nom = ratio_nom[0]\n",
    "\n",
    "    ratio_sys_up = dense_lookup(f[\"ratio_sys_tot_up\"].to_numpy()[0], ratio_edges)\n",
    "    ratio_sys_down = dense_lookup(f[\"ratio_sys_tot_down\"].to_numpy()[0], ratio_edges)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    rand_noise = np.random.normal(size=[n_LP_sf_toys, *ratio_nom.shape])\n",
    "\n",
    "    if trunc_gauss:\n",
    "        # produces array of shape ``[n_sf_toys, subjet_pt bins, ln(0.8/Delta) bins, ln(kT/GeV) bins]``\n",
    "        ratio_nom_smeared = ratio_nom + (ratio_nom_errs * rand_noise)\n",
    "        ratio_nom_smeared = np.maximum(ratio_nom_smeared, 0)\n",
    "        # save n_sf_toys lookups\n",
    "        ratio_smeared_lookups = [dense_lookup(ratio_nom, ratio_edges)] + [\n",
    "            dense_lookup(ratio_nom_smeared[i], ratio_edges) for i in range(n_LP_sf_toys)\n",
    "        ]\n",
    "    else:\n",
    "        ratio_smeared_lookups = None\n",
    "\n",
    "    if lnN:\n",
    "        # revised smearing (0s -> 1s, normal -> lnN)\n",
    "        zero_noms = ratio_nom == 0\n",
    "        ratio_nom[zero_noms] = 1\n",
    "        ratio_nom_errs[zero_noms] = 0\n",
    "\n",
    "        kappa = (ratio_nom + ratio_nom_errs) / ratio_nom\n",
    "        ratio_nom_smeared = ratio_nom * np.power(kappa, rand_noise)\n",
    "        ratio_lnN_smeared_lookups = [dense_lookup(ratio_nom, ratio_edges)] + [\n",
    "            dense_lookup(ratio_nom_smeared[i], ratio_edges) for i in range(n_LP_sf_toys)\n",
    "        ]\n",
    "    else:\n",
    "        ratio_lnN_smeared_lookups = None\n",
    "\n",
    "    return ratio_smeared_lookups, ratio_lnN_smeared_lookups, ratio_sys_up, ratio_sys_down\n",
    "\n",
    "\n",
    "(\n",
    "    ratio_smeared_lookups,\n",
    "    ratio_lnN_smeared_lookups,\n",
    "    ratio_sys_up,\n",
    "    ratio_sys_down,\n",
    ") = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "\n",
    "def get_lund_SFs(\n",
    "    events: NanoEventsArray,\n",
    "    fatjet_idx: Union[int, ak.Array],\n",
    "    num_prongs: int,\n",
    "    gen_quarks: GenParticleArray,\n",
    "    seed: int = 42,\n",
    "    trunc_gauss: bool = False,\n",
    "    lnN: bool = True,\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculates scale factors for jets based on splittings in the primary Lund Plane.\n",
    "    Calculates random smearings for statistical uncertainties, total up/down systematic variation,\n",
    "    and subjet matching and pT extrapolation systematic uncertainties.\n",
    "\n",
    "    Args:\n",
    "        events (NanoEventsArray): nano events\n",
    "        fatjet_idx (int | ak.Array): fatjet index\n",
    "        num_prongs (int): number of prongs / subjets per jet to r\n",
    "        seed (int, optional): seed for random smearings. Defaults to 42.\n",
    "        trunc_gauss (bool, optional): use truncated gaussians for smearing. Defaults to False.\n",
    "        lnN (bool, optional): use log normals for smearings. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, np.ndarray]: dictionary with nominal weights per jet, sys variations, and (optionally) random smearings.\n",
    "    \"\"\"\n",
    "    global ratio_smeared_lookups, ratio_lnN_smeared_lookups, ratio_sys_up, ratio_sys_down\n",
    "\n",
    "    if (lnN and ratio_lnN_smeared_lookups == []) or (trunc_gauss and ratio_smeared_lookups == []):\n",
    "        (\n",
    "            ratio_smeared_lookups,\n",
    "            ratio_lnN_smeared_lookups,\n",
    "            ratio_sys_up,\n",
    "            ratio_sys_down,\n",
    "        ) = _get_lund_lookups(seed, lnN, trunc_gauss)\n",
    "\n",
    "    print(\"ratio lnN smeared lookups\")\n",
    "    print(ratio_lnN_smeared_lookups[0]._values)\n",
    "\n",
    "    flat_logD, flat_logkt, flat_subjet_pt, ld_offsets, kt_subjets_vec = _get_lund_arrays(\n",
    "        events, fatjet_idx, num_prongs\n",
    "    )\n",
    "\n",
    "    sfs = {}\n",
    "\n",
    "    ### get scale factors per jet + smearings for stat unc. + syst. variations\n",
    "\n",
    "    if trunc_gauss:\n",
    "        sfs[\"lp_sf\"] = _calc_lund_SFs(\n",
    "            flat_logD, flat_logkt, flat_subjet_pt, ld_offsets, num_prongs, ratio_smeared_lookups\n",
    "        )\n",
    "\n",
    "    if lnN:\n",
    "        sfs[\"lp_sf_lnN\"] = _calc_lund_SFs(\n",
    "            flat_logD, flat_logkt, flat_subjet_pt, ld_offsets, num_prongs, ratio_lnN_smeared_lookups\n",
    "        )\n",
    "\n",
    "    sfs[\"lp_sf_sys_down\"] = _calc_lund_SFs(\n",
    "        flat_logD, flat_logkt, flat_subjet_pt, ld_offsets, num_prongs, [ratio_sys_down]\n",
    "    )\n",
    "\n",
    "    sfs[\"lp_sf_sys_up\"] = _calc_lund_SFs(\n",
    "        flat_logD, flat_logkt, flat_subjet_pt, ld_offsets, num_prongs, [ratio_sys_up]\n",
    "    )\n",
    "\n",
    "    ### subjet matching and pT extrapoation uncertainties\n",
    "\n",
    "    matching_dR = 0.2\n",
    "    sj_matched = []\n",
    "    sj_matched_idx = []\n",
    "\n",
    "    # get dR between gen quarks and subjets\n",
    "    for i in range(num_prongs):\n",
    "        sj_q_dr = kt_subjets_vec.delta_r(gen_quarks[:, i])\n",
    "        # is quark matched to a subjet (dR < 0.2)\n",
    "        sj_matched.append(ak.min(sj_q_dr, axis=1) <= matching_dR)\n",
    "        # save index of closest subjet\n",
    "        sj_matched_idx.append(ak.argmin(sj_q_dr, axis=1))\n",
    "\n",
    "    sj_matched = np.array(sj_matched).T\n",
    "    sj_matched_idx = np.array(sj_matched_idx).T\n",
    "\n",
    "    # mask quarks which aren't matched to a subjet, to avoid overcounting events\n",
    "    sj_matched_idx_mask = np.copy(sj_matched_idx)\n",
    "    sj_matched_idx_mask[~sj_matched] = -1\n",
    "\n",
    "    # events which have more than one quark matched to the same subjet\n",
    "    sfs[\"lp_sf_double_matched_event\"] = np.any(\n",
    "        [np.sum(sj_matched_idx_mask == i, axis=1) > 1 for i in range(3)], axis=0\n",
    "    ).astype(int)[:, np.newaxis]\n",
    "\n",
    "    # number of quarks per event which aren't matched\n",
    "    sfs[\"lp_sf_unmatched_quarks\"] = np.sum(~sj_matched, axis=1, keepdims=True)\n",
    "\n",
    "    # pT extrapolation uncertainty\n",
    "    sfs[\"lp_sf_num_sjpt_gt350\"] = np.sum(kt_subjets_vec.pt > 350, axis=1, keepdims=True).to_numpy()\n",
    "\n",
    "    return sfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_lnN_smeared_lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genbb = genbb[select]\n",
    "# genq = genq[select]\n",
    "\n",
    "i = 0\n",
    "\n",
    "fatjetsi = fatjets[select][:, i]\n",
    "bb_select = skimmed_events[\"ak8FatJetHbb\"][:, i].astype(bool)\n",
    "VV_select = skimmed_events[\"ak8FatJetHVV\"][:, i].astype(bool)\n",
    "\n",
    "# selectors for Hbb jets and HVV jets with 2, 3, or 4 prongs separately\n",
    "selectors = {\n",
    "    # name: (selector, gen quarks, num prongs)\n",
    "    \"bb\": (bb_select, genbb, 2),\n",
    "    **{\n",
    "        f\"VV{k}q\": (VV_select * (skimmed_events[\"ak8FatJetHVVNumProngs\"] == k), genq, k)\n",
    "        for k in range(2, 4 + 1)\n",
    "    },\n",
    "}\n",
    "\n",
    "selected_sfs = {}\n",
    "\n",
    "for key, (selector, gen_quarks, num_prongs) in selectors.items():\n",
    "    selected_sfs[key] = get_lund_SFs(\n",
    "        events[select][selector],\n",
    "        i,\n",
    "        num_prongs,\n",
    "        gen_quarks[selector],\n",
    "        trunc_gauss=False,\n",
    "        lnN=True,\n",
    "    )\n",
    "\n",
    "sf_dict = {}\n",
    "\n",
    "# collect all the scale factors, fill in 0s for unmatched jets\n",
    "for key, val in selected_sfs[\"bb\"].items():\n",
    "    arr = np.zeros((np.sum(select), val.shape[1]))\n",
    "\n",
    "    for select_key, (selector, _, _) in selectors.items():\n",
    "        arr[selector] = selected_sfs[select_key][key]\n",
    "\n",
    "    sf_dict[key] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_dict = {}\n",
    "\n",
    "# collect all the scale factors, fill in 0s for unmatched jets\n",
    "for key, val in selected_sfs[\"bb\"].items():\n",
    "    arr = np.zeros((np.sum(select), val.shape[1]))\n",
    "\n",
    "    for select_key, (selector, _, _) in selectors.items():\n",
    "        arr[selector] = selected_sfs[select_key][key]\n",
    "\n",
    "    sf_dict[key] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_dict[\"lp_sf_lnN\"][VV_select * (skimmed_events[\"ak8FatJetHVVNumProngs\"] == 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_sfs[\"VV2q\"][\"lp_sf_lnN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_sfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[selector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skimmed_events[\"ak8FatJetHVVNumProngs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(bb_select) + np.sum(VV_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(VV_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_idx = 0\n",
    "\n",
    "ak8_pfcands = presel_events.FatJetPFCands\n",
    "ak8_pfcands = ak8_pfcands[ak8_pfcands.jetIdx == jet_idx]\n",
    "pfcands = presel_events.PFCands[ak8_pfcands.pFCandsIdx]\n",
    "\n",
    "pfcands_vector_ptetaphi = ak.Array(\n",
    "    [\n",
    "        [{kin_key: cand[kin_key] for kin_key in skim_vars} for cand in event_cands]\n",
    "        for event_cands in merged_pfcands\n",
    "    ],\n",
    "    with_name=\"PtEtaPhiMLorentzVector\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15adc7883e707560d0d9727709639b8fe3f3cff1f197d2d643742923ff23a29c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
