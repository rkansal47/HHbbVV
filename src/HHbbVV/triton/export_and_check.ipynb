{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import tritonclient.grpc as triton_grpc\n",
    "import tritonclient.http as triton_http\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/lgray/hgg-coffea/blob/triton-bdts/src/hgg_coffea/tools/chained_quantile.py\n",
    "class wrapped_triton:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_url: str,\n",
    "    ) -> None:\n",
    "        fullprotocol, location = model_url.split(\"://\")\n",
    "        _, protocol = fullprotocol.split(\"+\")\n",
    "        address, model, version = location.split(\"/\")\n",
    "\n",
    "        self._protocol = protocol\n",
    "        self._address = address\n",
    "        self._model = model\n",
    "        self._version = version\n",
    "\n",
    "    def __call__(self, input_dict: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "        if self._protocol == \"grpc\":\n",
    "            client = triton_grpc.InferenceServerClient(url=self._address, verbose=False)\n",
    "            triton_protocol = triton_grpc\n",
    "        elif self._protocol == \"http\":\n",
    "            client = triton_http.InferenceServerClient(\n",
    "                url=self._address,\n",
    "                verbose=False,\n",
    "                concurrency=12,\n",
    "            )\n",
    "            triton_protocol = triton_http\n",
    "        else:\n",
    "            raise ValueError(f\"{self._protocol} does not encode a valid protocol (grpc or http)\")\n",
    "\n",
    "        # Infer\n",
    "        inputs = []\n",
    "\n",
    "        for key in input_dict:\n",
    "            input = triton_protocol.InferInput(key, input_dict[key].shape, \"FP32\")\n",
    "            input.set_data_from_numpy(input_dict[key])\n",
    "            inputs.append(input)\n",
    "\n",
    "        output = triton_protocol.InferRequestedOutput(\"softmax\")\n",
    "\n",
    "        request = client.infer(\n",
    "            self._model,\n",
    "            model_version=self._version,\n",
    "            inputs=inputs,\n",
    "            outputs=[output],\n",
    "        )\n",
    "\n",
    "        out = request.as_numpy(\"softmax\")\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "# pfs = 100\n",
    "# svs = 7\n",
    "pfs = 128\n",
    "svs = 10\n",
    "np.random.seed(42)\n",
    "\n",
    "# input_dict = {\n",
    "#     \"pf_points\": np.random.rand(batch_size, 2, pfs).astype(\"float32\"),\n",
    "#     \"pf_features\": np.random.rand(batch_size, 19, pfs).astype(\"float32\"),\n",
    "#     \"pf_mask\": (np.random.rand(batch_size, 1, pfs) > 0.2).astype(\"float32\"),\n",
    "#     \"sv_points\": np.random.rand(batch_size, 2, svs).astype(\"float32\"),\n",
    "#     \"sv_features\": np.random.rand(batch_size, 11, svs).astype(\"float32\"),\n",
    "#     \"sv_mask\": (np.random.rand(batch_size, 1, svs) > 0.2).astype(\"float32\"),\n",
    "# }\n",
    "\n",
    "input_dict = {\n",
    "    \"pf_features\": np.random.rand(batch_size, 25, pfs).astype(\"float32\"),\n",
    "    \"pf_vectors\": np.random.rand(batch_size, 4, pfs).astype(\"float32\"),\n",
    "    \"pf_mask\": (np.random.rand(batch_size, 1, pfs) > 0.2).astype(\"float32\"),\n",
    "    \"sv_features\": np.random.rand(batch_size, 11, svs).astype(\"float32\"),\n",
    "    \"sv_vectors\": np.random.rand(batch_size, 4, svs).astype(\"float32\"),\n",
    "    \"sv_mask\": (np.random.rand(batch_size, 1, svs) > 0.2).astype(\"float32\"),\n",
    "}\n",
    "\n",
    "# input_dict = {\n",
    "#     \"pf_points__0\": np.random.rand(batch_size, 2, pfs).astype(\"float32\"),\n",
    "#     \"pf_features__1\": np.random.rand(batch_size, 19, pfs).astype(\"float32\"),\n",
    "#     \"pf_mask__2\": (np.random.rand(batch_size, 1, pfs) > 0.2).astype(\"float32\"),\n",
    "#     \"sv_points__3\": np.random.rand(batch_size, 2, svs).astype(\"float32\"),\n",
    "#     \"sv_features__4\": np.random.rand(batch_size, 11, svs).astype(\"float32\"),\n",
    "#     \"sv_mask__5\": (np.random.rand(batch_size, 1, svs) > 0.2).astype(\"float32\"),\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -9.643893   -11.078968    -6.423805   ...  -5.95106      0.4450065\n",
      "    0.25934374]\n",
      " [-11.023443   -12.613604    -7.5247912  ...  -7.6548076    0.6109172\n",
      "    0.35383254]\n",
      " [ -5.1560373   -7.3972573   -2.6038532  ...  -2.6032228    0.46883643\n",
      "    0.20065397]\n",
      " [ -9.264318   -11.314613    -6.1544614  ...  -5.425825     0.66927564\n",
      "    0.41242862]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model_url = \"triton+grpc://ailab01.fnal.gov:8001/particlenet_hww/1\"\n",
    "# model_url = \"triton+grpc://prp-gpu-1.t2.ucsd.edu:8001/particlenet_hww/1\"\n",
    "# model_url = \"triton+grpc://67.58.49.52:8001/ak8_MD_vminclv2ParT_manual_fixwrap/1\"\n",
    "model_url = \"triton+grpc://67.58.49.52:8001/2023May30_ak8_MD_inclv8_part_2reg_manual/1\"\n",
    "triton_model = wrapped_triton(model_url)\n",
    "for i in tqdm(range(1)):\n",
    "    output = triton_model(input_dict)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX outputs: [[ -9.643891   -11.078964    -6.423809   ...  -5.951058     0.44500652\n",
      "    0.25934368]\n",
      " [-11.023443   -12.613605    -7.5247912  ...  -7.654809     0.61091703\n",
      "    0.35383236]\n",
      " [ -5.1560345   -7.3972545   -2.6038537  ...  -2.60322      0.46883655\n",
      "    0.2006541 ]\n",
      " [ -9.264317   -11.314613    -6.1544585  ...  -5.425826     0.6692767\n",
      "    0.4124295 ]]\n",
      "Shape: (4, 316)\n",
      "Softmax applied: False\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "\n",
    "model_dir = (\n",
    "    \"models/model_2023May30/ak8_MD_inclv8_part_2reg_manual.useamp.lite.gm5.ddp-bs768-lr6p75e-3/\"\n",
    ")\n",
    "\n",
    "onnx_model = onnx.load(model_dir + \"model.onnx\")\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "ort_sess = ort.InferenceSession(model_dir + \"model.onnx\")\n",
    "outputs = ort_sess.run(None, input_dict)[0]\n",
    "print(\"ONNX outputs:\", outputs)\n",
    "print(\"Shape:\", outputs.shape)\n",
    "print(\"Softmax applied:\", np.allclose(np.sum(outputs, axis=1), 1, atol=1e-5))\n",
    "\n",
    "# Print Result\n",
    "# predicted, actual = classes[outputs[0][0].argmax(0)], classes[y]\n",
    "# print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX outputs: [[7.8597404e-06 1.8713886e-06 1.9673070e-04 ... 3.1563517e-04\n",
      "  1.8921728e-01 1.5715510e-01]\n",
      " [5.0401678e-07 1.0276534e-07 1.6668266e-05 ... 1.4636063e-05\n",
      "  5.6909341e-02 4.4008151e-02]\n",
      " [3.9485330e-04 4.1984258e-05 5.0679673e-03 ... 5.0711799e-03\n",
      "  1.0946776e-01 8.3717473e-02]\n",
      " [3.6939423e-06 4.7539822e-07 8.2810249e-05 ... 1.7160318e-04\n",
      "  7.6136842e-02 5.8890816e-02]]\n",
      "Shape: (4, 316)\n",
      "Softmax applied: True\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "\n",
    "model_dir = (\n",
    "    \"models/model_2023May30/ak8_MD_inclv8_part_2reg_manual.useamp.lite.gm5.ddp-bs768-lr6p75e-3/\"\n",
    ")\n",
    "\n",
    "onnx_model = onnx.load(model_dir + \"model.onnx\")\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "ort_sess = ort.InferenceSession(model_dir + \"model.onnx\")\n",
    "outputs = ort_sess.run(None, input_dict)\n",
    "print(\"ONNX outputs:\", outputs[0])\n",
    "print(\"Shape:\", outputs[0].shape)\n",
    "print(\"Softmax applied:\", np.allclose(np.sum(outputs[0], axis=1), 1, atol=1e-5))\n",
    "\n",
    "# Print Result\n",
    "# predicted, actual = classes[outputs[0][0].argmax(0)], classes[y]\n",
    "# print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX cls outputs: [0.65362775 0.8990826  0.8068148  0.8649726 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"ONNX cls output sums:\", np.sum(outputs[0][:, :-2], axis=1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023May30 Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raghav/mambaforge/envs/python310/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ParticleTransformer import ParticleTransformerTagger\n",
    "\n",
    "part_model = ParticleTransformerTagger(\n",
    "    pf_input_dim=25,\n",
    "    sv_input_dim=11,\n",
    "    num_classes=314,  # one dim for regression\n",
    "    # network configurations\n",
    "    pair_input_dim=4,\n",
    "    embed_dims=[128, 512, 128],\n",
    "    pair_embed_dims=[64, 64, 64],\n",
    "    num_heads=8,\n",
    "    num_layers=8,\n",
    "    num_cls_layers=2,\n",
    "    block_params=None,\n",
    "    cls_block_params={\"dropout\": 0, \"attn_dropout\": 0, \"activation_dropout\": 0},\n",
    "    fc_params=[],\n",
    "    activation=\"gelu\",\n",
    "    # misc\n",
    "    trim=True,\n",
    "    for_inference=True,\n",
    ").cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ParticleTransformerTagger:\n\tMissing key(s) in state_dict: \"part.fc.0.weight\", \"part.fc.0.bias\". \n\tUnexpected key(s) in state_dict: \"part.fc.1.weight\", \"part.fc.1.bias\", \"part.fc.0.0.weight\", \"part.fc.0.0.bias\". \n\tsize mismatch for pf_embed.embed.1.weight: copying a param with shape torch.Size([64, 25]) from checkpoint, the shape in current model is torch.Size([128, 25]).\n\tsize mismatch for pf_embed.embed.1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for pf_embed.embed.3.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for pf_embed.embed.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for pf_embed.embed.4.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for pf_embed.embed.4.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for pf_embed.embed.6.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for pf_embed.embed.6.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for pf_embed.embed.7.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for pf_embed.embed.7.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for sv_embed.embed.1.weight: copying a param with shape torch.Size([64, 11]) from checkpoint, the shape in current model is torch.Size([128, 11]).\n\tsize mismatch for sv_embed.embed.1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for sv_embed.embed.3.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for sv_embed.embed.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for sv_embed.embed.4.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for sv_embed.embed.4.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for sv_embed.embed.6.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for sv_embed.embed.6.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for sv_embed.embed.7.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for sv_embed.embed.7.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_token: copying a param with shape torch.Size([1, 1, 64]) from checkpoint, the shape in current model is torch.Size([1, 1, 128]).\n\tsize mismatch for part.pair_embed.embed.1.weight: copying a param with shape torch.Size([32, 4, 1]) from checkpoint, the shape in current model is torch.Size([64, 4, 1]).\n\tsize mismatch for part.pair_embed.embed.1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.2.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.2.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.4.weight: copying a param with shape torch.Size([32, 32, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1]).\n\tsize mismatch for part.pair_embed.embed.4.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.5.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.5.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.5.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.5.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.7.weight: copying a param with shape torch.Size([32, 32, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1]).\n\tsize mismatch for part.pair_embed.embed.7.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.8.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.8.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.8.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.8.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.10.weight: copying a param with shape torch.Size([8, 32, 1]) from checkpoint, the shape in current model is torch.Size([8, 64, 1]).\n\tsize mismatch for part.blocks.0.w_resid: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.0.pre_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.0.pre_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.0.attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for part.blocks.0.attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for part.blocks.0.attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for part.blocks.0.attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.0.post_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.0.post_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.0.pre_fc_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.0.pre_fc_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.0.fc1.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for part.blocks.0.fc1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.0.post_fc_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.0.post_fc_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.0.fc2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for part.blocks.0.fc2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.1.w_resid: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.1.pre_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.1.pre_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.1.attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for part.blocks.1.attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for part.blocks.1.attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for part.blocks.1.attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.1.post_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.1.post_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.1.pre_fc_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.1.pre_fc_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.1.fc1.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for part.blocks.1.fc1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.1.post_fc_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.1.post_fc_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.1.fc2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for part.blocks.1.fc2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.2.w_resid: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.2.pre_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.2.pre_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.2.attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for part.blocks.2.attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for part.blocks.2.attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for part.blocks.2.attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.2.post_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.2.post_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.2.pre_fc_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.2.pre_fc_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.2.fc1.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for part.blocks.2.fc1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.2.post_fc_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.2.post_fc_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.2.fc2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for part.blocks.2.fc2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.3.w_resid: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.3.pre_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.3.pre_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.3.attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for part.blocks.3.attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for part.blocks.3.attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for part.blocks.3.attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.3.post_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.3.post_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.3.pre_fc_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.3.pre_fc_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.3.fc1.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for part.blocks.3.fc1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.3.post_fc_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.3.post_fc_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.3.fc2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for part.blocks.3.fc2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.4.w_resid: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.4.pre_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.4.pre_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.4.attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for part.blocks.4.attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for part.blocks.4.attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for part.blocks.4.attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.4.post_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.4.post_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.4.pre_fc_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.4.pre_fc_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.4.fc1.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for part.blocks.4.fc1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.4.post_fc_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.4.post_fc_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.4.fc2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for part.blocks.4.fc2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.5.w_resid: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.5.pre_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.5.pre_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.5.attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for part.blocks.5.attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for part.blocks.5.attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for part.blocks.5.attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.5.post_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.5.post_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.5.pre_fc_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.5.pre_fc_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.5.fc1.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for part.blocks.5.fc1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.5.post_fc_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.5.post_fc_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.5.fc2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for part.blocks.5.fc2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.6.w_resid: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.6.pre_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.6.pre_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.6.attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for part.blocks.6.attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for part.blocks.6.attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for part.blocks.6.attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.6.post_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.6.post_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.6.pre_fc_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.6.pre_fc_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.6.fc1.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for part.blocks.6.fc1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.6.post_fc_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.6.post_fc_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.6.fc2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for part.blocks.6.fc2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.7.w_resid: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.7.pre_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.7.pre_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.7.attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for part.blocks.7.attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for part.blocks.7.attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for part.blocks.7.attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.7.post_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.7.post_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.7.pre_fc_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.7.pre_fc_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.7.fc1.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for part.blocks.7.fc1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.7.post_fc_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.7.post_fc_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.7.fc2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for part.blocks.7.fc2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.0.w_resid: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.0.pre_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.0.pre_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.0.attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for part.cls_blocks.0.attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for part.cls_blocks.0.attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for part.cls_blocks.0.attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.0.post_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.0.post_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.0.pre_fc_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.0.pre_fc_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.0.fc1.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for part.cls_blocks.0.fc1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.cls_blocks.0.post_fc_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.cls_blocks.0.post_fc_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.cls_blocks.0.fc2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for part.cls_blocks.0.fc2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.1.w_resid: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.1.pre_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.1.pre_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.1.attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for part.cls_blocks.1.attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for part.cls_blocks.1.attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for part.cls_blocks.1.attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.1.post_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.1.post_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.1.pre_fc_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.1.pre_fc_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.1.fc1.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for part.cls_blocks.1.fc1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.cls_blocks.1.post_fc_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.cls_blocks.1.post_fc_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.cls_blocks.1.fc2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for part.cls_blocks.1.fc2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodels/model_2023May30/ak8_MD_inclv8_part_2reg_manual.useamp.lite.gm5.ddp-bs768-lr6p75e-3/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m part_model\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(model_dir \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mnet_best_epoch_state.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m, map_location\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)))\n\u001b[1;32m      3\u001b[0m _ \u001b[39m=\u001b[39m part_model\u001b[39m.\u001b[39meval()\n\u001b[1;32m      5\u001b[0m data_config \u001b[39m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minput_names\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39mpf_features\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpf_vectors\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpf_mask\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msv_features\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msv_vectors\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msv_mask\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m      7\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minput_shapes\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     },\n\u001b[1;32m     15\u001b[0m }\n",
      "File \u001b[0;32m~/mambaforge/envs/python310/lib/python3.10/site-packages/torch/nn/modules/module.py:1671\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   1667\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1668\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1670\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1671\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1672\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1673\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ParticleTransformerTagger:\n\tMissing key(s) in state_dict: \"part.fc.0.weight\", \"part.fc.0.bias\". \n\tUnexpected key(s) in state_dict: \"part.fc.1.weight\", \"part.fc.1.bias\", \"part.fc.0.0.weight\", \"part.fc.0.0.bias\". \n\tsize mismatch for pf_embed.embed.1.weight: copying a param with shape torch.Size([64, 25]) from checkpoint, the shape in current model is torch.Size([128, 25]).\n\tsize mismatch for pf_embed.embed.1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for pf_embed.embed.3.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for pf_embed.embed.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for pf_embed.embed.4.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for pf_embed.embed.4.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for pf_embed.embed.6.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for pf_embed.embed.6.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for pf_embed.embed.7.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for pf_embed.embed.7.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for sv_embed.embed.1.weight: copying a param with shape torch.Size([64, 11]) from checkpoint, the shape in current model is torch.Size([128, 11]).\n\tsize mismatch for sv_embed.embed.1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for sv_embed.embed.3.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for sv_embed.embed.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for sv_embed.embed.4.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for sv_embed.embed.4.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for sv_embed.embed.6.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for sv_embed.embed.6.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for sv_embed.embed.7.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for sv_embed.embed.7.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_token: copying a param with shape torch.Size([1, 1, 64]) from checkpoint, the shape in current model is torch.Size([1, 1, 128]).\n\tsize mismatch for part.pair_embed.embed.1.weight: copying a param with shape torch.Size([32, 4, 1]) from checkpoint, the shape in current model is torch.Size([64, 4, 1]).\n\tsize mismatch for part.pair_embed.embed.1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.2.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.2.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.4.weight: copying a param with shape torch.Size([32, 32, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1]).\n\tsize mismatch for part.pair_embed.embed.4.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.5.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.5.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.5.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.5.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.7.weight: copying a param with shape torch.Size([32, 32, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1]).\n\tsize mismatch for part.pair_embed.embed.7.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.8.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.8.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.8.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.8.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for part.pair_embed.embed.10.weight: copying a param with shape torch.Size([8, 32, 1]) from checkpoint, the shape in current model is torch.Size([8, 64, 1]).\n\tsize mismatch for part.blocks.0.w_resid: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.0.pre_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.0.pre_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.0.attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for part.blocks.0.attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for part.blocks.0.attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for part.blocks.0.attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.0.post_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.0.post_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.0.pre_fc_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.0.pre_fc_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.0.fc1.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for part.blocks.0.fc1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.0.post_fc_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.0.post_fc_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.0.fc2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for part.blocks.0.fc2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.1.w_resid: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.1.pre_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.1.pre_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.1.attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for part.blocks.1.attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for part.blocks.1.attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for part.blocks.1.attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.1.post_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.1.post_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.1.pre_fc_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.1.pre_fc_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.1.fc1.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for part.blocks.1.fc1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.1.post_fc_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.1.post_fc_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.1.fc2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for part.blocks.1.fc2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.2.w_resid: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.2.pre_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.2.pre_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.2.attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for part.blocks.2.attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for part.blocks.2.attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for part.blocks.2.attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.2.post_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.2.post_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.2.pre_fc_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.2.pre_fc_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.2.fc1.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for part.blocks.2.fc1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.2.post_fc_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.2.post_fc_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.2.fc2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for part.blocks.2.fc2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.3.w_resid: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.3.pre_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.3.pre_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.3.attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for part.blocks.3.attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for part.blocks.3.attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for part.blocks.3.attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.3.post_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.3.post_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.3.pre_fc_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.3.pre_fc_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.3.fc1.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for part.blocks.3.fc1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.3.post_fc_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.3.post_fc_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.3.fc2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for part.blocks.3.fc2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.4.w_resid: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.4.pre_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.4.pre_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.4.attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for part.blocks.4.attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for part.blocks.4.attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for part.blocks.4.attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.4.post_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.4.post_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.4.pre_fc_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.4.pre_fc_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.4.fc1.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for part.blocks.4.fc1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.4.post_fc_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.4.post_fc_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.4.fc2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for part.blocks.4.fc2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.5.w_resid: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.5.pre_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.5.pre_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.5.attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for part.blocks.5.attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for part.blocks.5.attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for part.blocks.5.attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.5.post_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.5.post_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.5.pre_fc_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.5.pre_fc_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.5.fc1.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for part.blocks.5.fc1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.5.post_fc_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.5.post_fc_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.5.fc2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for part.blocks.5.fc2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.6.w_resid: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.6.pre_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.6.pre_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.6.attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for part.blocks.6.attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for part.blocks.6.attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for part.blocks.6.attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.6.post_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.6.post_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.6.pre_fc_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.6.pre_fc_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.6.fc1.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for part.blocks.6.fc1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.6.post_fc_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.6.post_fc_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.6.fc2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for part.blocks.6.fc2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.7.w_resid: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.7.pre_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.7.pre_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.7.attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for part.blocks.7.attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for part.blocks.7.attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for part.blocks.7.attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.7.post_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.7.post_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.7.pre_fc_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.7.pre_fc_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.blocks.7.fc1.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for part.blocks.7.fc1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.7.post_fc_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.7.post_fc_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.blocks.7.fc2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for part.blocks.7.fc2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.0.w_resid: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.0.pre_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.0.pre_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.0.attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for part.cls_blocks.0.attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for part.cls_blocks.0.attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for part.cls_blocks.0.attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.0.post_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.0.post_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.0.pre_fc_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.0.pre_fc_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.0.fc1.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for part.cls_blocks.0.fc1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.cls_blocks.0.post_fc_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.cls_blocks.0.post_fc_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.cls_blocks.0.fc2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for part.cls_blocks.0.fc2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.1.w_resid: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.1.pre_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.1.pre_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.1.attn.in_proj_weight: copying a param with shape torch.Size([192, 64]) from checkpoint, the shape in current model is torch.Size([384, 128]).\n\tsize mismatch for part.cls_blocks.1.attn.in_proj_bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for part.cls_blocks.1.attn.out_proj.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for part.cls_blocks.1.attn.out_proj.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.1.post_attn_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.1.post_attn_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.1.pre_fc_norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.1.pre_fc_norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.cls_blocks.1.fc1.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for part.cls_blocks.1.fc1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.cls_blocks.1.post_fc_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.cls_blocks.1.post_fc_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for part.cls_blocks.1.fc2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for part.cls_blocks.1.fc2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.norm.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for part.norm.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128])."
     ]
    }
   ],
   "source": [
    "model_dir = (\n",
    "    \"models/model_2023May30/ak8_MD_inclv8_part_2reg_manual.useamp.lite.gm5.ddp-bs768-lr6p75e-3/\"\n",
    ")\n",
    "part_model.load_state_dict(\n",
    "    torch.load(model_dir + \"net_best_epoch_state.pt\", map_location=torch.device(\"cpu\"))\n",
    ")\n",
    "_ = part_model.eval()\n",
    "\n",
    "data_config = {\n",
    "    \"input_names\": [\"pf_features\", \"pf_vectors\", \"pf_mask\", \"sv_features\", \"sv_vectors\", \"sv_mask\"],\n",
    "    \"input_shapes\": {\n",
    "        \"pf_features\": (-1, 25, pfs),\n",
    "        \"pf_vectors\": (-1, 4, pfs),\n",
    "        \"pf_mask\": (-1, 1, pfs),\n",
    "        \"sv_features\": (-1, 11, svs),\n",
    "        \"sv_vectors\": (-1, 4, svs),\n",
    "        \"sv_mask\": (-1, 1, svs),\n",
    "    },\n",
    "}\n",
    "\n",
    "model_info = {\n",
    "    \"input_names\": list(data_config[\"input_names\"]),\n",
    "    \"input_shapes\": {k: ((1,) + s[1:]) for k, s in data_config[\"input_shapes\"].items()},\n",
    "    \"output_names\": [\"softmax\"],\n",
    "    \"dynamic_axes\": {\n",
    "        **{k: {0: \"N\", 2: \"n_\" + k.split(\"_\")[0]} for k in data_config[\"input_names\"]},\n",
    "        **{\"softmax\": {0: \"N\"}},\n",
    "    },\n",
    "}\n",
    "\n",
    "inputs = tuple(\n",
    "    torch.ones(model_info[\"input_shapes\"][k], dtype=torch.float32)\n",
    "    for k in model_info[\"input_names\"]\n",
    ")\n",
    "torch.onnx.export(\n",
    "    part_model,\n",
    "    inputs,\n",
    "    model_dir + \"ak8_MD_inclv8_part_2reg_manual.useamp.lite.gm5.ddp-bs768-lr6p75e-3/model.onnx\",\n",
    "    input_names=model_info[\"input_names\"],\n",
    "    output_names=model_info[\"output_names\"],\n",
    "    dynamic_axes=model_info.get(\"dynamic_axes\", None),\n",
    "    opset_version=11,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_tensors = [torch.Tensor(val) for key, val in input_dict.items()]\n",
    "in_tensors[2] = in_tensors[2].bool()\n",
    "in_tensors[5] = in_tensors[5].bool()\n",
    "\n",
    "out = part_model(*in_tensors)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dec22 Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ParticleTransformerHidden import ParticleTransformerTagger\n",
    "\n",
    "part_model = ParticleTransformerTagger(\n",
    "    pf_input_dim=25,\n",
    "    sv_input_dim=11,\n",
    "    num_classes=37 + 1,  # one dim for regression\n",
    "    # network configurations\n",
    "    pair_input_dim=4,\n",
    "    embed_dims=[128, 512, 128],\n",
    "    pair_embed_dims=[64, 64, 64],\n",
    "    num_heads=8,\n",
    "    num_layers=8,\n",
    "    num_cls_layers=2,\n",
    "    block_params=None,\n",
    "    cls_block_params={\"dropout\": 0, \"attn_dropout\": 0, \"activation_dropout\": 0},\n",
    "    fc_params=[],\n",
    "    activation=\"gelu\",\n",
    "    # misc\n",
    "    trim=True,\n",
    "    for_inference=True,\n",
    ").cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_model.load_state_dict(torch.load(\"net_best_epoch_state.pt\", map_location=torch.device(\"cpu\")))\n",
    "_ = part_model.eval()\n",
    "\n",
    "data_config = {\n",
    "    \"input_names\": [\"pf_features\", \"pf_vectors\", \"pf_mask\", \"sv_features\", \"sv_vectors\", \"sv_mask\"],\n",
    "    \"input_shapes\": {\n",
    "        \"pf_features\": (-1, 25, pfs),\n",
    "        \"pf_vectors\": (-1, 4, pfs),\n",
    "        \"pf_mask\": (-1, 1, pfs),\n",
    "        \"sv_features\": (-1, 11, svs),\n",
    "        \"sv_vectors\": (-1, 4, svs),\n",
    "        \"sv_mask\": (-1, 1, svs),\n",
    "    },\n",
    "}\n",
    "\n",
    "model_info = {\n",
    "    \"input_names\": list(data_config[\"input_names\"]),\n",
    "    \"input_shapes\": {k: ((1,) + s[1:]) for k, s in data_config[\"input_shapes\"].items()},\n",
    "    \"output_names\": [\"softmax\"],\n",
    "    \"dynamic_axes\": {\n",
    "        **{k: {0: \"N\", 2: \"n_\" + k.split(\"_\")[0]} for k in data_config[\"input_names\"]},\n",
    "        **{\"softmax\": {0: \"N\"}},\n",
    "    },\n",
    "}\n",
    "\n",
    "inputs = tuple(\n",
    "    torch.ones(model_info[\"input_shapes\"][k], dtype=torch.float32)\n",
    "    for k in model_info[\"input_names\"]\n",
    ")\n",
    "torch.onnx.export(\n",
    "    part_model,\n",
    "    inputs,\n",
    "    \"model.onnx\",\n",
    "    input_names=model_info[\"input_names\"],\n",
    "    output_names=model_info[\"output_names\"],\n",
    "    dynamic_axes=model_info.get(\"dynamic_axes\", None),\n",
    "    opset_version=11,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_tensors = [torch.Tensor(val) for key, val in input_dict.items()]\n",
    "in_tensors[2] = in_tensors[2].bool()\n",
    "in_tensors[5] = in_tensors[5].bool()\n",
    "\n",
    "out = part_model(*in_tensors)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31a7b1cb5f073f7a7d37b3db504c6954ce2b88e0f82e412b65ad0b5f2dd17394"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
