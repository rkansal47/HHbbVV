{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import tritonclient.grpc as triton_grpc\n",
    "import tritonclient.http as triton_http\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/lgray/hgg-coffea/blob/triton-bdts/src/hgg_coffea/tools/chained_quantile.py\n",
    "class wrapped_triton:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_url: str,\n",
    "    ) -> None:\n",
    "        fullprotocol, location = model_url.split(\"://\")\n",
    "        _, protocol = fullprotocol.split(\"+\")\n",
    "        address, model, version = location.split(\"/\")\n",
    "\n",
    "        self._protocol = protocol\n",
    "        self._address = address\n",
    "        self._model = model\n",
    "        self._version = version\n",
    "\n",
    "    def __call__(self, input_dict: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "        if self._protocol == \"grpc\":\n",
    "            client = triton_grpc.InferenceServerClient(url=self._address, verbose=False)\n",
    "            triton_protocol = triton_grpc\n",
    "        elif self._protocol == \"http\":\n",
    "            client = triton_http.InferenceServerClient(\n",
    "                url=self._address,\n",
    "                verbose=False,\n",
    "                concurrency=12,\n",
    "            )\n",
    "            triton_protocol = triton_http\n",
    "        else:\n",
    "            raise ValueError(f\"{self._protocol} does not encode a valid protocol (grpc or http)\")\n",
    "\n",
    "        # Infer\n",
    "        inputs = []\n",
    "\n",
    "        for key in input_dict:\n",
    "            input = triton_protocol.InferInput(key, input_dict[key].shape, \"FP32\")\n",
    "            input.set_data_from_numpy(input_dict[key])\n",
    "            inputs.append(input)\n",
    "\n",
    "        output = triton_protocol.InferRequestedOutput(\"softmax\")\n",
    "\n",
    "        request = client.infer(\n",
    "            self._model,\n",
    "            model_version=self._version,\n",
    "            inputs=inputs,\n",
    "            outputs=[output],\n",
    "        )\n",
    "\n",
    "        out = request.as_numpy(\"softmax\")\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "# pfs = 100\n",
    "# svs = 7\n",
    "pfs = 128\n",
    "svs = 10\n",
    "np.random.seed(42)\n",
    "\n",
    "# input_dict = {\n",
    "#     \"pf_points\": np.random.rand(batch_size, 2, pfs).astype(\"float32\"),\n",
    "#     \"pf_features\": np.random.rand(batch_size, 19, pfs).astype(\"float32\"),\n",
    "#     \"pf_mask\": (np.random.rand(batch_size, 1, pfs) > 0.2).astype(\"float32\"),\n",
    "#     \"sv_points\": np.random.rand(batch_size, 2, svs).astype(\"float32\"),\n",
    "#     \"sv_features\": np.random.rand(batch_size, 11, svs).astype(\"float32\"),\n",
    "#     \"sv_mask\": (np.random.rand(batch_size, 1, svs) > 0.2).astype(\"float32\"),\n",
    "# }\n",
    "\n",
    "input_dict = {\n",
    "    \"pf_features\": np.random.rand(batch_size, 25, pfs).astype(\"float32\"),\n",
    "    \"pf_vectors\": np.random.rand(batch_size, 4, pfs).astype(\"float32\"),\n",
    "    \"pf_mask\": (np.random.rand(batch_size, 1, pfs) > 0.2).astype(\"float32\"),\n",
    "    \"sv_features\": np.random.rand(batch_size, 11, svs).astype(\"float32\"),\n",
    "    \"sv_vectors\": np.random.rand(batch_size, 4, svs).astype(\"float32\"),\n",
    "    \"sv_mask\": (np.random.rand(batch_size, 1, svs) > 0.2).astype(\"float32\"),\n",
    "}\n",
    "\n",
    "# input_dict = {\n",
    "#     \"pf_points__0\": np.random.rand(batch_size, 2, pfs).astype(\"float32\"),\n",
    "#     \"pf_features__1\": np.random.rand(batch_size, 19, pfs).astype(\"float32\"),\n",
    "#     \"pf_mask__2\": (np.random.rand(batch_size, 1, pfs) > 0.2).astype(\"float32\"),\n",
    "#     \"sv_points__3\": np.random.rand(batch_size, 2, svs).astype(\"float32\"),\n",
    "#     \"sv_features__4\": np.random.rand(batch_size, 11, svs).astype(\"float32\"),\n",
    "#     \"sv_mask__5\": (np.random.rand(batch_size, 1, svs) > 0.2).astype(\"float32\"),\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_url = \"triton+grpc://ailab01.fnal.gov:8001/particlenet_hww/1\"\n",
    "# model_url = \"triton+grpc://prp-gpu-1.t2.ucsd.edu:8001/particlenet_hww/1\"\n",
    "# model_url = \"triton+grpc://67.58.49.48:8001/ak8_MD_vminclv2ParT_manual_fixwrap/1\"\n",
    "model_url = \"triton+grpc://67.58.49.48:8001/2023May30_ak8_MD_inclv8_part_2reg_manual/1\"\n",
    "triton_model = wrapped_triton(model_url)\n",
    "for i in tqdm(range(1)):\n",
    "    output = triton_model(input_dict)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "model_dir = (\n",
    "    \"models/model_2023May30/ak8_MD_inclv8_part_2reg_manual.useamp.lite.gm5.ddp-bs768-lr6p75e-3/\"\n",
    ")\n",
    "\n",
    "onnx_model = onnx.load(model_dir + \"model.onnx\")\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "ort_sess = ort.InferenceSession(model_dir + \"model.onnx\")\n",
    "outputs = ort_sess.run(None, input_dict)[0]\n",
    "print(\"ONNX outputs:\", outputs)\n",
    "print(\"Shape:\", outputs.shape)\n",
    "print(\"Softmax applied:\", np.allclose(np.sum(outputs, axis=1), 1, atol=1e-5))\n",
    "\n",
    "# Print Result\n",
    "# predicted, actual = classes[outputs[0][0].argmax(0)], classes[y]\n",
    "# print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "model_dir = (\n",
    "    \"models/model_2023May30/ak8_MD_inclv8_part_2reg_manual.useamp.lite.gm5.ddp-bs768-lr6p75e-3/\"\n",
    ")\n",
    "\n",
    "onnx_model = onnx.load(model_dir + \"model.onnx\")\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "ort_sess = ort.InferenceSession(model_dir + \"model.onnx\")\n",
    "outputs = ort_sess.run(None, input_dict)\n",
    "print(\"ONNX outputs:\", outputs[0])\n",
    "print(\"Shape:\", outputs[0].shape)\n",
    "print(\"Softmax applied:\", np.allclose(np.sum(outputs[0], axis=1), 1, atol=1e-5))\n",
    "\n",
    "# Print Result\n",
    "# predicted, actual = classes[outputs[0][0].argmax(0)], classes[y]\n",
    "# print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ONNX cls output sums:\", np.sum(outputs[0][:, :-2], axis=1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023May30 Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ParticleTransformer import ParticleTransformerTagger\n",
    "\n",
    "part_model = ParticleTransformerTagger(\n",
    "    pf_input_dim=25,\n",
    "    sv_input_dim=11,\n",
    "    num_classes=314,  # one dim for regression\n",
    "    # network configurations\n",
    "    pair_input_dim=4,\n",
    "    embed_dims=[128, 512, 128],\n",
    "    pair_embed_dims=[64, 64, 64],\n",
    "    num_heads=8,\n",
    "    num_layers=8,\n",
    "    num_cls_layers=2,\n",
    "    block_params=None,\n",
    "    cls_block_params={\"dropout\": 0, \"attn_dropout\": 0, \"activation_dropout\": 0},\n",
    "    fc_params=[],\n",
    "    activation=\"gelu\",\n",
    "    # misc\n",
    "    trim=True,\n",
    "    for_inference=True,\n",
    ").cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = (\n",
    "    \"models/model_2023May30/ak8_MD_inclv8_part_2reg_manual.useamp.lite.gm5.ddp-bs768-lr6p75e-3/\"\n",
    ")\n",
    "part_model.load_state_dict(\n",
    "    torch.load(model_dir + \"net_best_epoch_state.pt\", map_location=torch.device(\"cpu\"))\n",
    ")\n",
    "_ = part_model.eval()\n",
    "\n",
    "data_config = {\n",
    "    \"input_names\": [\"pf_features\", \"pf_vectors\", \"pf_mask\", \"sv_features\", \"sv_vectors\", \"sv_mask\"],\n",
    "    \"input_shapes\": {\n",
    "        \"pf_features\": (-1, 25, pfs),\n",
    "        \"pf_vectors\": (-1, 4, pfs),\n",
    "        \"pf_mask\": (-1, 1, pfs),\n",
    "        \"sv_features\": (-1, 11, svs),\n",
    "        \"sv_vectors\": (-1, 4, svs),\n",
    "        \"sv_mask\": (-1, 1, svs),\n",
    "    },\n",
    "}\n",
    "\n",
    "model_info = {\n",
    "    \"input_names\": list(data_config[\"input_names\"]),\n",
    "    \"input_shapes\": {k: ((1,) + s[1:]) for k, s in data_config[\"input_shapes\"].items()},\n",
    "    \"output_names\": [\"softmax\"],\n",
    "    \"dynamic_axes\": {\n",
    "        **{k: {0: \"N\", 2: \"n_\" + k.split(\"_\")[0]} for k in data_config[\"input_names\"]},\n",
    "        \"softmax\": {0: \"N\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "inputs = tuple(\n",
    "    torch.ones(model_info[\"input_shapes\"][k], dtype=torch.float32)\n",
    "    for k in model_info[\"input_names\"]\n",
    ")\n",
    "torch.onnx.export(\n",
    "    part_model,\n",
    "    inputs,\n",
    "    model_dir + \"ak8_MD_inclv8_part_2reg_manual.useamp.lite.gm5.ddp-bs768-lr6p75e-3/model.onnx\",\n",
    "    input_names=model_info[\"input_names\"],\n",
    "    output_names=model_info[\"output_names\"],\n",
    "    dynamic_axes=model_info.get(\"dynamic_axes\"),\n",
    "    opset_version=11,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_tensors = [torch.Tensor(val) for key, val in input_dict.items()]\n",
    "in_tensors[2] = in_tensors[2].bool()\n",
    "in_tensors[5] = in_tensors[5].bool()\n",
    "\n",
    "out = part_model(*in_tensors)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dec22 Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ParticleTransformerHidden import ParticleTransformerTagger\n",
    "\n",
    "part_model = ParticleTransformerTagger(\n",
    "    pf_input_dim=25,\n",
    "    sv_input_dim=11,\n",
    "    num_classes=37 + 1,  # one dim for regression\n",
    "    # network configurations\n",
    "    pair_input_dim=4,\n",
    "    embed_dims=[128, 512, 128],\n",
    "    pair_embed_dims=[64, 64, 64],\n",
    "    num_heads=8,\n",
    "    num_layers=8,\n",
    "    num_cls_layers=2,\n",
    "    block_params=None,\n",
    "    cls_block_params={\"dropout\": 0, \"attn_dropout\": 0, \"activation_dropout\": 0},\n",
    "    fc_params=[],\n",
    "    activation=\"gelu\",\n",
    "    # misc\n",
    "    trim=True,\n",
    "    for_inference=True,\n",
    ").cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_model.load_state_dict(torch.load(\"net_best_epoch_state.pt\", map_location=torch.device(\"cpu\")))\n",
    "_ = part_model.eval()\n",
    "\n",
    "data_config = {\n",
    "    \"input_names\": [\"pf_features\", \"pf_vectors\", \"pf_mask\", \"sv_features\", \"sv_vectors\", \"sv_mask\"],\n",
    "    \"input_shapes\": {\n",
    "        \"pf_features\": (-1, 25, pfs),\n",
    "        \"pf_vectors\": (-1, 4, pfs),\n",
    "        \"pf_mask\": (-1, 1, pfs),\n",
    "        \"sv_features\": (-1, 11, svs),\n",
    "        \"sv_vectors\": (-1, 4, svs),\n",
    "        \"sv_mask\": (-1, 1, svs),\n",
    "    },\n",
    "}\n",
    "\n",
    "model_info = {\n",
    "    \"input_names\": list(data_config[\"input_names\"]),\n",
    "    \"input_shapes\": {k: ((1,) + s[1:]) for k, s in data_config[\"input_shapes\"].items()},\n",
    "    \"output_names\": [\"softmax\"],\n",
    "    \"dynamic_axes\": {\n",
    "        **{k: {0: \"N\", 2: \"n_\" + k.split(\"_\")[0]} for k in data_config[\"input_names\"]},\n",
    "        \"softmax\": {0: \"N\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "inputs = tuple(\n",
    "    torch.ones(model_info[\"input_shapes\"][k], dtype=torch.float32)\n",
    "    for k in model_info[\"input_names\"]\n",
    ")\n",
    "torch.onnx.export(\n",
    "    part_model,\n",
    "    inputs,\n",
    "    \"model.onnx\",\n",
    "    input_names=model_info[\"input_names\"],\n",
    "    output_names=model_info[\"output_names\"],\n",
    "    dynamic_axes=model_info.get(\"dynamic_axes\", None),\n",
    "    opset_version=11,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_tensors = [torch.Tensor(val) for key, val in input_dict.items()]\n",
    "in_tensors[2] = in_tensors[2].bool()\n",
    "in_tensors[5] = in_tensors[5].bool()\n",
    "\n",
    "out = part_model(*in_tensors)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
