{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tritonclient.grpc as triton_grpc\n",
    "import tritonclient.http as triton_http\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from https://github.com/lgray/hgg-coffea/blob/triton-bdts/src/hgg_coffea/tools/chained_quantile.py\n",
    "class wrapped_triton:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_url: str,\n",
    "    ) -> None:\n",
    "        fullprotocol, location = model_url.split(\"://\")\n",
    "        _, protocol = fullprotocol.split(\"+\")\n",
    "        address, model, version = location.split(\"/\")\n",
    "\n",
    "        self._protocol = protocol\n",
    "        self._address = address\n",
    "        self._model = model\n",
    "        self._version = version\n",
    "\n",
    "    def __call__(self, input_dict: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "        if self._protocol == \"grpc\":\n",
    "            client = triton_grpc.InferenceServerClient(url=self._address, verbose=False)\n",
    "            triton_protocol = triton_grpc\n",
    "        elif self._protocol == \"http\":\n",
    "            client = triton_http.InferenceServerClient(\n",
    "                url=self._address,\n",
    "                verbose=False,\n",
    "                concurrency=12,\n",
    "            )\n",
    "            triton_protocol = triton_http\n",
    "        else:\n",
    "            raise ValueError(f\"{self._protocol} does not encode a valid protocol (grpc or http)\")\n",
    "\n",
    "        # Infer\n",
    "        inputs = []\n",
    "\n",
    "        for key in input_dict:\n",
    "            input = triton_protocol.InferInput(key, input_dict[key].shape, \"FP32\")\n",
    "            input.set_data_from_numpy(input_dict[key])\n",
    "            inputs.append(input)\n",
    "\n",
    "        output = triton_protocol.InferRequestedOutput(\"softmax\")\n",
    "\n",
    "        request = client.infer(\n",
    "            self._model,\n",
    "            model_version=self._version,\n",
    "            inputs=inputs,\n",
    "            outputs=[output],\n",
    "        )\n",
    "\n",
    "        out = request.as_numpy(\"softmax\")\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 880\n",
    "# pfs = 100\n",
    "# svs = 7\n",
    "pfs = 128\n",
    "svs = 10\n",
    "\n",
    "# input_dict = {\n",
    "#     \"pf_points\": np.random.rand(batch_size, 2, pfs).astype(\"float32\"),\n",
    "#     \"pf_features\": np.random.rand(batch_size, 19, pfs).astype(\"float32\"),\n",
    "#     \"pf_mask\": (np.random.rand(batch_size, 1, pfs) > 0.2).astype(\"float32\"),\n",
    "#     \"sv_points\": np.random.rand(batch_size, 2, svs).astype(\"float32\"),\n",
    "#     \"sv_features\": np.random.rand(batch_size, 11, svs).astype(\"float32\"),\n",
    "#     \"sv_mask\": (np.random.rand(batch_size, 1, svs) > 0.2).astype(\"float32\"),\n",
    "# }\n",
    "\n",
    "input_dict = {\n",
    "    \"pf_features\": np.random.rand(batch_size, 25, pfs).astype(\"float32\"),\n",
    "    \"pf_vectors\": np.random.rand(batch_size, 4, pfs).astype(\"float32\"),\n",
    "    \"pf_mask\": (np.random.rand(batch_size, 1, pfs) > 0.2).astype(\"float32\"),\n",
    "    \"sv_features\": np.random.rand(batch_size, 11, svs).astype(\"float32\"),\n",
    "    \"sv_vectors\": np.random.rand(batch_size, 4, svs).astype(\"float32\"),\n",
    "    \"sv_mask\": (np.random.rand(batch_size, 1, svs) > 0.2).astype(\"float32\"),\n",
    "}\n",
    "\n",
    "# input_dict = {\n",
    "#     \"pf_points__0\": np.random.rand(batch_size, 2, pfs).astype(\"float32\"),\n",
    "#     \"pf_features__1\": np.random.rand(batch_size, 19, pfs).astype(\"float32\"),\n",
    "#     \"pf_mask__2\": (np.random.rand(batch_size, 1, pfs) > 0.2).astype(\"float32\"),\n",
    "#     \"sv_points__3\": np.random.rand(batch_size, 2, svs).astype(\"float32\"),\n",
    "#     \"sv_features__4\": np.random.rand(batch_size, 11, svs).astype(\"float32\"),\n",
    "#     \"sv_mask__5\": (np.random.rand(batch_size, 1, svs) > 0.2).astype(\"float32\"),\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.4366233e-03 2.7789180e-03 1.4810998e-03 ... 1.3419604e-03\n",
      "  5.3415606e-03 1.0536162e-03]\n",
      " [5.8962428e-04 2.6495671e-03 2.1507419e-03 ... 3.6234359e-04\n",
      "  3.9466871e-05 1.1135796e-02]\n",
      " [1.8111541e-03 6.1054532e-03 4.3359781e-03 ... 4.5285108e-03\n",
      "  4.9895938e-03 4.3286658e-03]\n",
      " ...\n",
      " [2.6687947e-03 5.2311928e-03 2.8158592e-03 ... 3.7654082e-03\n",
      "  1.0825735e-02 2.2049018e-03]\n",
      " [2.7392500e-03 2.7328818e-03 1.0838965e-03 ... 2.2412995e-03\n",
      "  1.9178806e-03 1.2975576e-03]\n",
      " [1.9648573e-03 4.6280641e-03 3.3738450e-03 ... 3.7505146e-04\n",
      "  7.2018191e-04 2.7100766e-02]]\n"
     ]
    }
   ],
   "source": [
    "# model_url = \"triton+grpc://ailab01.fnal.gov:8001/particlenet_hww/1\"\n",
    "# model_url = \"triton+grpc://prp-gpu-1.t2.ucsd.edu:8001/particlenet_hww/1\"\n",
    "model_url = \"triton+grpc://67.58.49.52:8001/ak8_MD_vminclv2ParT_manual_fixwrap/1\"\n",
    "# model_url = \"triton+grpc://localhost:8001/particlenet_hww_ul_4q_3q/1\"\n",
    "# model_url = \"triton+grpc://67.58.49.52:8001/particlenet_hww_ul_4q_3q/1\"\n",
    "triton_model = wrapped_triton(model_url)\n",
    "output = triton_model(input_dict)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[2.40560714e-03, 4.73927893e-03, 4.42884630e-03, 7.70280650e-03,\n",
      "        7.44594727e-03, 4.89646792e-02, 2.44864757e-04, 1.84084594e-04,\n",
      "        3.34733166e-03, 5.55951335e-03, 2.30525038e-04, 5.30063233e-04,\n",
      "        1.57437345e-03, 1.06911850e-03, 2.14584306e-05, 1.38277428e-05,\n",
      "        2.32464038e-02, 5.26734889e-02, 9.97571088e-03, 1.04071777e-02,\n",
      "        3.24207067e-05, 3.28148562e-05, 7.48393745e-07, 7.66754225e-02,\n",
      "        9.75757986e-02, 1.65003791e-01, 7.47575983e-02, 1.00998513e-01,\n",
      "        1.26040773e-02, 4.62465100e-02, 6.31558970e-02, 1.60297364e-01,\n",
      "        6.35669276e-04, 1.13147628e-02, 3.94817034e-04, 9.87346284e-04,\n",
      "        4.52140672e-03],\n",
      "       [1.02613051e-03, 3.39163793e-03, 2.07269844e-03, 8.19048751e-03,\n",
      "        1.25318337e-02, 4.28423323e-02, 6.32008596e-04, 6.25702844e-04,\n",
      "        1.71326788e-03, 1.57124607e-03, 5.84401190e-04, 2.10922817e-03,\n",
      "        1.14760280e-03, 5.08062483e-04, 1.14137052e-04, 8.69628202e-05,\n",
      "        1.43004835e-01, 4.31774035e-02, 8.73050466e-03, 9.02655907e-03,\n",
      "        1.45769263e-05, 3.80198912e-07, 4.15077039e-07, 5.90641461e-02,\n",
      "        3.09982877e-02, 8.30990672e-02, 2.94297803e-02, 2.74373926e-02,\n",
      "        4.21226285e-02, 1.29914016e-01, 2.06379488e-01, 9.11063701e-02,\n",
      "        4.21749521e-03, 4.15934157e-03, 2.49783974e-03, 5.44063095e-03,\n",
      "        1.03111682e-03],\n",
      "       [1.66845182e-03, 3.64692812e-03, 2.89837131e-03, 4.86101676e-03,\n",
      "        7.48188701e-03, 1.40794534e-02, 9.03459208e-04, 5.26467687e-04,\n",
      "        2.02787109e-03, 1.55750872e-03, 3.14598088e-04, 5.91911084e-04,\n",
      "        1.07947027e-03, 3.05540278e-04, 4.22612320e-05, 2.96632879e-05,\n",
      "        5.50755933e-02, 4.36562598e-02, 5.99780446e-03, 8.71451851e-03,\n",
      "        2.17835313e-05, 8.61926401e-06, 4.27868457e-07, 9.02741998e-02,\n",
      "        7.14722201e-02, 1.86128020e-01, 6.56804517e-02, 6.22666441e-02,\n",
      "        3.44388224e-02, 7.51931965e-02, 1.05264127e-01, 1.22886643e-01,\n",
      "        4.66966070e-03, 1.13344304e-02, 1.43859303e-03, 6.60695415e-03,\n",
      "        6.85623521e-03],\n",
      "       [4.62662056e-03, 7.83766061e-03, 4.27037384e-03, 2.01767706e-03,\n",
      "        2.24544550e-03, 6.52226200e-03, 6.22416555e-04, 1.64934585e-03,\n",
      "        3.05514634e-02, 8.17193165e-02, 3.25232482e-04, 3.42901098e-03,\n",
      "        9.88645386e-03, 4.62830858e-03, 2.20900802e-06, 7.18750107e-06,\n",
      "        3.09956577e-02, 7.39605129e-02, 1.03556328e-02, 1.61585175e-02,\n",
      "        1.37844609e-05, 2.62309564e-04, 6.64427091e-08, 3.93500216e-02,\n",
      "        4.16316763e-02, 3.23371440e-01, 8.53620991e-02, 9.06055272e-02,\n",
      "        6.37303432e-03, 2.40937900e-02, 2.08340436e-02, 2.99450755e-02,\n",
      "        1.62140757e-03, 3.63662280e-02, 6.53141615e-05, 1.40187982e-03,\n",
      "        6.89101219e-03],\n",
      "       [2.03629909e-03, 3.15504754e-03, 1.71309232e-03, 2.29708035e-03,\n",
      "        2.39835400e-03, 6.03561243e-03, 1.54996669e-04, 1.23658392e-04,\n",
      "        1.07710389e-02, 7.09561491e-03, 1.81801792e-04, 9.05116263e-04,\n",
      "        1.09113259e-02, 1.86760747e-03, 3.02845388e-06, 6.00468229e-06,\n",
      "        1.14395060e-01, 6.80042058e-02, 1.49212694e-02, 2.08507776e-02,\n",
      "        3.21558014e-06, 2.38054636e-05, 2.97100264e-08, 5.11101894e-02,\n",
      "        2.69963164e-02, 1.98006168e-01, 7.73117244e-02, 9.26602259e-02,\n",
      "        1.68176554e-02, 5.41842990e-02, 1.02826640e-01, 8.60600099e-02,\n",
      "        7.84013944e-04, 1.38756111e-02, 4.92379710e-04, 2.19748821e-03,\n",
      "        8.82323179e-03],\n",
      "       [2.63676280e-03, 6.06680289e-03, 2.61350255e-03, 6.36544265e-03,\n",
      "        7.56744714e-03, 4.82675806e-02, 1.05008869e-04, 1.23717589e-04,\n",
      "        1.32966079e-02, 2.92983353e-02, 8.67534036e-05, 6.88784232e-04,\n",
      "        4.77317069e-03, 2.55475705e-03, 7.38689141e-06, 1.30460821e-05,\n",
      "        1.34585416e-02, 6.27045482e-02, 9.64016002e-03, 1.29977288e-02,\n",
      "        1.14714576e-05, 8.77021375e-05, 2.60991186e-07, 7.38220736e-02,\n",
      "        9.11559612e-02, 1.41583651e-01, 8.99177194e-02, 6.93340972e-02,\n",
      "        1.05566159e-02, 4.37865630e-02, 7.43102282e-02, 1.22026905e-01,\n",
      "        5.21351583e-04, 5.20420112e-02, 1.01869373e-04, 4.79609822e-04,\n",
      "        6.99579995e-03],\n",
      "       [2.19087978e-03, 3.23095312e-03, 1.49549206e-03, 1.28474133e-02,\n",
      "        7.49933533e-03, 1.14605520e-02, 3.56867473e-04, 1.32475878e-04,\n",
      "        5.62349148e-03, 2.78597814e-03, 1.64682046e-04, 1.71040010e-04,\n",
      "        1.69672316e-03, 4.03090526e-04, 4.78661968e-05, 2.36598516e-05,\n",
      "        8.32541436e-02, 2.19073035e-02, 7.09187146e-03, 1.14430049e-02,\n",
      "        4.83168042e-05, 3.79924204e-05, 1.96322685e-06, 1.08277187e-01,\n",
      "        2.30791755e-02, 1.68793306e-01, 3.30831185e-02, 6.27726912e-02,\n",
      "        3.64046171e-02, 5.83925880e-02, 1.90397322e-01, 1.00474246e-01,\n",
      "        2.79459194e-03, 2.67766379e-02, 2.18242058e-03, 2.98561528e-03,\n",
      "        9.67142545e-03],\n",
      "       [2.60602473e-03, 6.06194045e-03, 6.48480467e-03, 5.24291350e-03,\n",
      "        6.60770806e-03, 2.93504205e-02, 1.40409567e-04, 1.17491698e-04,\n",
      "        3.11701819e-02, 3.09709739e-02, 7.95513479e-05, 3.12839460e-04,\n",
      "        8.74308869e-03, 3.68170021e-03, 4.41717639e-05, 3.66688473e-05,\n",
      "        6.22019544e-02, 1.28152937e-01, 4.04050248e-03, 7.28918752e-03,\n",
      "        3.56937699e-06, 5.69726799e-05, 2.66550273e-07, 4.27597389e-02,\n",
      "        1.01912670e-01, 1.32168725e-01, 1.18109092e-01, 9.89402831e-02,\n",
      "        9.14235692e-03, 4.03584167e-02, 3.69383506e-02, 4.38604988e-02,\n",
      "        1.94850640e-04, 3.02860625e-02, 4.43454715e-04, 4.01076773e-04,\n",
      "        1.10881403e-02],\n",
      "       [9.72755661e-04, 2.83560762e-03, 2.09250348e-03, 7.08562974e-03,\n",
      "        1.99222248e-02, 1.70437302e-02, 6.27118628e-04, 4.44679637e-04,\n",
      "        7.59567250e-04, 6.22394960e-04, 2.68107106e-04, 7.24267273e-04,\n",
      "        4.60452429e-04, 2.34048755e-04, 1.98413749e-04, 1.55608912e-04,\n",
      "        7.50128031e-02, 3.14960107e-02, 5.07137878e-03, 4.78138635e-03,\n",
      "        9.16231329e-06, 4.43051817e-07, 4.91673404e-07, 4.33409438e-02,\n",
      "        5.18652536e-02, 1.26850814e-01, 6.27304837e-02, 7.75047541e-02,\n",
      "        3.53763327e-02, 9.05281901e-02, 1.76466465e-01, 1.30844772e-01,\n",
      "        6.25297474e-03, 8.51495564e-03, 7.06573855e-03, 8.18227977e-03,\n",
      "        3.65726859e-03],\n",
      "       [1.18808018e-03, 2.60169245e-03, 1.44573965e-03, 1.61221239e-03,\n",
      "        2.26570200e-03, 6.82756910e-03, 1.22456395e-05, 1.75701007e-05,\n",
      "        1.45553514e-01, 2.46136382e-01, 5.47590980e-06, 3.54246076e-05,\n",
      "        1.04971025e-02, 5.45290019e-03, 5.14718386e-06, 7.95967026e-06,\n",
      "        1.66995171e-02, 3.97547148e-02, 3.19603574e-03, 6.24249410e-03,\n",
      "        9.24222513e-06, 4.82693547e-03, 3.04615833e-06, 1.89753268e-02,\n",
      "        1.23844156e-02, 6.41736761e-02, 1.92459356e-02, 2.05983259e-02,\n",
      "        3.28172417e-03, 9.90303326e-03, 2.93984395e-02, 9.32308938e-03,\n",
      "        6.60474543e-05, 2.99182653e-01, 1.61686854e-04, 4.12492227e-05,\n",
      "        1.88677423e-02]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "onnx_model = onnx.load(\"model.onnx\")\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "ort_sess = ort.InferenceSession('model.onnx')\n",
    "outputs = ort_sess.run(None, input_dict)\n",
    "print(outputs)\n",
    "\n",
    "# Print Result \n",
    "# predicted, actual = classes[outputs[0][0].argmax(0)], classes[y]\n",
    "# print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 166)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ParticleTransformerHidden import ParticleTransformerTagger\n",
    "\n",
    "part_model = ParticleTransformerTagger(\n",
    "    pf_input_dim=25,\n",
    "    sv_input_dim=11,\n",
    "    num_classes=37 + 1, # one dim for regression\n",
    "    # network configurations\n",
    "    pair_input_dim=4,\n",
    "    embed_dims=[128, 512, 128],\n",
    "    pair_embed_dims=[64, 64, 64],\n",
    "    num_heads=8,\n",
    "    num_layers=8,\n",
    "    num_cls_layers=2,\n",
    "    block_params=None,\n",
    "    cls_block_params={'dropout': 0, 'attn_dropout': 0, 'activation_dropout': 0},\n",
    "    fc_params=[],\n",
    "    activation='gelu',\n",
    "    # misc\n",
    "    trim=True,\n",
    "    for_inference=True,\n",
    ").cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/python310/lib/python3.10/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "/opt/anaconda3/envs/python310/lib/python3.10/site-packages/torch/nn/functional.py:5046: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert embed_dim == embed_dim_to_check, \\\n",
      "/opt/anaconda3/envs/python310/lib/python3.10/site-packages/torch/nn/functional.py:5053: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert head_dim * num_heads == embed_dim, f\"embed_dim {embed_dim} not divisible by num_heads {num_heads}\"\n",
      "/opt/anaconda3/envs/python310/lib/python3.10/site-packages/torch/nn/functional.py:5059: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert key.shape == value.shape, f\"key shape {key.shape} does not match value shape {value.shape}\"\n",
      "/opt/anaconda3/envs/python310/lib/python3.10/site-packages/torch/nn/functional.py:5093: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_mask.shape != correct_3d_size:\n",
      "/opt/anaconda3/envs/python310/lib/python3.10/site-packages/torch/nn/functional.py:5155: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert key_padding_mask.shape == (bsz, src_len), \\\n",
      "/opt/anaconda3/envs/python310/lib/python3.10/site-packages/torch/nn/functional.py:4849: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  q = q / math.sqrt(E)\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    }
   ],
   "source": [
    "part_model.load_state_dict(torch.load(\"net_best_epoch_state.pt\", map_location=torch.device(\"cpu\")))\n",
    "_ = part_model.eval()\n",
    "\n",
    "data_config = {\n",
    "    \"input_names\": [\"pf_features\", \"pf_vectors\", \"pf_mask\", \"sv_features\", \"sv_vectors\", \"sv_mask\"],\n",
    "    \"input_shapes\": {\n",
    "        \"pf_features\": (-1, 25, pfs),\n",
    "        \"pf_vectors\": (-1, 4, pfs),\n",
    "        \"pf_mask\": (-1, 1, pfs),\n",
    "        \"sv_features\": (-1, 11, svs),\n",
    "        \"sv_vectors\": (-1, 4, svs),\n",
    "        \"sv_mask\": (-1, 1, svs),\n",
    "    },\n",
    "}\n",
    "\n",
    "model_info = {\n",
    "    \"input_names\": list(data_config[\"input_names\"]),\n",
    "    \"input_shapes\": {k: ((1,) + s[1:]) for k, s in data_config[\"input_shapes\"].items()},\n",
    "    \"output_names\": [\"softmax\"],\n",
    "    \"dynamic_axes\": {\n",
    "        **{k: {0: \"N\", 2: \"n_\" + k.split(\"_\")[0]} for k in data_config[\"input_names\"]},\n",
    "        **{\"softmax\": {0: \"N\"}},\n",
    "    },\n",
    "}\n",
    "\n",
    "inputs = tuple(\n",
    "    torch.ones(model_info[\"input_shapes\"][k], dtype=torch.float32)\n",
    "    for k in model_info[\"input_names\"]\n",
    ")\n",
    "torch.onnx.export(\n",
    "    part_model,\n",
    "    inputs,\n",
    "    \"model.onnx\",\n",
    "    input_names=model_info[\"input_names\"],\n",
    "    output_names=model_info[\"output_names\"],\n",
    "    dynamic_axes=model_info.get(\"dynamic_axes\", None),\n",
    "    opset_version=11,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5299e-03,  2.9319e-03,  1.9769e-03,  5.4878e-03,  7.8053e-03,\n",
      "          1.2931e-02,  1.3580e-04,  6.4316e-05,  5.6223e-03,  3.5439e-03,\n",
      "          5.7096e-05,  7.1246e-05,  2.4305e-03,  7.1927e-04,  4.5472e-05,\n",
      "          2.3293e-05,  7.5924e-02,  3.5818e-02,  8.9561e-03,  9.3922e-03,\n",
      "          4.7991e-06,  2.2116e-05,  4.7249e-07,  9.0073e-02,  4.4058e-02,\n",
      "          1.9110e-01,  5.3661e-02,  8.5326e-02,  2.5597e-02,  6.6441e-02,\n",
      "          1.0755e-01,  1.1822e-01,  5.6389e-04,  2.4701e-02,  1.2583e-03,\n",
      "          8.0306e-04,  1.5152e-02,  1.1746e+02, -1.4494e+00, -3.8877e+00,\n",
      "         -4.1724e-01, -2.1862e+00,  4.6684e-01,  2.2150e-01,  2.5049e-01,\n",
      "          1.4588e+00,  5.7455e-01,  6.8124e-01, -2.3397e+00, -7.7287e-01,\n",
      "          7.0747e-01,  1.9500e+00, -3.2054e-01, -3.3750e-01, -3.7859e-01,\n",
      "         -3.5869e-01,  8.4656e-01,  2.2947e+00, -5.7608e-01,  1.1714e+00,\n",
      "         -9.2651e-02, -8.0103e-01,  2.5926e-02,  1.4265e+00, -2.4823e+00,\n",
      "          1.6136e+00,  1.8963e+00,  6.2551e+00,  6.8286e-01, -3.8628e-01,\n",
      "         -1.1575e+00,  6.8476e-01,  8.5872e-01, -7.8506e-01,  2.7799e+00,\n",
      "          4.6129e-01,  7.1114e-01, -3.3843e-02, -5.5794e-01, -2.9919e-01,\n",
      "         -1.4846e+00,  4.3219e-01, -1.4646e-01,  1.8458e+00, -2.4080e-01,\n",
      "          1.0279e+00, -1.6629e+00, -7.0566e-01, -1.8657e-01, -7.4921e-02,\n",
      "         -1.1353e+00,  1.0703e-01,  2.0712e-01, -2.2964e+00,  1.6433e+00,\n",
      "          8.9940e-02,  3.7805e-01, -2.2146e+00, -5.3493e-01, -4.0156e-02,\n",
      "         -5.8408e-01, -8.0333e-01, -2.4586e+00,  9.8550e-02, -3.2820e+00,\n",
      "         -1.4683e+00,  7.9389e+00,  6.2665e-01,  1.6939e-01,  1.1479e+00,\n",
      "         -3.5398e-01,  3.5816e-01, -1.0650e+00, -1.2613e+00, -1.1165e+00,\n",
      "         -1.5212e-01, -7.9048e-01, -2.9893e-02, -1.7999e+00,  4.1555e-01,\n",
      "         -9.1506e+00,  2.9598e-01, -1.1166e-01, -2.2689e+00, -4.0743e-01,\n",
      "          2.9964e+00,  1.1030e+00, -4.7359e+00,  1.6196e-01,  9.1396e-01,\n",
      "          1.6023e+00,  9.6509e-01,  1.3732e-02, -1.0029e+00,  2.4971e+00,\n",
      "          1.9553e+00, -1.7973e+00, -1.4033e+00, -7.1935e-01, -2.5925e-01,\n",
      "         -2.3469e-01, -8.4961e-01, -5.0476e-01, -6.1761e-01, -9.6350e-01,\n",
      "         -1.0327e+00,  5.3326e-01,  3.7144e+00,  5.2717e-01,  7.0532e-01,\n",
      "          2.7586e-01, -7.5351e-01,  9.4370e-01,  2.7028e-01, -9.8784e-02,\n",
      "         -3.4679e-01,  1.8617e-01, -3.3025e-01,  5.8429e-02,  1.1500e-01,\n",
      "          2.0636e-01,  4.2734e-01,  5.0767e-02, -1.4037e+00,  2.5175e+00,\n",
      "          1.6230e+00]], grad_fn=<CatBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/python310/lib/python3.10/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    }
   ],
   "source": [
    "in_tensors = [torch.Tensor(val) for key, val in input_dict.items()]\n",
    "in_tensors[2] = in_tensors[2].bool()\n",
    "in_tensors[5] = in_tensors[5].bool()\n",
    "\n",
    "out = part_model(*in_tensors)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.52987312e-03,  2.93191988e-03,  1.97693706e-03,\n",
       "         5.48777496e-03,  7.80526781e-03,  1.29312817e-02,\n",
       "         1.35801572e-04,  6.43163876e-05,  5.62225794e-03,\n",
       "         3.54393735e-03,  5.70958327e-05,  7.12459951e-05,\n",
       "         2.43050908e-03,  7.19271193e-04,  4.54720794e-05,\n",
       "         2.32931434e-05,  7.59244114e-02,  3.58182155e-02,\n",
       "         8.95608403e-03,  9.39224847e-03,  4.79914434e-06,\n",
       "         2.21154678e-05,  4.72491791e-07,  9.00733918e-02,\n",
       "         4.40584160e-02,  1.91101983e-01,  5.36605977e-02,\n",
       "         8.53259340e-02,  2.55966950e-02,  6.64411336e-02,\n",
       "         1.07548602e-01,  1.18220098e-01,  5.63886017e-04,\n",
       "         2.47012507e-02,  1.25828688e-03,  8.03059200e-04,\n",
       "         1.51520669e-02,  1.17462372e+02, -1.44942522e+00,\n",
       "        -3.88765955e+00, -4.17238146e-01, -2.18617630e+00,\n",
       "         4.66837943e-01,  2.21496135e-01,  2.50485390e-01,\n",
       "         1.45877016e+00,  5.74553132e-01,  6.81239963e-01,\n",
       "        -2.33971190e+00, -7.72875249e-01,  7.07470357e-01,\n",
       "         1.94998372e+00, -3.20540398e-01, -3.37501228e-01,\n",
       "        -3.78589600e-01, -3.58688295e-01,  8.46563578e-01,\n",
       "         2.29472423e+00, -5.76084495e-01,  1.17144680e+00,\n",
       "        -9.26510617e-02, -8.01026106e-01,  2.59260591e-02,\n",
       "         1.42652380e+00, -2.48230267e+00,  1.61363983e+00,\n",
       "         1.89631426e+00,  6.25512600e+00,  6.82864428e-01,\n",
       "        -3.86280149e-01, -1.15748513e+00,  6.84761107e-01,\n",
       "         8.58720064e-01, -7.85056114e-01,  2.77994680e+00,\n",
       "         4.61292177e-01,  7.11136281e-01, -3.38433571e-02,\n",
       "        -5.57939529e-01, -2.99192607e-01, -1.48460102e+00,\n",
       "         4.32187229e-01, -1.46460190e-01,  1.84582233e+00,\n",
       "        -2.40803346e-01,  1.02788234e+00, -1.66289389e+00,\n",
       "        -7.05655456e-01, -1.86570972e-01, -7.49205500e-02,\n",
       "        -1.13534272e+00,  1.07026987e-01,  2.07119957e-01,\n",
       "        -2.29636812e+00,  1.64329708e+00,  8.99407342e-02,\n",
       "         3.78047019e-01, -2.21461058e+00, -5.34928858e-01,\n",
       "        -4.01555151e-02, -5.84082127e-01, -8.03329110e-01,\n",
       "        -2.45860362e+00,  9.85501930e-02, -3.28197074e+00,\n",
       "        -1.46827316e+00,  7.93893337e+00,  6.26650393e-01,\n",
       "         1.69391245e-01,  1.14793801e+00, -3.53978753e-01,\n",
       "         3.58162254e-01, -1.06499791e+00, -1.26129329e+00,\n",
       "        -1.11649430e+00, -1.52122587e-01, -7.90478647e-01,\n",
       "        -2.98935622e-02, -1.79986322e+00,  4.15549010e-01,\n",
       "        -9.15061665e+00,  2.95982957e-01, -1.11661263e-01,\n",
       "        -2.26893806e+00, -4.07427788e-01,  2.99639583e+00,\n",
       "         1.10301161e+00, -4.73590517e+00,  1.61961943e-01,\n",
       "         9.13960874e-01,  1.60228312e+00,  9.65091646e-01,\n",
       "         1.37326941e-02, -1.00292659e+00,  2.49710369e+00,\n",
       "         1.95532024e+00, -1.79733491e+00, -1.40328312e+00,\n",
       "        -7.19354153e-01, -2.59252250e-01, -2.34689966e-01,\n",
       "        -8.49607408e-01, -5.04762590e-01, -6.17605746e-01,\n",
       "        -9.63499486e-01, -1.03271711e+00,  5.33264279e-01,\n",
       "         3.71438241e+00,  5.27171910e-01,  7.05322683e-01,\n",
       "         2.75861830e-01, -7.53513157e-01,  9.43696380e-01,\n",
       "         2.70279795e-01, -9.87844542e-02, -3.46789420e-01,\n",
       "         1.86171070e-01, -3.30248594e-01,  5.84291182e-02,\n",
       "         1.14996746e-01,  2.06357971e-01,  4.27338094e-01,\n",
       "         5.07665314e-02, -1.40367317e+00,  2.51749849e+00,\n",
       "         1.62298036e+00]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.1155e-04,  2.0606e-03,  1.5020e-03,  2.6562e-03,  9.1622e-03,\n",
       "          3.2599e-03,  1.4781e-03,  9.5434e-04,  3.3028e-04,  6.9261e-04,\n",
       "          8.2008e-04,  9.9679e-04,  1.7369e-04,  2.3844e-04,  3.0242e-04,\n",
       "          3.9385e-04,  4.7146e-03,  1.8696e-03,  1.0828e-03,  8.7911e-04,\n",
       "          1.6338e-04,  4.5681e-05,  9.7771e-06,  1.5454e-01,  3.1454e-02,\n",
       "          3.7411e-02,  1.8826e-02,  9.6202e-02,  1.1759e-02,  4.5734e-02,\n",
       "          2.2030e-01,  1.1428e-01,  6.0745e-02,  5.3709e-02,  2.1316e-02,\n",
       "          4.7606e-02,  5.1513e-02,  4.4246e+01, -2.1041e-02, -3.4620e+00,\n",
       "         -5.6576e-02,  2.3017e-01, -4.3493e-01, -4.8431e-01,  5.6465e-02,\n",
       "         -5.0499e-01,  8.3038e-02,  2.3239e-01, -1.3889e+00,  1.7120e+00,\n",
       "          1.3771e-01, -1.2663e+00,  6.6230e-01,  7.4898e-02, -3.1588e-01,\n",
       "          3.6870e-01, -6.8668e-01,  1.6357e+00, -8.1999e-01,  2.1713e-01,\n",
       "          6.4083e-01,  8.7478e-01,  9.4118e-01, -4.9381e-01, -1.2368e+00,\n",
       "          1.0730e+00,  1.7268e+00,  3.7660e+00, -6.7726e-01,  8.4824e-01,\n",
       "          4.6144e-01,  2.0225e-01,  1.7881e+00,  1.9420e-01,  1.8283e+00,\n",
       "          9.1964e-01,  5.5956e-01, -4.7744e-01, -5.5663e-01, -5.8612e-01,\n",
       "         -1.4102e+00,  2.4762e-01,  6.0904e-01,  3.9106e-01,  5.6764e-01,\n",
       "          1.3071e-01,  2.4557e-01, -4.7081e-02,  2.8010e-01,  4.1005e-01,\n",
       "         -3.8123e-01, -2.4296e-01,  6.7043e-02, -1.6497e+00,  8.9188e-01,\n",
       "         -3.1135e-01, -1.0625e-02, -3.0718e+00, -5.4417e-01,  2.4900e-02,\n",
       "          4.6762e-01,  1.0117e-01, -3.3451e+00, -5.2773e-01, -4.1818e+00,\n",
       "         -2.4967e-01,  7.9518e+00, -2.8008e-01, -3.0822e-01,  3.9351e-01,\n",
       "         -2.2870e-01, -4.7540e-02, -2.3264e-01, -3.3452e-01, -2.4542e-01,\n",
       "         -1.0442e+00,  7.3891e-01,  5.5050e-01, -1.6255e+00, -4.6427e-02,\n",
       "         -6.1029e+00, -6.0916e-01, -7.7775e-01, -2.9953e+00, -7.7228e-01,\n",
       "          9.6793e-01, -9.4810e-01, -1.9821e+00,  1.9788e-01, -1.3584e-01,\n",
       "          2.5486e+00,  2.9703e-01,  2.3074e-01, -1.8084e-01, -4.3078e-01,\n",
       "          6.9299e-01, -1.9842e+00, -7.0539e-01, -2.4653e-01, -9.6330e-01,\n",
       "         -7.8405e-01, -8.5037e-01, -9.2285e-01, -5.4372e-01,  1.0601e+00,\n",
       "          6.9960e-02, -8.5179e-01,  4.2882e+00,  6.4952e-02, -3.8099e-01,\n",
       "          1.5795e+00,  8.3206e-02,  6.9494e-01, -5.4985e-01,  2.3210e-02,\n",
       "          6.0039e-01, -2.0198e-01, -2.7308e-01, -2.6412e-01,  5.0381e-01,\n",
       "         -5.9945e-01,  2.0151e-02,  3.9549e-01, -5.9732e-01,  2.6158e+00,\n",
       "          1.5727e+00]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31a7b1cb5f073f7a7d37b3db504c6954ce2b88e0f82e412b65ad0b5f2dd17394"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
