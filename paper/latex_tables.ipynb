{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [\"2016\", \"2017\", \"2018\"]\n",
    "years_vfp = [\"2016APV\", \"2016\", \"2017\", \"2018\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"samples/JetHT.yaml\") as f:\n",
    "    samples = yaml.safe_load(f)[\"samples\"]\n",
    "\n",
    "lines = []\n",
    "\n",
    "for year in years:\n",
    "    datasets = list(samples[f\"JetHT{year}\"][\"datasets\"].values())\n",
    "    for i in range(len(datasets)):\n",
    "        datasets[i] = datasets[i].replace(\"_\", \"\\_\")\n",
    "\n",
    "    lend = len(datasets)\n",
    "    lines.append(rf\"\\multirow{{{lend}}}{{*}}{{ {year} }} & {datasets[0]} \\\\\" + \"\\n\")\n",
    "\n",
    "    for dataset in datasets[1:]:\n",
    "        lines.append(rf\" & {dataset} \\\\\" + \"\\n\")\n",
    "\n",
    "    lines.append(r\"\\hline\" + \"\\n\")\n",
    "\n",
    "# remove trailing \"\\\\\" and \"\\hline\"\n",
    "lines = lines[:-1]\n",
    "lines[-1] = lines[-1][:-4]\n",
    "\n",
    "with open(\"tables/datasets_jetht.tex\", \"w\") as f:\n",
    "    f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"samples/SingleMuon.yaml\") as f:\n",
    "    samples = yaml.safe_load(f)[\"samples\"]\n",
    "\n",
    "lines = []\n",
    "\n",
    "for year in years:\n",
    "    datasets = list(samples[f\"SingleMu{year}\"][\"datasets\"].values())\n",
    "    for i in range(len(datasets)):\n",
    "        datasets[i] = datasets[i].replace(\"_\", \"\\_\")\n",
    "\n",
    "    lend = len(datasets)\n",
    "    lines.append(rf\"\\multirow{{{lend}}}{{*}}{{ {year} }} & {datasets[0]} \\\\\" + \"\\n\")\n",
    "\n",
    "    for dataset in datasets[1:]:\n",
    "        lines.append(rf\" & {dataset} \\\\\" + \"\\n\")\n",
    "\n",
    "    lines.append(r\"\\hline\" + \"\\n\")\n",
    "\n",
    "# remove trailing \"\\\\\" and \"\\hline\"\n",
    "lines = lines[:-1]\n",
    "lines[-1] = lines[-1][:-4]\n",
    "\n",
    "with open(\"tables/datasets_singlemuon.tex\", \"w\") as f:\n",
    "    f.writelines(lines)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xsec(xsec, round=3):\n",
    "    if isinstance(xsec, str):\n",
    "        xsec = eval(xsec)\n",
    "\n",
    "    xsec = str(np.round(xsec, round))\n",
    "\n",
    "    # remove trailing zeros\n",
    "    for i in range(len(xsec)):\n",
    "        if xsec[-1] == \"0\":\n",
    "            xsec = xsec[:-1]\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # remove trailing \".\"\n",
    "    if xsec[-1] == \".\":\n",
    "        xsec = xsec[:-1]\n",
    "\n",
    "    return xsec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process or sample and corresponding citation in paper\n",
    "refs = {\n",
    "    \"WW\": \"Gehrmann:2014fva\",\n",
    "    \"WZ\": \"Campbell:2011bn\",\n",
    "    \"ZZ\": \"Cascioli:2014yka\",\n",
    "    \"SingleTop\": \"SingleTopNNLORef\",\n",
    "    \"TTbar\": \"TtbarNNLO\",\n",
    "    \"HH\": \"LHCHiggsHH\",\n",
    "    # \"HWW (inclusive)\": \"YR4\",\n",
    "    # \"Hbb (inclusive)\": \"YR4\",\n",
    "    # \"QCD\": \"xsdb\",\n",
    "    # \"V+Jets\": \"xsdb\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"samples/MC_bg.yaml\") as f:\n",
    "    samples = yaml.safe_load(f)[\"samples\"]\n",
    "\n",
    "with open(\"../data/xsecs.json\") as f:\n",
    "    xsecs = json.load(f)\n",
    "\n",
    "lines = []\n",
    "\n",
    "for sample in np.sort(list(samples.keys())):\n",
    "    sample_text = sample\n",
    "    if sample in refs:\n",
    "        sample_text += rf\"~\\cite{{{refs[sample]}}}\"\n",
    "\n",
    "    for key, dataset in samples[sample][\"datasets\"].items():\n",
    "        dname = dataset.split(\"/\")[1].replace(\"_\", \"\\_\")\n",
    "        citation = \"\" if key not in refs else rf\"~\\cite{{{refs[key]}}}\"\n",
    "        lines.append(rf\" & {dname} & {process_xsec(xsecs[key])}{citation} \\\\\" + \"\\n\")\n",
    "\n",
    "    lend = len(samples[sample][\"datasets\"])\n",
    "    lines[-lend] = rf\"\\multirow{{{lend}}}{{*}}{{{sample_text}}}\" + lines[-lend]\n",
    "    lines.append(r\"\\hline\" + \"\\n\")\n",
    "\n",
    "# remove trailing \"\\\\\" and \"\\hline\"\n",
    "lines = lines[:-1]\n",
    "lines[-1] = lines[-1][:-4]\n",
    "\n",
    "with open(\"tables/datasets_mcbg.tex\", \"w\") as f:\n",
    "    f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"samples/MC_sig.yaml\") as f:\n",
    "    samples = yaml.safe_load(f)[\"samples\"]\n",
    "\n",
    "with open(\"../data/xsecs.json\") as f:\n",
    "    xsecs = json.load(f)\n",
    "\n",
    "lines = []\n",
    "\n",
    "for sample in np.sort(list(samples.keys())):\n",
    "    for key, dataset in samples[sample][\"datasets\"].items():\n",
    "        if key.startswith(\"GluGluToHHTo\"):\n",
    "            key = key.split(\"_pn4q\")[0]\n",
    "            cl = key.split(\"cHHH\")[1].split(\"_\")[0].replace(\"p\", \".\")\n",
    "            sm = \"SM\" if cl == \"1\" else \"BSM\"\n",
    "            pname = rf\"ggHH {sm} ($\\kappa_{{\\lambda}} = {cl}$)\"\n",
    "        else:\n",
    "            cv = key.split(\"CV_\")[1].split(\"_C2V\")[0].replace(\"_\", \".\")\n",
    "            c2v = key.split(\"C2V_\")[1].split(\"_C3\")[0].replace(\"_\", \".\")\n",
    "            c3 = key.split(\"C3_\")[1].replace(\"_\", \".\")\n",
    "            sm = \"SM\" if (cv == \"1\" and c2v == \"1\" and c3 == \"1\") else \"BSM\"\n",
    "            pname = rf\"VBFHH {sm} ($\\kappa_\\mathrm{{V}} = {cv}$, $\\kappa_\\mathrm{{2V}} = {c2v}$, $\\kappa_{{\\lambda}} = {c3}$)\"\n",
    "\n",
    "        dname = dataset.split(\"/\")[1].replace(\"_\", \"\\_\")\n",
    "        lines.append(\n",
    "            rf\"{pname} & {dname} & {float(process_xsec(xsecs[key], 10)) * 1000:.2f} \\\\\" + \"\\n\"\n",
    "        )\n",
    "\n",
    "# remove trailing \"\\\\\"\n",
    "lines[-1] = lines[-1][:-4]\n",
    "\n",
    "with open(\"tables/datasets_mcsig_nonres.tex\", \"w\") as f:\n",
    "    f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mxmy(sample):\n",
    "    mY = int(sample.split(\"-\")[-1])\n",
    "    mX = int(sample.split(\"NMSSM_XToYHTo2W2BTo4Q2B_MX-\")[1].split(\"_\")[0])\n",
    "\n",
    "    return (mX, mY)\n",
    "\n",
    "\n",
    "with open(\"../data/xhy/parsed_miniaod_2017.yaml\") as f:\n",
    "    samples = yaml.safe_load(f)[\"samples\"][\"XHY\"][\"datasets\"]\n",
    "\n",
    "mps = [[*mxmy(sample)] for sample in samples]\n",
    "mxs = np.sort(np.unique(np.array(mps)[:, 0]))\n",
    "mys = np.sort(np.unique(np.array(mps)[:, 1]))\n",
    "\n",
    "lines = []\n",
    "\n",
    "line = r\"$M_Y$ [\\gev] & \" + \" & \".join(mxs.astype(str)) + r\"\\\\\" + \"\\n\"\n",
    "lines.append(line)\n",
    "\n",
    "for my in mys:\n",
    "    line = rf\"{my}\"\n",
    "    for mx in mxs:\n",
    "        line += \" & \"\n",
    "        if [mx, my] in mps:\n",
    "            line += r\"$\\checkmark$ \"\n",
    "    line += r\"\\\\\" + \"\\n\"\n",
    "    lines.append(line)\n",
    "\n",
    "# remove trailing \"\\\\\\n\"\n",
    "lines[-1] = lines[-1][:-3]\n",
    "\n",
    "with open(\"tables/datasets_mcsig_res.tex\", \"w\") as f:\n",
    "    f.writelines(lines)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from triggers import HLTs\n",
    "\n",
    "lines = []\n",
    "\n",
    "for year in years:\n",
    "    hlts = deepcopy(HLTs[year])\n",
    "    for i in range(len(hlts)):\n",
    "        hlts[i] = hlts[i].replace(\"_\", \"\\_\")\n",
    "\n",
    "    lend = len(hlts)\n",
    "    lines.append(rf\"\\multirow{{{lend}}}{{*}}{{ {year} }} & {hlts[0]} \\\\\" + \"\\n\")\n",
    "\n",
    "    for hlt in hlts[1:]:\n",
    "        lines.append(rf\" & {hlt} \\\\\" + \"\\n\")\n",
    "\n",
    "    lines.append(r\"\\hline\" + \"\\n\")\n",
    "\n",
    "# remove trailing \"\\\\\" and \"\\hline\"\n",
    "lines = lines[:-1]\n",
    "lines[-1] = lines[-1][:-4]\n",
    "\n",
    "with open(\"tables/hlts.tex\", \"w\") as f:\n",
    "    f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from triggers import muon_HLTs\n",
    "\n",
    "lines = []\n",
    "\n",
    "for year in years:\n",
    "    hlts = deepcopy(muon_HLTs[year])\n",
    "    for i in range(len(hlts)):\n",
    "        hlts[i] = hlts[i].replace(\"_\", \"\\_\")\n",
    "\n",
    "    lend = len(hlts)\n",
    "    lines.append(rf\"\\multirow{{{lend}}}{{*}}{{ {year} }} & {hlts[0]} \\\\\" + \"\\n\")\n",
    "\n",
    "    for hlt in hlts[1:]:\n",
    "        lines.append(rf\" & {hlt} \\\\\" + \"\\n\")\n",
    "\n",
    "    lines.append(r\"\\hline\" + \"\\n\")\n",
    "\n",
    "# remove trailing \"\\\\\" and \"\\hline\"\n",
    "lines = lines[:-1]\n",
    "lines[-1] = lines[-1][:-4]\n",
    "\n",
    "with open(\"tables/hlts_singlemuon.tex\", \"w\") as f:\n",
    "    f.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lund plane SFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonresonant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# templates_dir = Path(\"../src/HHbbVV/postprocessing/templates/24Apr26NonresBDT995AllSigs\")\n",
    "templates_dir = Path(\"../src/HHbbVV/postprocessing/templates/25Feb6NonresMatchingFix\")\n",
    "dfs = {\n",
    "    \"ggF\": pd.read_csv(templates_dir / \"lpsfs_passggf.csv\"),\n",
    "    \"VBF\": pd.read_csv(templates_dir / \"lpsfs_passvbf.csv\"),\n",
    "}\n",
    "\n",
    "sig_map = {\n",
    "    \"HHbbVV\": r\"SM ggF \\HH\",\n",
    "    \"VBFHHbbVV\": r\"SM VBF \\HH\",\n",
    "    \"qqHH_CV_1_C2V_0_kl_1_HHbbVV\": r\"VBF \\HH ($\\kapvv = 0$)\",\n",
    "    \"qqHH_CV_1_C2V_2_kl_1_HHbbVV\": r\"VBF \\HH ($\\kapvv = 2$)\",\n",
    "}\n",
    "\n",
    "df = dfs[\"ggF\"]\n",
    "\n",
    "lines = []\n",
    "\n",
    "for j, (region, df) in enumerate(dfs.items()):\n",
    "    sigs = df.to_numpy()[:, 0]\n",
    "    for i, (sig, siglabel) in enumerate(sig_map.items()):\n",
    "        if i == 0:\n",
    "            region_label = rf\"\\multirow{{{len(sig_map)}}}{{*}}{{{region}}}\"\n",
    "        else:\n",
    "            region_label = \"\"\n",
    "\n",
    "        line = [region_label, siglabel]\n",
    "        row = df[df[\"Unnamed: 0\"] == sig]\n",
    "\n",
    "        sf = row[\"SF\"].values[0][:4]\n",
    "        pm = row[\"SF\"].values[0].split(\"+\")[1].split(\"-\")\n",
    "        line.append(rf\"${sf}^{{+{pm[0]}}}_{{-{pm[1]}}}$\")\n",
    "\n",
    "        for unc in [\"stat_unc\", \"sys_unc\", \"sj_pt_unc\", \"np_unc\"]:\n",
    "            line.append(f\"${row[unc].values[0]:.2f}$\")\n",
    "\n",
    "        for unc in [\"unmatched\", \"dist\"]:\n",
    "            pm = row[unc].values[0].split(\"+\")[1].split(\"-\")\n",
    "            line.append(rf\"${{}}^{{+{pm[0]}}}_{{-{pm[1]}}}$\")\n",
    "\n",
    "        lines.append(\" & \".join(line) + r\" \\\\\" + \"\\n\")\n",
    "\n",
    "    if j != len(dfs) - 1:\n",
    "        lines[-1] = lines[-1][:-1] + r\"[3mm]\" + \"\\n\"\n",
    "\n",
    "# remove trailing \"\\\\\" and \"\\n\"\n",
    "lines[-1] = lines[-1][:-4]\n",
    "\n",
    "with Path(\"tables/nonres_lpsfs.tex\").open(\"w\") as f:\n",
    "    f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row[\"SF\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resonant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HHbbVV.hh_vars import res_samples\n",
    "from HHbbVV.postprocessing import utils\n",
    "\n",
    "# pick random subset for table\n",
    "table_samples = list(res_samples.values())[::16]\n",
    "templates_dir = Path(\"/ceph/cms/store/user/rkansal/bbVV/templates/25Feb8XHYFix\")\n",
    "\n",
    "\n",
    "def _sig_map(sample: str):\n",
    "    mx, my = utils.mxmy(sample)\n",
    "    return rf\"X[{mx}]$\\rightarrow$HY[{my}]\"\n",
    "\n",
    "\n",
    "lines = []\n",
    "\n",
    "for sample in table_samples:\n",
    "    df = pd.read_csv(templates_dir / sample / f\"lpsfs_pass.csv\")\n",
    "    line = [_sig_map(sample)]\n",
    "\n",
    "    sf = df[\"SF\"].values[0][:4]\n",
    "    pm = df[\"SF\"].values[0].split(\"+\")[1].split(\"-\")\n",
    "    line.append(rf\"${sf}^{{+{pm[0]}}}_{{-{pm[1]}}}$\")\n",
    "\n",
    "    for unc in [\"stat_unc\", \"sys_unc\", \"sj_pt_unc\", \"np_unc\"]:\n",
    "        line.append(f\"${df[unc].values[0]:.2f}$\")\n",
    "\n",
    "    for unc in [\"unmatched\", \"dist\"]:\n",
    "        pm = df[unc].values[0].split(\"+\")[1].split(\"-\")\n",
    "        line.append(rf\"${{}}^{{+{pm[0]}}}_{{-{pm[1]}}}$\")\n",
    "\n",
    "    lines.append(\" & \".join(line) + r\" \\\\\" + \"\\n\")\n",
    "\n",
    "# remove trailing \"\\\\\" and \"\\n\"\n",
    "lines[-1] = lines[-1][:-4]\n",
    "\n",
    "with Path(\"tables/res_lpsfs.tex\").open(\"w\") as f:\n",
    "    f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
